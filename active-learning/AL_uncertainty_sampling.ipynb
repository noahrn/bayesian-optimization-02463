{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>02463 - Active Machine Learning and Agency</center>\n",
    "## <center>Active machine learning - Uncertainty sampling</center>\n",
    "\n",
    "<center>Kristoffer H. Madsen</center>\n",
    "<br>\n",
    "<center>\n",
    "<table style=\"border:none;border-collapse: collapse;border-style:hidden;background-color:#FFFFFF;border-spacing:0;margin=0;padding:0;cellspacing:0;cellpadding:0\">\n",
    "<tr style=\"background-color:#FFFFFF; border:none; borderstyle:hidden; border-bottom:none\">\n",
    "<th  style=\"text-align:center\">Associate Professor</th><th>  </th>\n",
    "<th style=\"text-align:center\">Senior Researcher</th>\n",
    "</tr>\n",
    "        <tr style=\"background-color:#FFFFFF; border:none; borderstyle:hidden; border-bottom:none\">\n",
    "<th style=\"text-align:center\">Section for Cognitive Systems</th><th> </th>\n",
    "<th style=\"text-align:center\">Computational Neuroimaging Group</th>\n",
    "</tr>\n",
    "    <tr style=\"background-color:#FFFFFF; border:none; borderstyle:hidden; border-bottom:none\">\n",
    "<th style=\"text-align:center\">Department of Applied Mathematics and computer Science</th><th> </th>\n",
    "<th style=\"text-align:center\">Danish Research Centre for Magnetic Resonance</th>\n",
    "</tr>\n",
    "    <tr style=\"background-color:#FFFFFF\">\n",
    "<th style=\"text-align:center\">Technical University of Denmark</th><th></th>\n",
    "<th style=\"text-align:center\">Copenhagen University Hospital Hvidovre</th>\n",
    "</tr>\n",
    "        <tr style=\"background-color:#FFFFFF\">\n",
    "<th style=\"text-align:center\">khma@dtu.dk</th><th></th>\n",
    "<th style=\"text-align:center\">kristofferm@drcmr.dk</th>\n",
    "</tr>\n",
    "            <tr style=\"background-color:#FFFFFF\">\n",
    "<th style=\"text-align:center\"><center><img src=imgs/dtu.png width=70></center></th><th>  </th>\n",
    "<th style=\"text-align:center\"><center><img src=imgs/drcmr.png width=210></center></th>\n",
    "</tr>\n",
    "</table>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ingredients\n",
    "\n",
    "<img src=\"imgs/active_learn.png\" width=500>\n",
    "\n",
    "- Teacher / oracle (can give labels)\n",
    "- Learner (selects which labels to obtain)\n",
    "    - Uncertainty quantification\n",
    "    - Again knowing how uncertain we are about the labels of data points (where we do not know the label) is key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# For categorical data\n",
    "- One possibility would be to simply sample the point where the probability of the most likely label is smallest\n",
    "    - Least confidence sampling: $$x^* = \\underset{x \\in U}{\\text{argmax}} ~ (1 - p_\\theta(\\hat{y}_1 | x))$$\n",
    "\n",
    "- Sample the point where the difference between the two most likely options is smallest\n",
    "    - Margin sampling: $$x^* = \\underset{x \\in U}{\\text{argmin}} ~ (p_\\theta(\\hat{y}_1 | x) - p_\\theta(\\hat{y}_2 | x))$$\n",
    "\n",
    "- Sample the point with the largest entropy\n",
    "    - Entropy sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is entropy?\n",
    "## Information content\n",
    "- Number of possible configurations\n",
    "    - Say we assume that the prior possibility for rain is 50%.\n",
    "    - That is there are two possible configurations for this variable either rain or no rain (with equal probability, and independent of other variables).\n",
    "    - The total number of options quantifies uncertainty (freedom in the system)\n",
    "    - If now someone (whom we trust) says that it won't rain we reduce the number of options by half \n",
    "    - In large systems the possible configurations become quite large, therefore using the logarithm of these options is more convenient\n",
    "    - Also this log transformation ensures that if we get two independent pieces of information the information will be additive otherwise it would be multiplicative\n",
    "- Information before:\n",
    "    - $I=\\operatorname{log}_2(2)=1$\n",
    "- Information after:\n",
    "    - $I=\\operatorname{log}_2(1)=0$\n",
    "- Change in information: 1 bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can write this is terms of the probability distribution (where $N=\\tfrac{1}{p(X=\\operatorname{rain})}$):\n",
    "$$I(X=\\operatorname{rain}) = \\operatorname{log}_2\\left(\\frac{1}{P(\\operatorname{X=rain})}\\right)$$\n",
    "- But this only considers one of the options so we can take the expectation over the possibilities\n",
    "$$H(X)=-\\sum_x P(X=x)\\operatorname{log}_2(P(x))=1$$\n",
    "- Which gives us the typical definition of entropy (here measured in bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What if you live in the dessert?\n",
    "- Then apriori we would not expect rain say $P(X=\\operatorname{rain}) = 0.01$\n",
    "    - $H(x)=-0.01\\operatorname{log}_2(0.01)-0.99\\operatorname{log}_2(0.99)\\approx0.08$\n",
    "    - If someone where to tell os that it would rain, that would highly informative, otherwise (most of the times) the information gain is much less\n",
    "    - On average we do not gain much information\n",
    "- This readily generalizes to more options\n",
    "    - In case of 8 options getting one label gets us $\\operatorname{log}_2(8)=3$ bits of information\n",
    "- And to cases where the label is noisy (here possibility for the remaining options will not be reduced to zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Relative entropy\n",
    "$$D_{\\text{KL}}(P\\mid\\mid Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
    "- Information gain obtained by updating the prior distribution $Q(x)$ to $P(x)$\n",
    "- The change in information weighted by the probability represented in P(X)\n",
    "- Non-symmetric (expectation is over P(x) exchanging to Q(x) will not always give the same result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Select point with highest entropy:\n",
    "$$x^* = \\underset{x \\in U}{\\text{argmax}} ~ -\\sum_{y\\in Y} p_\\theta(y | x) \\log p_\\theta(y | x)$$\n",
    "\n",
    "With binary labels these three strategies (least confident, largest margin, entropy) are equivalent (the log is a monotonic function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two classes (binary)\n",
    "<img src=imgs/uncertainty_sampling_metrics_2d.png width=1000\n",
    "    >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Three classes (ternary)\n",
    "<img src=imgs/uncertainty_sampling_3class.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Version space\n",
    "- The set of models which are compatible with the currently sampled data\n",
    "- To optimally learn model we wish to maximally reduce the version space\n",
    "- For discrete and noiseless models the size version space can be measured as the number of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When we consider non-discrete settings actually all models become somehow compatible with the data\n",
    "- Then the size of the model space can be considered a probability weigthed volume"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
