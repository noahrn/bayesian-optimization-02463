{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Setting up transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Loading the CIFAR-10 datasets\n",
    "data_dir = './CIFAR10_data'\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "# Assuming you start with a small labeled dataset and the rest is unlabeled\n",
    "initial_train_size = 1000\n",
    "unlabeled_size = len(trainset) - initial_train_size\n",
    "\n",
    "trainset_labeled_indices = list(range(initial_train_size))\n",
    "trainset_unlabeled_indices = list(range(initial_train_size, len(trainset)))\n",
    "\n",
    "labeled_set = torch.utils.data.Subset(trainset, trainset_labeled_indices)\n",
    "unlabeled_set = torch.utils.data.Subset(trainset, trainset_unlabeled_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(labeled_set, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds \n",
    "seed=42\n",
    "random.seed(seed)  # Python\n",
    "np.random.seed(seed)  # NumPy\n",
    "torch.manual_seed(seed)  # PyTorch\n",
    "torch.cuda.manual_seed_all(seed)  # PyTorch CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:\n",
    "                print(f'[{epoch + 1}, {i + 1}]: loss: {running_loss / 200}')\n",
    "                running_loss = 0.0\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_sampling(model, unlabeled_loader, n):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    uncertainties = []\n",
    "    with torch.no_grad():  # No need to compute gradients\n",
    "        for inputs, _ in unlabeled_loader:\n",
    "            outputs = model(inputs)\n",
    "            # We'll use entropy as a measure of uncertainty\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            log_probabilities = F.log_softmax(outputs, dim=1)\n",
    "            entropy = -(probabilities * log_probabilities).sum(dim=1)\n",
    "            uncertainties.extend(entropy.tolist())\n",
    "    \n",
    "    # Get the indices of the top n uncertain samples\n",
    "    uncertain_indices = np.argsort(uncertainties)[-n:]\n",
    "    \n",
    "    model.train()  # Set the model back to training mode\n",
    "    return uncertain_indices\n",
    "\n",
    "def random_sampling(n, unlabeled_set_size):\n",
    "    # Randomly select n indices from the remaining unlabeled set\n",
    "    random_indices = np.random.choice(range(unlabeled_set_size), size=n, replace=False)\n",
    "    return random_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    model.train()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoretisk bedste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset_full = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "# testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "# # DataLoaders\n",
    "# trainloader_full = DataLoader(trainset_full, batch_size=64, shuffle=True)\n",
    "# testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# # Model Initialization\n",
    "# model_full = EnhancedCNN()\n",
    "\n",
    "# # Train the model on the full training set\n",
    "# train_model(model_full, trainloader_full, epochs=10)  # Adjust epochs as needed\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# accuracy_full = evaluate_model(model_full, testloader)\n",
    "# print(f'Accuracy on the full CIFAR-10 test set: {accuracy_full*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Finished Training\n",
      "Test Accuracy after iteration 1: 0.5315\n",
      "Iteration 2\n",
      "Finished Training\n",
      "Test Accuracy after iteration 2: 0.5696\n",
      "Iteration 3\n",
      "Finished Training\n",
      "Test Accuracy after iteration 3: 0.5889\n",
      "Iteration 4\n",
      "Finished Training\n",
      "Test Accuracy after iteration 4: 0.6098\n",
      "Iteration 5\n",
      "Finished Training\n",
      "Test Accuracy after iteration 5: 0.638\n",
      "Iteration 6\n",
      "Finished Training\n",
      "Test Accuracy after iteration 6: 0.6441\n",
      "Iteration 7\n",
      "Finished Training\n",
      "Test Accuracy after iteration 7: 0.657\n",
      "Iteration 8\n",
      "Finished Training\n",
      "Test Accuracy after iteration 8: 0.6731\n",
      "Iteration 9\n",
      "Finished Training\n",
      "Test Accuracy after iteration 9: 0.6851\n",
      "Iteration 10\n",
      "Finished Training\n",
      "Test Accuracy after iteration 10: 0.6919\n",
      "Iteration 11\n",
      "Finished Training\n",
      "Test Accuracy after iteration 11: 0.6982\n",
      "Iteration 12\n",
      "[1, 200]: loss: 0.2889192111231387\n",
      "[2, 200]: loss: 0.10160226825624705\n",
      "[3, 200]: loss: 0.045718347229994835\n",
      "[4, 200]: loss: 0.020077182480599733\n",
      "[5, 200]: loss: 0.06810297301271931\n",
      "[6, 200]: loss: 0.028088438300183042\n",
      "[7, 200]: loss: 0.01061259655863978\n",
      "[8, 200]: loss: 0.0068993604090064765\n",
      "[9, 200]: loss: 0.007131533467909321\n",
      "[10, 200]: loss: 0.005225415265304036\n",
      "Finished Training\n",
      "Test Accuracy after iteration 12: 0.696\n",
      "Iteration 13\n",
      "[1, 200]: loss: 0.2700465199537575\n",
      "[2, 200]: loss: 0.07510672237258405\n",
      "[3, 200]: loss: 0.026767941666767003\n",
      "[4, 200]: loss: 0.014516355227679014\n",
      "[5, 200]: loss: 0.00870170961599797\n",
      "[6, 200]: loss: 0.0060345733707072215\n",
      "[7, 200]: loss: 0.0032134305528597907\n",
      "[8, 200]: loss: 0.002770988997945096\n",
      "[9, 200]: loss: 0.0023915069064241834\n",
      "[10, 200]: loss: 0.0022500516471336595\n",
      "Finished Training\n",
      "Test Accuracy after iteration 13: 0.7162\n",
      "Iteration 14\n",
      "[1, 200]: loss: 0.25116202819161115\n",
      "[2, 200]: loss: 0.06511067358776927\n",
      "[3, 200]: loss: 0.03034875844139606\n",
      "[4, 200]: loss: 0.01575043040444143\n",
      "[5, 200]: loss: 0.022609876232454553\n",
      "[6, 200]: loss: 0.021800231079105288\n",
      "[7, 200]: loss: 0.006898108975728973\n",
      "[8, 200]: loss: 0.004760346313123592\n",
      "[9, 200]: loss: 0.0036960893560899423\n",
      "[10, 200]: loss: 0.0031430154264671726\n",
      "Finished Training\n",
      "Test Accuracy after iteration 14: 0.7167\n",
      "Iteration 15\n",
      "[1, 200]: loss: 0.22582266390323638\n",
      "[2, 200]: loss: 0.05416929588187486\n",
      "[3, 200]: loss: 0.0236330885021016\n",
      "[4, 200]: loss: 0.017666774602839722\n",
      "[5, 200]: loss: 0.031063340036198497\n",
      "[6, 200]: loss: 0.02850967485748697\n",
      "[7, 200]: loss: 0.008166583567508497\n",
      "[8, 200]: loss: 0.005575277519528754\n",
      "[9, 200]: loss: 0.004415868085343391\n",
      "[10, 200]: loss: 0.0033678574749501423\n",
      "Finished Training\n",
      "Test Accuracy after iteration 15: 0.7183\n",
      "Iteration 16\n",
      "[1, 200]: loss: 0.20721158880740403\n",
      "[2, 200]: loss: 0.058552589979954064\n",
      "[3, 200]: loss: 0.022970906820846723\n",
      "[4, 200]: loss: 0.011089593690121547\n",
      "[5, 200]: loss: 0.006995174106559716\n",
      "[6, 200]: loss: 0.003875950892979745\n",
      "[7, 200]: loss: 0.002903459934750572\n",
      "[8, 200]: loss: 0.0025199273772886954\n",
      "[9, 200]: loss: 0.0018490311308414675\n",
      "[10, 200]: loss: 0.0018273056426551192\n",
      "Finished Training\n",
      "Test Accuracy after iteration 16: 0.7307\n",
      "Iteration 17\n",
      "[1, 200]: loss: 0.18163571612909435\n",
      "[2, 200]: loss: 0.05149841592647135\n",
      "[3, 200]: loss: 0.04719793915050104\n",
      "[4, 200]: loss: 0.019990943957236596\n",
      "[5, 200]: loss: 0.01886220455635339\n",
      "[6, 200]: loss: 0.013095302459551022\n",
      "[7, 200]: loss: 0.004870321070193313\n",
      "[8, 200]: loss: 0.003467287181993015\n",
      "[9, 200]: loss: 0.0029760960888233967\n",
      "[10, 200]: loss: 0.0026688528317026796\n",
      "Finished Training\n",
      "Test Accuracy after iteration 17: 0.7307\n",
      "Iteration 18\n",
      "[1, 200]: loss: 0.16047213391400875\n",
      "[2, 200]: loss: 0.05102214469574392\n",
      "[3, 200]: loss: 0.027282807504525407\n",
      "[4, 200]: loss: 0.01808013818808831\n",
      "[5, 200]: loss: 0.03660852928413078\n",
      "[6, 200]: loss: 0.027313072882825508\n",
      "[7, 200]: loss: 0.00840819334553089\n",
      "[8, 200]: loss: 0.005099471704452299\n",
      "[9, 200]: loss: 0.005003139908658341\n",
      "[10, 200]: loss: 0.0036557936845929362\n",
      "Finished Training\n",
      "Test Accuracy after iteration 18: 0.7368\n",
      "Iteration 19\n",
      "[1, 200]: loss: 0.1645757727441378\n",
      "[2, 200]: loss: 0.0541839269688353\n",
      "[3, 200]: loss: 0.023436106418957933\n",
      "[4, 200]: loss: 0.012668503658496776\n",
      "[5, 200]: loss: 0.013656874355510808\n",
      "[6, 200]: loss: 0.007768461520900018\n",
      "[7, 200]: loss: 0.003907247130409814\n",
      "[8, 200]: loss: 0.0027068271600001028\n",
      "[9, 200]: loss: 0.0020837963449594097\n",
      "[10, 200]: loss: 0.0019166759648942388\n",
      "Finished Training\n",
      "Test Accuracy after iteration 19: 0.7369\n",
      "Iteration 20\n",
      "[1, 200]: loss: 0.13640660561970436\n",
      "[2, 200]: loss: 0.06741033918689937\n",
      "[3, 200]: loss: 0.042783064192626626\n",
      "[4, 200]: loss: 0.01137449598114472\n",
      "[5, 200]: loss: 0.00846847557928413\n",
      "[6, 200]: loss: 0.005835522156266961\n",
      "[7, 200]: loss: 0.00440057208907092\n",
      "[8, 200]: loss: 0.002595325818983838\n",
      "[9, 200]: loss: 0.0024786165916884785\n",
      "[10, 200]: loss: 0.0016755524319887628\n",
      "Finished Training\n",
      "Test Accuracy after iteration 20: 0.7427\n",
      "Iteration 21\n",
      "[1, 200]: loss: 0.13282918438548222\n",
      "[2, 200]: loss: 0.03613483152585104\n",
      "[3, 200]: loss: 0.01772564340615645\n",
      "[4, 200]: loss: 0.013893700593616814\n",
      "[5, 200]: loss: 0.03134390023653395\n",
      "[6, 200]: loss: 0.03886234171222895\n",
      "[7, 200]: loss: 0.008612918017315679\n",
      "[8, 200]: loss: 0.005777398670616094\n",
      "[9, 200]: loss: 0.004223163776914589\n",
      "[10, 200]: loss: 0.003409735594468657\n",
      "Finished Training\n",
      "Test Accuracy after iteration 21: 0.7452\n",
      "Iteration 22\n",
      "[1, 200]: loss: 0.12995683540706524\n",
      "[2, 200]: loss: 0.04066882908111438\n",
      "[3, 200]: loss: 0.019432316252496095\n",
      "[4, 200]: loss: 0.00910788347304333\n",
      "[5, 200]: loss: 0.01975893749622628\n",
      "[6, 200]: loss: 0.023182973281363956\n",
      "[7, 200]: loss: 0.0062212257258943285\n",
      "[8, 200]: loss: 0.004590043033531401\n",
      "[9, 200]: loss: 0.0029840243014041335\n",
      "[10, 200]: loss: 0.002620051038247766\n",
      "Finished Training\n",
      "Test Accuracy after iteration 22: 0.7461\n",
      "Iteration 23\n",
      "[1, 200]: loss: 0.11459043687093072\n",
      "[2, 200]: loss: 0.03401649576728232\n",
      "[3, 200]: loss: 0.014772973588551395\n",
      "[4, 200]: loss: 0.008687082038377411\n",
      "[5, 200]: loss: 0.011043387909303419\n",
      "[6, 200]: loss: 0.023609569490654393\n",
      "[7, 200]: loss: 0.005541875000053551\n",
      "[8, 200]: loss: 0.0036249092452635525\n",
      "[9, 200]: loss: 0.0026276864485407715\n",
      "[10, 200]: loss: 0.0022023222644929775\n",
      "Finished Training\n",
      "Test Accuracy after iteration 23: 0.7504\n",
      "Iteration 24\n",
      "[1, 200]: loss: 0.10653111780062317\n",
      "[2, 200]: loss: 0.02786404350074008\n",
      "[3, 200]: loss: 0.017239402434206567\n",
      "[4, 200]: loss: 0.011952928607352078\n",
      "[5, 200]: loss: 0.018064715644868556\n",
      "[6, 200]: loss: 0.029693373881746084\n",
      "[7, 200]: loss: 0.0069206672627478835\n",
      "[8, 200]: loss: 0.004464118514733855\n",
      "[9, 200]: loss: 0.0032136347058985847\n",
      "[10, 200]: loss: 0.0032719111524056642\n",
      "Finished Training\n",
      "Test Accuracy after iteration 24: 0.7526\n",
      "Iteration 25\n",
      "[1, 200]: loss: 0.10459491796558723\n",
      "[1, 400]: loss: 0.12338669414864853\n",
      "[2, 200]: loss: 0.035855534233851356\n",
      "[2, 400]: loss: 0.03924962564604357\n",
      "[3, 200]: loss: 0.01800406094727805\n",
      "[3, 400]: loss: 0.015102029864210635\n",
      "[4, 200]: loss: 0.008737145646882709\n",
      "[4, 400]: loss: 0.012841163712146227\n",
      "[5, 200]: loss: 0.0143239842512412\n",
      "[5, 400]: loss: 0.03538825181662105\n",
      "[6, 200]: loss: 0.02804117204155773\n",
      "[6, 400]: loss: 0.01283674875390716\n",
      "[7, 200]: loss: 0.005376850694883615\n",
      "[7, 400]: loss: 0.005895031294494401\n",
      "[8, 200]: loss: 0.003943614400050137\n",
      "[8, 400]: loss: 0.004426810824079439\n",
      "[9, 200]: loss: 0.0036425141112704294\n",
      "[9, 400]: loss: 0.002770821482554311\n",
      "[10, 200]: loss: 0.002455508151469985\n",
      "[10, 400]: loss: 0.002658608347992413\n",
      "Finished Training\n",
      "Test Accuracy after iteration 25: 0.7575\n",
      "Iteration 26\n",
      "[1, 200]: loss: 0.08840557318995707\n",
      "[1, 400]: loss: 0.0999563062377274\n",
      "[2, 200]: loss: 0.028078339553903788\n",
      "[2, 400]: loss: 0.028629314193385654\n",
      "[3, 200]: loss: 0.016113817177538296\n",
      "[3, 400]: loss: 0.01526795788668096\n",
      "[4, 200]: loss: 0.009247572740714532\n",
      "[4, 400]: loss: 0.01581829845206812\n",
      "[5, 200]: loss: 0.018828996394877322\n",
      "[5, 400]: loss: 0.03660612672916613\n",
      "[6, 200]: loss: 0.023644456254551187\n",
      "[6, 400]: loss: 0.013689263708947692\n",
      "[7, 200]: loss: 0.005530755216022953\n",
      "[7, 400]: loss: 0.005344437936291797\n",
      "[8, 200]: loss: 0.004107239922013832\n",
      "[8, 400]: loss: 0.003384958858950995\n",
      "[9, 200]: loss: 0.0028688361498643645\n",
      "[9, 400]: loss: 0.00278783499947167\n",
      "[10, 200]: loss: 0.0022760807574377395\n",
      "[10, 400]: loss: 0.002368607110256562\n",
      "Finished Training\n",
      "Test Accuracy after iteration 26: 0.7594\n",
      "Iteration 27\n",
      "[1, 200]: loss: 0.07994818107719766\n",
      "[1, 400]: loss: 0.08661712895845994\n",
      "[2, 200]: loss: 0.029503110177465715\n",
      "[2, 400]: loss: 0.027316253922181205\n",
      "[3, 200]: loss: 0.013526358695235103\n",
      "[3, 400]: loss: 0.016434051664255094\n",
      "[4, 200]: loss: 0.012636135966167784\n",
      "[4, 400]: loss: 0.016304593437234872\n",
      "[5, 200]: loss: 0.022874403865425847\n",
      "[5, 400]: loss: 0.027590168261667714\n",
      "[6, 200]: loss: 0.015921207617211622\n",
      "[6, 400]: loss: 0.0098001986372401\n",
      "[7, 200]: loss: 0.005057528045435902\n",
      "[7, 400]: loss: 0.005198060662660282\n",
      "[8, 200]: loss: 0.003503222716826713\n",
      "[8, 400]: loss: 0.0036018651763151865\n",
      "[9, 200]: loss: 0.0024411203752970323\n",
      "[9, 400]: loss: 0.0029038891654636243\n",
      "[10, 200]: loss: 0.0020739817302091977\n",
      "[10, 400]: loss: 0.002534255360660609\n",
      "Finished Training\n",
      "Test Accuracy after iteration 27: 0.7627\n",
      "Iteration 28\n",
      "[1, 200]: loss: 0.0803581405559089\n",
      "[1, 400]: loss: 0.0827472993079573\n",
      "[2, 200]: loss: 0.030183368695143143\n",
      "[2, 400]: loss: 0.02131841123336926\n",
      "[3, 200]: loss: 0.02589406278450042\n",
      "[3, 400]: loss: 0.018647587563609704\n",
      "[4, 200]: loss: 0.00861140233464539\n",
      "[4, 400]: loss: 0.011192808917112416\n",
      "[5, 200]: loss: 0.01809611950739054\n",
      "[5, 400]: loss: 0.020289895341556986\n",
      "[6, 200]: loss: 0.014319399620580953\n",
      "[6, 400]: loss: 0.00853368710493669\n",
      "[7, 200]: loss: 0.004099919396539917\n",
      "[7, 400]: loss: 0.004311268775491044\n",
      "[8, 200]: loss: 0.003387283989286516\n",
      "[8, 400]: loss: 0.003566684320976492\n",
      "[9, 200]: loss: 0.0032765241886954753\n",
      "[9, 400]: loss: 0.0028938922769157216\n",
      "[10, 200]: loss: 0.002936705755273579\n",
      "[10, 400]: loss: 0.002592347437457647\n",
      "Finished Training\n",
      "Test Accuracy after iteration 28: 0.7624\n",
      "Iteration 29\n",
      "[1, 200]: loss: 0.0639320360304555\n",
      "[1, 400]: loss: 0.07310475561302156\n",
      "[2, 200]: loss: 0.023037961113732307\n",
      "[2, 400]: loss: 0.021932536149397493\n",
      "[3, 200]: loss: 0.015515037786390167\n",
      "[3, 400]: loss: 0.015996025771310086\n",
      "[4, 200]: loss: 0.014136679245712002\n",
      "[4, 400]: loss: 0.01733946528431261\n",
      "[5, 200]: loss: 0.02047541157633532\n",
      "[5, 400]: loss: 0.028693771577381996\n",
      "[6, 200]: loss: 0.015937196347804276\n",
      "[6, 400]: loss: 0.010004032467113575\n",
      "[7, 200]: loss: 0.0050187079815077595\n",
      "[7, 400]: loss: 0.005216842757363338\n",
      "[8, 200]: loss: 0.0031752133615373167\n",
      "[8, 400]: loss: 0.003365377945447108\n",
      "[9, 200]: loss: 0.0022648195817600936\n",
      "[9, 400]: loss: 0.002696494694391731\n",
      "[10, 200]: loss: 0.0023956107125559355\n",
      "[10, 400]: loss: 0.0019609906805999346\n",
      "Finished Training\n",
      "Test Accuracy after iteration 29: 0.7625\n",
      "Iteration 30\n",
      "[1, 200]: loss: 0.04720312907069456\n",
      "[1, 400]: loss: 0.06811455728719011\n",
      "[2, 200]: loss: 0.020981871059047988\n",
      "[2, 400]: loss: 0.02483163559692912\n",
      "[3, 200]: loss: 0.013868169493798632\n",
      "[3, 400]: loss: 0.019919198874849827\n",
      "[4, 200]: loss: 0.01647429697404732\n",
      "[4, 400]: loss: 0.020046043115726206\n",
      "[5, 200]: loss: 0.028517305370769463\n",
      "[5, 400]: loss: 0.028402006859541872\n",
      "[6, 200]: loss: 0.013901265386230079\n",
      "[6, 400]: loss: 0.008360815919586458\n",
      "[7, 200]: loss: 0.004771215560467681\n",
      "[7, 400]: loss: 0.004484931002079975\n",
      "[8, 200]: loss: 0.00322122302299249\n",
      "[8, 400]: loss: 0.002562770686854492\n",
      "[9, 200]: loss: 0.002071589821061934\n",
      "[9, 400]: loss: 0.00205600818626408\n",
      "[10, 200]: loss: 0.001716927805537125\n",
      "[10, 400]: loss: 0.002192779748729663\n",
      "Finished Training\n",
      "Test Accuracy after iteration 30: 0.7652\n",
      "Iteration 31\n",
      "[1, 200]: loss: 0.046434908081428146\n",
      "[1, 400]: loss: 0.05699709957378218\n",
      "[2, 200]: loss: 0.020475384276651312\n",
      "[2, 400]: loss: 0.018872005642042496\n",
      "[3, 200]: loss: 0.0131539211657946\n",
      "[3, 400]: loss: 0.015506709654000589\n",
      "[4, 200]: loss: 0.011014955473074224\n",
      "[4, 400]: loss: 0.013478924381488469\n",
      "[5, 200]: loss: 0.020689871473878158\n",
      "[5, 400]: loss: 0.033606810595374556\n",
      "[6, 200]: loss: 0.014251969786273548\n",
      "[6, 400]: loss: 0.009379733467212645\n",
      "[7, 200]: loss: 0.004272624443256063\n",
      "[7, 400]: loss: 0.0045622013081447225\n",
      "[8, 200]: loss: 0.0036875373708608094\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "# Instantiate the enhanced model\n",
    "model = EnhancedCNN()\n",
    "\n",
    "# Reset the initial dataset split\n",
    "trainset_labeled_indices = list(range(initial_train_size))\n",
    "trainset_unlabeled_indices = list(range(initial_train_size, len(trainset)))\n",
    "\n",
    "# Perform iterative training and sampling\n",
    "for iteration in range(49):  # Adjust iterations as needed\n",
    "    print(f\"Iteration {iteration+1}\")\n",
    "    \n",
    "    # Prepare the DataLoader for the unlabeled dataset\n",
    "    unlabeled_set = Subset(trainset, trainset_unlabeled_indices)\n",
    "    unlabeled_loader = DataLoader(unlabeled_set, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Uncertainty Sampling\n",
    "    uncertain_indices = uncertainty_sampling(model, unlabeled_loader, n=1000)\n",
    "    \n",
    "    # Update datasets\n",
    "    new_indices = [trainset_unlabeled_indices[idx] for idx in uncertain_indices]\n",
    "    trainset_labeled_indices.extend(new_indices)\n",
    "    trainset_unlabeled_indices = [idx for idx in trainset_unlabeled_indices if idx not in new_indices]\n",
    "    \n",
    "    labeled_set = Subset(trainset, trainset_labeled_indices)\n",
    "    trainloader = DataLoader(labeled_set, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Retrain the model\n",
    "    train_model(model, trainloader, epochs=10)  # Increase epochs for deeper learning\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = evaluate_model(model, testloader)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Test Accuracy after iteration {iteration+1}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the model for the Random Sampling experiment\n",
    "model_random = EnhancedCNN()\n",
    "\n",
    "# Reset the initial dataset split\n",
    "trainset_labeled_indices_random = list(range(initial_train_size))\n",
    "trainset_unlabeled_indices_random = list(range(initial_train_size, len(trainset)))\n",
    "\n",
    "# Initialize the labeled and unlabeled sets for Random Sampling\n",
    "labeled_set_random = Subset(trainset, trainset_labeled_indices_random)\n",
    "unlabeled_set_random = Subset(trainset, trainset_unlabeled_indices_random)\n",
    "\n",
    "# Initialize the DataLoader for the labeled dataset\n",
    "trainloader_random = DataLoader(labeled_set_random, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sampling Iteration 1\n",
      "[1, 5]: loss: 3.1066638827323914\n",
      "[1, 10]: loss: 2.513816177845001\n",
      "[1, 15]: loss: 2.3575871884822845\n",
      "[1, 20]: loss: 2.6919004023075104\n",
      "[1, 25]: loss: 2.4309937357902527\n",
      "[1, 30]: loss: 2.405039459466934\n",
      "[1, 35]: loss: 2.0817820727825165\n",
      "[2, 5]: loss: 1.6549124419689178\n",
      "[2, 10]: loss: 1.7085617184638977\n",
      "[2, 15]: loss: 1.698606699705124\n",
      "[2, 20]: loss: 1.6984830796718597\n",
      "[2, 25]: loss: 2.0729644894599915\n",
      "[2, 30]: loss: 1.601035237312317\n",
      "[2, 35]: loss: 1.6433078348636627\n",
      "[3, 5]: loss: 1.2065377086400986\n",
      "[3, 10]: loss: 1.0092443525791168\n",
      "[3, 15]: loss: 1.2187486737966537\n",
      "[3, 20]: loss: 1.106302872300148\n",
      "[3, 25]: loss: 1.2517579048871994\n",
      "[3, 30]: loss: 1.1668940037488937\n",
      "[3, 35]: loss: 1.553945779800415\n",
      "[4, 5]: loss: 0.8060212880373001\n",
      "[4, 10]: loss: 0.9652696996927261\n",
      "[4, 15]: loss: 0.9415095448493958\n",
      "[4, 20]: loss: 1.000303253531456\n",
      "[4, 25]: loss: 0.7310764491558075\n",
      "[4, 30]: loss: 0.8862002342939377\n",
      "[4, 35]: loss: 0.6744445264339447\n",
      "[5, 5]: loss: 0.5456276349723339\n",
      "[5, 10]: loss: 0.4470067173242569\n",
      "[5, 15]: loss: 0.4019671007990837\n",
      "[5, 20]: loss: 0.4773090109229088\n",
      "[5, 25]: loss: 0.3853098750114441\n",
      "[5, 30]: loss: 0.44189490377902985\n",
      "[5, 35]: loss: 0.45785096287727356\n",
      "[6, 5]: loss: 0.30000874027609825\n",
      "[6, 10]: loss: 0.19222015887498856\n",
      "[6, 15]: loss: 0.2502557374536991\n",
      "[6, 20]: loss: 0.1882893079891801\n",
      "[6, 25]: loss: 0.23516910523176193\n",
      "[6, 30]: loss: 0.16719290055334568\n",
      "[6, 35]: loss: 0.197842538356781\n",
      "[7, 5]: loss: 0.11698322556912899\n",
      "[7, 10]: loss: 0.1793951839208603\n",
      "[7, 15]: loss: 0.14965984970331192\n",
      "[7, 20]: loss: 0.1109792236238718\n",
      "[7, 25]: loss: 0.13404501415789127\n",
      "[7, 30]: loss: 0.10687010362744331\n",
      "[7, 35]: loss: 0.09933578036725521\n",
      "[8, 5]: loss: 0.1589369811117649\n",
      "[8, 10]: loss: 0.12520223204046488\n",
      "[8, 15]: loss: 0.13389602024108171\n",
      "[8, 20]: loss: 0.0967279439792037\n",
      "[8, 25]: loss: 0.10168739315122366\n",
      "[8, 30]: loss: 0.13377142790704966\n",
      "[8, 35]: loss: 0.10945446044206619\n",
      "[9, 5]: loss: 0.08168148994445801\n",
      "[9, 10]: loss: 0.1132342666387558\n",
      "[9, 15]: loss: 0.10189467575401068\n",
      "[9, 20]: loss: 0.08310231473296881\n",
      "[9, 25]: loss: 0.0897788517177105\n",
      "[9, 30]: loss: 0.08509107772260904\n",
      "[9, 35]: loss: 0.09831253997981548\n",
      "[10, 5]: loss: 0.090900675393641\n",
      "[10, 10]: loss: 0.08156018890440464\n",
      "[10, 15]: loss: 0.0623060530051589\n",
      "[10, 20]: loss: 0.10609177779406309\n",
      "[10, 25]: loss: 0.10451631806790829\n",
      "[10, 30]: loss: 0.10362534923478961\n",
      "[10, 35]: loss: 0.09389107208698988\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 1: 0.4566\n",
      "Random Sampling Iteration 2\n",
      "[1, 5]: loss: 0.6465659737586975\n",
      "[1, 10]: loss: 0.6703325621783733\n",
      "[1, 15]: loss: 0.41634925082325935\n",
      "[1, 20]: loss: 0.9042265936732292\n",
      "[1, 25]: loss: 0.8045187070965767\n",
      "[1, 30]: loss: 0.8067782297730446\n",
      "[1, 35]: loss: 0.9012043625116348\n",
      "[1, 40]: loss: 0.9578942060470581\n",
      "[2, 5]: loss: 0.28823982179164886\n",
      "[2, 10]: loss: 0.4019746966660023\n",
      "[2, 15]: loss: 0.38230327144265175\n",
      "[2, 20]: loss: 0.280238825827837\n",
      "[2, 25]: loss: 0.26895489916205406\n",
      "[2, 30]: loss: 0.3727375715970993\n",
      "[2, 35]: loss: 0.3270057775080204\n",
      "[2, 40]: loss: 0.2035206500440836\n",
      "[3, 5]: loss: 0.16345576383173466\n",
      "[3, 10]: loss: 0.12545637041330338\n",
      "[3, 15]: loss: 0.3263075463473797\n",
      "[3, 20]: loss: 0.12320380099117756\n",
      "[3, 25]: loss: 0.12244938220828772\n",
      "[3, 30]: loss: 0.21283720154315233\n",
      "[3, 35]: loss: 0.24170358665287495\n",
      "[3, 40]: loss: 0.1611810065805912\n",
      "[4, 5]: loss: 0.16284117009490728\n",
      "[4, 10]: loss: 0.08324520569294691\n",
      "[4, 15]: loss: 0.12454862799495459\n",
      "[4, 20]: loss: 0.10878644976764917\n",
      "[4, 25]: loss: 0.07086034584790468\n",
      "[4, 30]: loss: 0.05470221024006605\n",
      "[4, 35]: loss: 0.10783067066222429\n",
      "[4, 40]: loss: 0.07756340922787786\n",
      "[5, 5]: loss: 0.04052153415977955\n",
      "[5, 10]: loss: 0.04074041498824954\n",
      "[5, 15]: loss: 0.08439682982861996\n",
      "[5, 20]: loss: 0.05118787894025445\n",
      "[5, 25]: loss: 0.04035844141617417\n",
      "[5, 30]: loss: 0.047531799180433154\n",
      "[5, 35]: loss: 0.0869550269562751\n",
      "[5, 40]: loss: 0.09189771441742778\n",
      "[6, 5]: loss: 0.02486883569508791\n",
      "[6, 10]: loss: 0.03762170812115073\n",
      "[6, 15]: loss: 0.033123506931588054\n",
      "[6, 20]: loss: 0.03527966351248324\n",
      "[6, 25]: loss: 0.030716917477548122\n",
      "[6, 30]: loss: 0.03201280836947262\n",
      "[6, 35]: loss: 0.04274640651419759\n",
      "[6, 40]: loss: 0.020499904174357653\n",
      "[7, 5]: loss: 0.019263698486611247\n",
      "[7, 10]: loss: 0.032119281706400216\n",
      "[7, 15]: loss: 0.01816889946348965\n",
      "[7, 20]: loss: 0.029014392115641385\n",
      "[7, 25]: loss: 0.03952647466212511\n",
      "[7, 30]: loss: 0.0332347322255373\n",
      "[7, 35]: loss: 0.04061073227785528\n",
      "[7, 40]: loss: 0.019497199915349483\n",
      "[8, 5]: loss: 0.0300280109513551\n",
      "[8, 10]: loss: 0.016604948206804693\n",
      "[8, 15]: loss: 0.02132575074210763\n",
      "[8, 20]: loss: 0.025234912987798452\n",
      "[8, 25]: loss: 0.012340239365585148\n",
      "[8, 30]: loss: 0.0245961343171075\n",
      "[8, 35]: loss: 0.019246125943027437\n",
      "[8, 40]: loss: 0.017984863719902933\n",
      "[9, 5]: loss: 0.02035683114081621\n",
      "[9, 10]: loss: 0.024415856460109353\n",
      "[9, 15]: loss: 0.014583062613382936\n",
      "[9, 20]: loss: 0.024240055703558028\n",
      "[9, 25]: loss: 0.03181522758677602\n",
      "[9, 30]: loss: 0.022513430332764983\n",
      "[9, 35]: loss: 0.021933726500719786\n",
      "[9, 40]: loss: 0.01989473623689264\n",
      "[10, 5]: loss: 0.012213244917802513\n",
      "[10, 10]: loss: 0.01624926063232124\n",
      "[10, 15]: loss: 0.031232646433636546\n",
      "[10, 20]: loss: 0.013143612304702401\n",
      "[10, 25]: loss: 0.017172586754895747\n",
      "[10, 30]: loss: 0.02075272158253938\n",
      "[10, 35]: loss: 0.01428120059426874\n",
      "[10, 40]: loss: 0.01563523488584906\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 2: 0.4536\n",
      "Random Sampling Iteration 3\n",
      "[1, 5]: loss: 0.4858078588731587\n",
      "[1, 10]: loss: 0.29784531611949205\n",
      "[1, 15]: loss: 0.4986884221434593\n",
      "[1, 20]: loss: 0.57118309289217\n",
      "[1, 25]: loss: 0.8790514636784792\n",
      "[1, 30]: loss: 0.9021303206682205\n",
      "[1, 35]: loss: 0.5946025289595127\n",
      "[1, 40]: loss: 0.8354938849806786\n",
      "[1, 45]: loss: 1.1441284194588661\n",
      "[1, 50]: loss: 0.8553612679243088\n",
      "[2, 5]: loss: 0.2912637237459421\n",
      "[2, 10]: loss: 0.40518709272146225\n",
      "[2, 15]: loss: 0.48931656777858734\n",
      "[2, 20]: loss: 0.296719565987587\n",
      "[2, 25]: loss: 0.232038676738739\n",
      "[2, 30]: loss: 0.26236392557621\n",
      "[2, 35]: loss: 0.11420992203056812\n",
      "[2, 40]: loss: 0.15476911514997482\n",
      "[2, 45]: loss: 0.1783947767689824\n",
      "[2, 50]: loss: 0.21780446730554104\n",
      "[3, 5]: loss: 0.09905376890674233\n",
      "[3, 10]: loss: 0.09355211351066828\n",
      "[3, 15]: loss: 0.08321044128388166\n",
      "[3, 20]: loss: 0.10586932767182589\n",
      "[3, 25]: loss: 0.07265552785247564\n",
      "[3, 30]: loss: 0.08318565832450986\n",
      "[3, 35]: loss: 0.05466727539896965\n",
      "[3, 40]: loss: 0.08531470503658056\n",
      "[3, 45]: loss: 0.06574796978384256\n",
      "[3, 50]: loss: 0.10802785074338317\n",
      "[4, 5]: loss: 0.052757634315639734\n",
      "[4, 10]: loss: 0.025961763691157103\n",
      "[4, 15]: loss: 0.032726719276979566\n",
      "[4, 20]: loss: 0.023749760468490422\n",
      "[4, 25]: loss: 0.06086543225683272\n",
      "[4, 30]: loss: 0.04938206821680069\n",
      "[4, 35]: loss: 0.06249661953188479\n",
      "[4, 40]: loss: 0.034781494876369834\n",
      "[4, 45]: loss: 0.024373303400352597\n",
      "[4, 50]: loss: 0.023148498265072703\n",
      "[5, 5]: loss: 0.05856394057627767\n",
      "[5, 10]: loss: 0.047810844727791846\n",
      "[5, 15]: loss: 0.05273011373355985\n",
      "[5, 20]: loss: 0.014611427090130746\n",
      "[5, 25]: loss: 0.025923602050170302\n",
      "[5, 30]: loss: 0.04131648410111666\n",
      "[5, 35]: loss: 0.053048993926495314\n",
      "[5, 40]: loss: 0.03340979968197644\n",
      "[5, 45]: loss: 0.03206476604100317\n",
      "[5, 50]: loss: 0.01359865558333695\n",
      "[6, 5]: loss: 0.02190560183953494\n",
      "[6, 10]: loss: 0.04593170335283503\n",
      "[6, 15]: loss: 0.012520224670879543\n",
      "[6, 20]: loss: 0.012881917122285813\n",
      "[6, 25]: loss: 0.020343893207609653\n",
      "[6, 30]: loss: 0.021357913268730044\n",
      "[6, 35]: loss: 0.03516520536504686\n",
      "[6, 40]: loss: 0.019148343126289546\n",
      "[6, 45]: loss: 0.02377290476579219\n",
      "[6, 50]: loss: 0.009207713534124196\n",
      "[7, 5]: loss: 0.0096492730663158\n",
      "[7, 10]: loss: 0.014294686901848763\n",
      "[7, 15]: loss: 0.013507356226909906\n",
      "[7, 20]: loss: 0.020833365502767265\n",
      "[7, 25]: loss: 0.014546640682965517\n",
      "[7, 30]: loss: 0.012872403545770794\n",
      "[7, 35]: loss: 0.014358949207235128\n",
      "[7, 40]: loss: 0.011941727017983794\n",
      "[7, 45]: loss: 0.017816951585700735\n",
      "[7, 50]: loss: 0.007785095775034279\n",
      "[8, 5]: loss: 0.009213067358359694\n",
      "[8, 10]: loss: 0.023760683834552765\n",
      "[8, 15]: loss: 0.013298451958689839\n",
      "[8, 20]: loss: 0.00987777259433642\n",
      "[8, 25]: loss: 0.060996123822405934\n",
      "[8, 30]: loss: 0.017875842633657157\n",
      "[8, 35]: loss: 0.008257722249254584\n",
      "[8, 40]: loss: 0.020303356635849923\n",
      "[8, 45]: loss: 0.011097489099483937\n",
      "[8, 50]: loss: 0.007650437706615776\n",
      "[9, 5]: loss: 0.009324327867943794\n",
      "[9, 10]: loss: 0.010983556392602623\n",
      "[9, 15]: loss: 0.016452727606520057\n",
      "[9, 20]: loss: 0.008025958086363971\n",
      "[9, 25]: loss: 0.007518012949731201\n",
      "[9, 30]: loss: 0.012690963572822511\n",
      "[9, 35]: loss: 0.00734564452432096\n",
      "[9, 40]: loss: 0.009837870573392138\n",
      "[9, 45]: loss: 0.006109026609919965\n",
      "[9, 50]: loss: 0.008688470697961748\n",
      "[10, 5]: loss: 0.007846398424590006\n",
      "[10, 10]: loss: 0.008879152941517532\n",
      "[10, 15]: loss: 0.008052067423705012\n",
      "[10, 20]: loss: 0.012097878905478865\n",
      "[10, 25]: loss: 0.008569581201300025\n",
      "[10, 30]: loss: 0.006169022817630321\n",
      "[10, 35]: loss: 0.009485210699494928\n",
      "[10, 40]: loss: 0.008471011300571263\n",
      "[10, 45]: loss: 0.01914913806831464\n",
      "[10, 50]: loss: 0.014782199403271079\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 3: 0.4581\n",
      "Random Sampling Iteration 4\n",
      "[1, 5]: loss: 0.5192819416697603\n",
      "[1, 10]: loss: 0.5381820574402809\n",
      "[1, 15]: loss: 0.4542272686958313\n",
      "[1, 20]: loss: 0.5055947080254555\n",
      "[1, 25]: loss: 0.6332785375416279\n",
      "[1, 30]: loss: 0.8166861087083817\n",
      "[1, 35]: loss: 0.7092838883399963\n",
      "[1, 40]: loss: 0.5193939208984375\n",
      "[1, 45]: loss: 0.5323139280080795\n",
      "[1, 50]: loss: 0.7268098443746567\n",
      "[1, 55]: loss: 0.6304125227034092\n",
      "[2, 5]: loss: 0.1955482605844736\n",
      "[2, 10]: loss: 0.12448978051543236\n",
      "[2, 15]: loss: 0.22273785807192326\n",
      "[2, 20]: loss: 0.15149697568267584\n",
      "[2, 25]: loss: 0.12641119863837957\n",
      "[2, 30]: loss: 0.2502298280596733\n",
      "[2, 35]: loss: 0.18780158180743456\n",
      "[2, 40]: loss: 0.26782261580228806\n",
      "[2, 45]: loss: 0.30007071048021317\n",
      "[2, 50]: loss: 0.1285974057391286\n",
      "[2, 55]: loss: 0.15667624678462744\n",
      "[3, 5]: loss: 0.06356092542409897\n",
      "[3, 10]: loss: 0.17804084718227386\n",
      "[3, 15]: loss: 0.2496757209300995\n",
      "[3, 20]: loss: 0.17054854985326529\n",
      "[3, 25]: loss: 0.07311921194195747\n",
      "[3, 30]: loss: 0.17086104210466146\n",
      "[3, 35]: loss: 0.05950678186491132\n",
      "[3, 40]: loss: 0.10203683376312256\n",
      "[3, 45]: loss: 0.09260546625591815\n",
      "[3, 50]: loss: 0.06357355695217848\n",
      "[3, 55]: loss: 0.11992599815130234\n",
      "[4, 5]: loss: 0.08532788953743875\n",
      "[4, 10]: loss: 0.110561428591609\n",
      "[4, 15]: loss: 0.08679491793736815\n",
      "[4, 20]: loss: 0.08471736032515764\n",
      "[4, 25]: loss: 0.1155552938580513\n",
      "[4, 30]: loss: 0.1292396318167448\n",
      "[4, 35]: loss: 0.16630861721932888\n",
      "[4, 40]: loss: 0.11716144066303968\n",
      "[4, 45]: loss: 0.19773551728576422\n",
      "[4, 50]: loss: 0.09667968144640326\n",
      "[4, 55]: loss: 0.16036662785336375\n",
      "[5, 5]: loss: 0.08767860336229205\n",
      "[5, 10]: loss: 0.12147512100636959\n",
      "[5, 15]: loss: 0.20556901395320892\n",
      "[5, 20]: loss: 0.13037753477692604\n",
      "[5, 25]: loss: 0.10257382970303297\n",
      "[5, 30]: loss: 0.17570463754236698\n",
      "[5, 35]: loss: 0.1943542119115591\n",
      "[5, 40]: loss: 0.19034641422331333\n",
      "[5, 45]: loss: 0.12124744895845652\n",
      "[5, 50]: loss: 0.11679490888491273\n",
      "[5, 55]: loss: 0.12891923543065786\n",
      "[6, 5]: loss: 0.10480067692697048\n",
      "[6, 10]: loss: 0.10074290726333857\n",
      "[6, 15]: loss: 0.057999348267912865\n",
      "[6, 20]: loss: 0.05475401214789599\n",
      "[6, 25]: loss: 0.04498848458752036\n",
      "[6, 30]: loss: 0.11736227478832006\n",
      "[6, 35]: loss: 0.03817697451449931\n",
      "[6, 40]: loss: 0.07396208867430687\n",
      "[6, 45]: loss: 0.04720602463930845\n",
      "[6, 50]: loss: 0.04521879320964217\n",
      "[6, 55]: loss: 0.048017475521191955\n",
      "[7, 5]: loss: 0.025956285069696605\n",
      "[7, 10]: loss: 0.016223411192186177\n",
      "[7, 15]: loss: 0.030233836034312844\n",
      "[7, 20]: loss: 0.02794691186863929\n",
      "[7, 25]: loss: 0.02940227324143052\n",
      "[7, 30]: loss: 0.02158970176242292\n",
      "[7, 35]: loss: 0.03597893752157688\n",
      "[7, 40]: loss: 0.05643826286541298\n",
      "[7, 45]: loss: 0.014680106192827225\n",
      "[7, 50]: loss: 0.05111920856870711\n",
      "[7, 55]: loss: 0.03848558710888028\n",
      "[8, 5]: loss: 0.024303453043103218\n",
      "[8, 10]: loss: 0.013343088096007705\n",
      "[8, 15]: loss: 0.022751971147954464\n",
      "[8, 20]: loss: 0.019450533902272582\n",
      "[8, 25]: loss: 0.025664045242592692\n",
      "[8, 30]: loss: 0.033167790388688445\n",
      "[8, 35]: loss: 0.008138185483403504\n",
      "[8, 40]: loss: 0.01577708381228149\n",
      "[8, 45]: loss: 0.016154675744473934\n",
      "[8, 50]: loss: 0.026151717407628894\n",
      "[8, 55]: loss: 0.020922231138683856\n",
      "[9, 5]: loss: 0.022845749626867473\n",
      "[9, 10]: loss: 0.018680394161492586\n",
      "[9, 15]: loss: 0.05203799076844007\n",
      "[9, 20]: loss: 0.028245264082215726\n",
      "[9, 25]: loss: 0.023178252391517162\n",
      "[9, 30]: loss: 0.017261942441109568\n",
      "[9, 35]: loss: 0.02467205550055951\n",
      "[9, 40]: loss: 0.016795082774478942\n",
      "[9, 45]: loss: 0.013344038859941065\n",
      "[9, 50]: loss: 0.022536262637004256\n",
      "[9, 55]: loss: 0.03949213225860149\n",
      "[10, 5]: loss: 0.0172579288482666\n",
      "[10, 10]: loss: 0.011536723643075675\n",
      "[10, 15]: loss: 0.016784629318863153\n",
      "[10, 20]: loss: 0.016277338960208\n",
      "[10, 25]: loss: 0.022742830333299935\n",
      "[10, 30]: loss: 0.010308979428373277\n",
      "[10, 35]: loss: 0.010790766711579636\n",
      "[10, 40]: loss: 0.016361085581593215\n",
      "[10, 45]: loss: 0.011428350466303527\n",
      "[10, 50]: loss: 0.007917756913229823\n",
      "[10, 55]: loss: 0.018218381330370903\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 4: 0.462\n",
      "Random Sampling Iteration 5\n",
      "[1, 5]: loss: 0.21360039268620312\n",
      "[1, 10]: loss: 0.7096702884882689\n",
      "[1, 15]: loss: 0.5381525848060846\n",
      "[1, 20]: loss: 0.4476804817095399\n",
      "[1, 25]: loss: 0.816968634724617\n",
      "[1, 30]: loss: 0.30476122815161943\n",
      "[1, 35]: loss: 0.8737424984574318\n",
      "[1, 40]: loss: 0.44471510872244835\n",
      "[1, 45]: loss: 0.6609750017523766\n",
      "[1, 50]: loss: 0.5404256824404001\n",
      "[1, 55]: loss: 0.3889463050290942\n",
      "[1, 60]: loss: 0.289850614964962\n",
      "[2, 5]: loss: 0.2299612294882536\n",
      "[2, 10]: loss: 0.06531876418739557\n",
      "[2, 15]: loss: 0.18382005207240582\n",
      "[2, 20]: loss: 0.2841895530000329\n",
      "[2, 25]: loss: 0.21209165174514055\n",
      "[2, 30]: loss: 0.10249117901548743\n",
      "[2, 35]: loss: 0.1420926321297884\n",
      "[2, 40]: loss: 0.1290170243009925\n",
      "[2, 45]: loss: 0.0824998402968049\n",
      "[2, 50]: loss: 0.15512068755924702\n",
      "[2, 55]: loss: 0.13298642449080944\n",
      "[2, 60]: loss: 0.16595551138743758\n",
      "[3, 5]: loss: 0.057828047196380794\n",
      "[3, 10]: loss: 0.05982681130990386\n",
      "[3, 15]: loss: 0.04183549527078867\n",
      "[3, 20]: loss: 0.05361060472205281\n",
      "[3, 25]: loss: 0.03400326264090836\n",
      "[3, 30]: loss: 0.01760802313219756\n",
      "[3, 35]: loss: 0.04875742632430047\n",
      "[3, 40]: loss: 0.029726371867582202\n",
      "[3, 45]: loss: 0.05295702745206654\n",
      "[3, 50]: loss: 0.05786947347223759\n",
      "[3, 55]: loss: 0.028223367873579264\n",
      "[3, 60]: loss: 0.026465509086847305\n",
      "[4, 5]: loss: 0.03317797649651766\n",
      "[4, 10]: loss: 0.021999355289153755\n",
      "[4, 15]: loss: 0.040548212826251984\n",
      "[4, 20]: loss: 0.037749792682006955\n",
      "[4, 25]: loss: 0.034772640792652965\n",
      "[4, 30]: loss: 0.01910306396894157\n",
      "[4, 35]: loss: 0.010996730532497168\n",
      "[4, 40]: loss: 0.05441658588824794\n",
      "[4, 45]: loss: 0.03229084215126932\n",
      "[4, 50]: loss: 0.03709498874377459\n",
      "[4, 55]: loss: 0.02336567360907793\n",
      "[4, 60]: loss: 0.015808666008524597\n",
      "[5, 5]: loss: 0.01711907284334302\n",
      "[5, 10]: loss: 0.011514739890117198\n",
      "[5, 15]: loss: 0.016299828595947474\n",
      "[5, 20]: loss: 0.06494509743060917\n",
      "[5, 25]: loss: 0.01166966778691858\n",
      "[5, 30]: loss: 0.020871536573395133\n",
      "[5, 35]: loss: 0.06330327491741627\n",
      "[5, 40]: loss: 0.026437227032147348\n",
      "[5, 45]: loss: 0.02072046638932079\n",
      "[5, 50]: loss: 0.029992493567988276\n",
      "[5, 55]: loss: 0.027723598293960094\n",
      "[5, 60]: loss: 0.04471058846684173\n",
      "[6, 5]: loss: 0.016114782192744315\n",
      "[6, 10]: loss: 0.013867096975445747\n",
      "[6, 15]: loss: 0.008040189452003688\n",
      "[6, 20]: loss: 0.030472538201138377\n",
      "[6, 25]: loss: 0.06064218666870147\n",
      "[6, 30]: loss: 0.1031940637039952\n",
      "[6, 35]: loss: 0.04930882033659145\n",
      "[6, 40]: loss: 0.019419335294514894\n",
      "[6, 45]: loss: 0.013809290830977261\n",
      "[6, 50]: loss: 0.02094021235825494\n",
      "[6, 55]: loss: 0.007835454598534852\n",
      "[6, 60]: loss: 0.033738761383574456\n",
      "[7, 5]: loss: 0.013469468394760042\n",
      "[7, 10]: loss: 0.00802700244821608\n",
      "[7, 15]: loss: 0.011690140061546117\n",
      "[7, 20]: loss: 0.011907124135177583\n",
      "[7, 25]: loss: 0.017872902913950384\n",
      "[7, 30]: loss: 0.008955091470852494\n",
      "[7, 35]: loss: 0.004843686649110168\n",
      "[7, 40]: loss: 0.017724227014696226\n",
      "[7, 45]: loss: 0.007924033619929105\n",
      "[7, 50]: loss: 0.011984277749434114\n",
      "[7, 55]: loss: 0.006030241842381656\n",
      "[7, 60]: loss: 0.0058246845728717744\n",
      "[8, 5]: loss: 0.017544420843478292\n",
      "[8, 10]: loss: 0.037785038352012634\n",
      "[8, 15]: loss: 0.011867149558383971\n",
      "[8, 20]: loss: 0.005934594169957563\n",
      "[8, 25]: loss: 0.01232089672703296\n",
      "[8, 30]: loss: 0.0021835524530615658\n",
      "[8, 35]: loss: 0.008752965601161122\n",
      "[8, 40]: loss: 0.006051434902474284\n",
      "[8, 45]: loss: 0.006970360525883734\n",
      "[8, 50]: loss: 0.00538577965926379\n",
      "[8, 55]: loss: 0.011731136386515573\n",
      "[8, 60]: loss: 0.009644312201999128\n",
      "[9, 5]: loss: 0.00860339411883615\n",
      "[9, 10]: loss: 0.004039166436996311\n",
      "[9, 15]: loss: 0.008367618836928159\n",
      "[9, 20]: loss: 0.017491871665697545\n",
      "[9, 25]: loss: 0.011097733280621469\n",
      "[9, 30]: loss: 0.007728955941274762\n",
      "[9, 35]: loss: 0.007259315345436335\n",
      "[9, 40]: loss: 0.005307272222125903\n",
      "[9, 45]: loss: 0.005555879499297589\n",
      "[9, 50]: loss: 0.007376814464805648\n",
      "[9, 55]: loss: 0.006062228727387264\n",
      "[9, 60]: loss: 0.009603704005712643\n",
      "[10, 5]: loss: 0.0034983547520823777\n",
      "[10, 10]: loss: 0.014157776022329926\n",
      "[10, 15]: loss: 0.005600190022960305\n",
      "[10, 20]: loss: 0.008823619515169412\n",
      "[10, 25]: loss: 0.00680040882434696\n",
      "[10, 30]: loss: 0.005669813021086156\n",
      "[10, 35]: loss: 0.015359154436737299\n",
      "[10, 40]: loss: 0.006378880585543811\n",
      "[10, 45]: loss: 0.004875153594184667\n",
      "[10, 50]: loss: 0.0041153188503813\n",
      "[10, 55]: loss: 0.005863021535333246\n",
      "[10, 60]: loss: 0.008800568321021274\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 5: 0.4815\n",
      "Random Sampling Iteration 6\n",
      "[1, 5]: loss: 0.5347167169675231\n",
      "[1, 10]: loss: 0.2151671526953578\n",
      "[1, 15]: loss: 0.39002091996371746\n",
      "[1, 20]: loss: 0.3153309281915426\n",
      "[1, 25]: loss: 0.43948070891201496\n",
      "[1, 30]: loss: 0.35346006974577904\n",
      "[1, 35]: loss: 0.2768473415635526\n",
      "[1, 40]: loss: 0.31397963128983974\n",
      "[1, 45]: loss: 0.3645734703168273\n",
      "[1, 50]: loss: 0.660538237541914\n",
      "[1, 55]: loss: 0.5500407908111811\n",
      "[1, 60]: loss: 0.7562123388051987\n",
      "[1, 65]: loss: 0.2695544948801398\n",
      "[2, 5]: loss: 0.12466166354715824\n",
      "[2, 10]: loss: 0.36336443945765495\n",
      "[2, 15]: loss: 0.18666408210992813\n",
      "[2, 20]: loss: 0.07328545418567955\n",
      "[2, 25]: loss: 0.057969413697719574\n",
      "[2, 30]: loss: 0.09425048157572746\n",
      "[2, 35]: loss: 0.14754262659698725\n",
      "[2, 40]: loss: 0.10537506360560656\n",
      "[2, 45]: loss: 0.19692604267038405\n",
      "[2, 50]: loss: 0.0637864590389654\n",
      "[2, 55]: loss: 0.1320060221478343\n",
      "[2, 60]: loss: 0.07760571083053946\n",
      "[2, 65]: loss: 0.05549072055146098\n",
      "[3, 5]: loss: 0.014050890458747745\n",
      "[3, 10]: loss: 0.09451837185770273\n",
      "[3, 15]: loss: 0.0281829847372137\n",
      "[3, 20]: loss: 0.05257418076507747\n",
      "[3, 25]: loss: 0.021554284961894155\n",
      "[3, 30]: loss: 0.03248283197171986\n",
      "[3, 35]: loss: 0.06747805140912533\n",
      "[3, 40]: loss: 0.03910614171763882\n",
      "[3, 45]: loss: 0.02107971120858565\n",
      "[3, 50]: loss: 0.0701326834387146\n",
      "[3, 55]: loss: 0.01639169006375596\n",
      "[3, 60]: loss: 0.03890862362459302\n",
      "[3, 65]: loss: 0.022165575239341706\n",
      "[4, 5]: loss: 0.017535519087687135\n",
      "[4, 10]: loss: 0.009415986482053995\n",
      "[4, 15]: loss: 0.010779688134789467\n",
      "[4, 20]: loss: 0.012495834234869108\n",
      "[4, 25]: loss: 0.007861262827645987\n",
      "[4, 30]: loss: 0.014361395995365456\n",
      "[4, 35]: loss: 0.00871247606119141\n",
      "[4, 40]: loss: 0.020207335939630866\n",
      "[4, 45]: loss: 0.04140571330208331\n",
      "[4, 50]: loss: 0.016657916945405304\n",
      "[4, 55]: loss: 0.012825115787563846\n",
      "[4, 60]: loss: 0.050444240565411747\n",
      "[4, 65]: loss: 0.038822345552034676\n",
      "[5, 5]: loss: 0.024004114558920264\n",
      "[5, 10]: loss: 0.06192111852578819\n",
      "[5, 15]: loss: 0.015165088814683259\n",
      "[5, 20]: loss: 0.05453832767670974\n",
      "[5, 25]: loss: 0.01580578915309161\n",
      "[5, 30]: loss: 0.07050486875232309\n",
      "[5, 35]: loss: 0.005044715566327795\n",
      "[5, 40]: loss: 0.022021513897925615\n",
      "[5, 45]: loss: 0.05225204158341512\n",
      "[5, 50]: loss: 0.03583714412525296\n",
      "[5, 55]: loss: 0.006915845267940313\n",
      "[5, 60]: loss: 0.02631810854654759\n",
      "[5, 65]: loss: 0.026655666180886328\n",
      "[6, 5]: loss: 0.012320834095589817\n",
      "[6, 10]: loss: 0.012748946319334209\n",
      "[6, 15]: loss: 0.02374728987342678\n",
      "[6, 20]: loss: 0.01014503528131172\n",
      "[6, 25]: loss: 0.0045629089581780136\n",
      "[6, 30]: loss: 0.00946005218429491\n",
      "[6, 35]: loss: 0.015603238018229604\n",
      "[6, 40]: loss: 0.013499609311111271\n",
      "[6, 45]: loss: 0.03347576956730336\n",
      "[6, 50]: loss: 0.006062132713850588\n",
      "[6, 55]: loss: 0.02440401740022935\n",
      "[6, 60]: loss: 0.019533532598870806\n",
      "[6, 65]: loss: 0.020672972197644413\n",
      "[7, 5]: loss: 0.00962829205673188\n",
      "[7, 10]: loss: 0.005968771321931854\n",
      "[7, 15]: loss: 0.008357177372090518\n",
      "[7, 20]: loss: 0.007544483669335023\n",
      "[7, 25]: loss: 0.019771498395130038\n",
      "[7, 30]: loss: 0.0073236312600784\n",
      "[7, 35]: loss: 0.006665205961326137\n",
      "[7, 40]: loss: 0.0049577392055653036\n",
      "[7, 45]: loss: 0.010880946298129857\n",
      "[7, 50]: loss: 0.004336470447015017\n",
      "[7, 55]: loss: 0.010452627917402424\n",
      "[7, 60]: loss: 0.07932031631935388\n",
      "[7, 65]: loss: 0.016587140038609505\n",
      "[8, 5]: loss: 0.007356387868640013\n",
      "[8, 10]: loss: 0.006368821021169424\n",
      "[8, 15]: loss: 0.00710396800423041\n",
      "[8, 20]: loss: 0.008362594584468752\n",
      "[8, 25]: loss: 0.0075284772901795805\n",
      "[8, 30]: loss: 0.009386774094309658\n",
      "[8, 35]: loss: 0.004540322639513761\n",
      "[8, 40]: loss: 0.007681936636799946\n",
      "[8, 45]: loss: 0.005389965808717534\n",
      "[8, 50]: loss: 0.00996512919664383\n",
      "[8, 55]: loss: 0.003392309386981651\n",
      "[8, 60]: loss: 0.01067746410262771\n",
      "[8, 65]: loss: 0.018880870688008144\n",
      "[9, 5]: loss: 0.006521408518892713\n",
      "[9, 10]: loss: 0.00755689266952686\n",
      "[9, 15]: loss: 0.006620455416850746\n",
      "[9, 20]: loss: 0.00375501332746353\n",
      "[9, 25]: loss: 0.005669683392625302\n",
      "[9, 30]: loss: 0.005816036311443895\n",
      "[9, 35]: loss: 0.007486137008527294\n",
      "[9, 40]: loss: 0.006606976850889623\n",
      "[9, 45]: loss: 0.0060927569575142115\n",
      "[9, 50]: loss: 0.0030741650552954525\n",
      "[9, 55]: loss: 0.015096355928108096\n",
      "[9, 60]: loss: 0.004779753857292235\n",
      "[9, 65]: loss: 0.00537038994661998\n",
      "[10, 5]: loss: 0.0033027713652700186\n",
      "[10, 10]: loss: 0.006228054757229984\n",
      "[10, 15]: loss: 0.0031330491765402257\n",
      "[10, 20]: loss: 0.0045755445171380416\n",
      "[10, 25]: loss: 0.0030490590434055775\n",
      "[10, 30]: loss: 0.006789804698200896\n",
      "[10, 35]: loss: 0.005211537296418101\n",
      "[10, 40]: loss: 0.008478711824864149\n",
      "[10, 45]: loss: 0.008670845039887354\n",
      "[10, 50]: loss: 0.0029735856514889747\n",
      "[10, 55]: loss: 0.009846516768448055\n",
      "[10, 60]: loss: 0.010651645890902728\n",
      "[10, 65]: loss: 0.0064823061402421445\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 6: 0.482\n",
      "Random Sampling Iteration 7\n",
      "[1, 5]: loss: 0.14453908859286457\n",
      "[1, 10]: loss: 0.2530151005194057\n",
      "[1, 15]: loss: 0.2482907447265461\n",
      "[1, 20]: loss: 0.4310805252753198\n",
      "[1, 25]: loss: 0.2568585593253374\n",
      "[1, 30]: loss: 0.3703710490372032\n",
      "[1, 35]: loss: 0.6340717561542988\n",
      "[1, 40]: loss: 0.5310346418991685\n",
      "[1, 45]: loss: 0.515709612518549\n",
      "[1, 50]: loss: 0.35188160091638565\n",
      "[1, 55]: loss: 0.3812404773198068\n",
      "[1, 60]: loss: 0.2100494196638465\n",
      "[1, 65]: loss: 0.9541403129696846\n",
      "[1, 70]: loss: 0.2883529489627108\n",
      "[1, 75]: loss: 0.27521195635199547\n",
      "[2, 5]: loss: 0.11166003940161318\n",
      "[2, 10]: loss: 0.10376323107630014\n",
      "[2, 15]: loss: 0.1798780425451696\n",
      "[2, 20]: loss: 0.20695606200024486\n",
      "[2, 25]: loss: 0.06664741784334183\n",
      "[2, 30]: loss: 0.16599811473861337\n",
      "[2, 35]: loss: 0.20362965809181333\n",
      "[2, 40]: loss: 0.18907847971422598\n",
      "[2, 45]: loss: 0.060071107116527855\n",
      "[2, 50]: loss: 0.22601705230772495\n",
      "[2, 55]: loss: 0.0822833611164242\n",
      "[2, 60]: loss: 0.16654262598603964\n",
      "[2, 65]: loss: 0.09895692812278867\n",
      "[2, 70]: loss: 0.08451376156881452\n",
      "[2, 75]: loss: 0.04051350778900087\n",
      "[3, 5]: loss: 0.12607438256964087\n",
      "[3, 10]: loss: 0.04726480878889561\n",
      "[3, 15]: loss: 0.09865361941047013\n",
      "[3, 20]: loss: 0.08083822764456272\n",
      "[3, 25]: loss: 0.03679864035802893\n",
      "[3, 30]: loss: 0.03679428994655609\n",
      "[3, 35]: loss: 0.04571841284632683\n",
      "[3, 40]: loss: 0.025888848351314664\n",
      "[3, 45]: loss: 0.09916544752195477\n",
      "[3, 50]: loss: 0.021554429491516203\n",
      "[3, 55]: loss: 0.09003853565081954\n",
      "[3, 60]: loss: 0.07176542142406106\n",
      "[3, 65]: loss: 0.06139634468127042\n",
      "[3, 70]: loss: 0.08938525198027492\n",
      "[3, 75]: loss: 0.08391782362014055\n",
      "[4, 5]: loss: 0.007203957298770547\n",
      "[4, 10]: loss: 0.017167648882605135\n",
      "[4, 15]: loss: 0.030376269482076168\n",
      "[4, 20]: loss: 0.016856123344041407\n",
      "[4, 25]: loss: 0.0105253413785249\n",
      "[4, 30]: loss: 0.016642389819025993\n",
      "[4, 35]: loss: 0.02596660325070843\n",
      "[4, 40]: loss: 0.052843836368992925\n",
      "[4, 45]: loss: 0.02083178097382188\n",
      "[4, 50]: loss: 0.01870152202900499\n",
      "[4, 55]: loss: 0.0667905422160402\n",
      "[4, 60]: loss: 0.11643611174076796\n",
      "[4, 65]: loss: 0.012835493427701294\n",
      "[4, 70]: loss: 0.18350196396932006\n",
      "[4, 75]: loss: 0.09842398343607783\n",
      "[5, 5]: loss: 0.009609745349735022\n",
      "[5, 10]: loss: 0.02232349873520434\n",
      "[5, 15]: loss: 0.025121052574831992\n",
      "[5, 20]: loss: 0.041268132044933736\n",
      "[5, 25]: loss: 0.02035069972043857\n",
      "[5, 30]: loss: 0.03852044220548123\n",
      "[5, 35]: loss: 0.039404221024597064\n",
      "[5, 40]: loss: 0.028718974033836275\n",
      "[5, 45]: loss: 0.007499688246753067\n",
      "[5, 50]: loss: 0.02880976814776659\n",
      "[5, 55]: loss: 0.07443440426141024\n",
      "[5, 60]: loss: 0.014065471361391246\n",
      "[5, 65]: loss: 0.02097341639455408\n",
      "[5, 70]: loss: 0.020993714104406536\n",
      "[5, 75]: loss: 0.06328002200461924\n",
      "[6, 5]: loss: 0.02025959175080061\n",
      "[6, 10]: loss: 0.014763932442292571\n",
      "[6, 15]: loss: 0.03395132225705311\n",
      "[6, 20]: loss: 0.0045594377152156085\n",
      "[6, 25]: loss: 0.01630825491156429\n",
      "[6, 30]: loss: 0.02305128425359726\n",
      "[6, 35]: loss: 0.03194255259586498\n",
      "[6, 40]: loss: 0.016221397905610502\n",
      "[6, 45]: loss: 0.013429397135041654\n",
      "[6, 50]: loss: 0.02075818089360837\n",
      "[6, 55]: loss: 0.012890147452708334\n",
      "[6, 60]: loss: 0.01768523606006056\n",
      "[6, 65]: loss: 0.010730956972111017\n",
      "[6, 70]: loss: 0.036770801525563\n",
      "[6, 75]: loss: 0.009552391071338207\n",
      "[7, 5]: loss: 0.01857377146370709\n",
      "[7, 10]: loss: 0.00592542567756027\n",
      "[7, 15]: loss: 0.013843438879121095\n",
      "[7, 20]: loss: 0.007482512679416686\n",
      "[7, 25]: loss: 0.010646930488292128\n",
      "[7, 30]: loss: 0.00791475863661617\n",
      "[7, 35]: loss: 0.0026732909464044496\n",
      "[7, 40]: loss: 0.002658947109011933\n",
      "[7, 45]: loss: 0.027522637974470854\n",
      "[7, 50]: loss: 0.006919452454894781\n",
      "[7, 55]: loss: 0.00648162531433627\n",
      "[7, 60]: loss: 0.003061908282688819\n",
      "[7, 65]: loss: 0.010848227539099753\n",
      "[7, 70]: loss: 0.00579027843195945\n",
      "[7, 75]: loss: 0.023145960294641554\n",
      "[8, 5]: loss: 0.005135801387950778\n",
      "[8, 10]: loss: 0.005524430249352008\n",
      "[8, 15]: loss: 0.012231317756231874\n",
      "[8, 20]: loss: 0.021919596882071346\n",
      "[8, 25]: loss: 0.020765866327565163\n",
      "[8, 30]: loss: 0.008709302579518408\n",
      "[8, 35]: loss: 0.009874760289676487\n",
      "[8, 40]: loss: 0.0075496820791158825\n",
      "[8, 45]: loss: 0.0033799030061345547\n",
      "[8, 50]: loss: 0.010211678760242648\n",
      "[8, 55]: loss: 0.0066894527117256075\n",
      "[8, 60]: loss: 0.003212368712411262\n",
      "[8, 65]: loss: 0.004008704039733857\n",
      "[8, 70]: loss: 0.006432117166696116\n",
      "[8, 75]: loss: 0.006734751106705517\n",
      "[9, 5]: loss: 0.018430589174386114\n",
      "[9, 10]: loss: 0.005002412246540189\n",
      "[9, 15]: loss: 0.010210063133854419\n",
      "[9, 20]: loss: 0.004504379961872473\n",
      "[9, 25]: loss: 0.0056286931212525815\n",
      "[9, 30]: loss: 0.026372256048489362\n",
      "[9, 35]: loss: 0.004734085901873186\n",
      "[9, 40]: loss: 0.006336679885862395\n",
      "[9, 45]: loss: 0.0086871498497203\n",
      "[9, 50]: loss: 0.012634577506105416\n",
      "[9, 55]: loss: 0.002483834461600054\n",
      "[9, 60]: loss: 0.015780043497215956\n",
      "[9, 65]: loss: 0.0031974461671779864\n",
      "[9, 70]: loss: 0.005319430580129847\n",
      "[9, 75]: loss: 0.003925737779354677\n",
      "[10, 5]: loss: 0.00807099137455225\n",
      "[10, 10]: loss: 0.002713044552365318\n",
      "[10, 15]: loss: 0.004652003874070942\n",
      "[10, 20]: loss: 0.003920418661436997\n",
      "[10, 25]: loss: 0.003677703774883412\n",
      "[10, 30]: loss: 0.007031044864561409\n",
      "[10, 35]: loss: 0.00432587722025346\n",
      "[10, 40]: loss: 0.007493974349927157\n",
      "[10, 45]: loss: 0.018310515180928633\n",
      "[10, 50]: loss: 0.010348741838242859\n",
      "[10, 55]: loss: 0.010013914536102675\n",
      "[10, 60]: loss: 0.003056116693187505\n",
      "[10, 65]: loss: 0.008568602919694968\n",
      "[10, 70]: loss: 0.0025297796382801607\n",
      "[10, 75]: loss: 0.0041288830689154565\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 7: 0.4883\n",
      "Random Sampling Iteration 8\n",
      "[1, 5]: loss: 0.10876561130862683\n",
      "[1, 10]: loss: 0.5811424376443028\n",
      "[1, 15]: loss: 0.22199775092303753\n",
      "[1, 20]: loss: 0.3772061562631279\n",
      "[1, 25]: loss: 0.4900663229636848\n",
      "[1, 30]: loss: 0.37759805750101805\n",
      "[1, 35]: loss: 0.5989113375544548\n",
      "[1, 40]: loss: 0.5759689398109913\n",
      "[1, 45]: loss: 0.3783645387738943\n",
      "[1, 50]: loss: 0.7271551031153649\n",
      "[1, 55]: loss: 0.6518027633428574\n",
      "[1, 60]: loss: 0.1806549581233412\n",
      "[1, 65]: loss: 0.22364349849522114\n",
      "[1, 70]: loss: 0.1303765051998198\n",
      "[1, 75]: loss: 0.6454897783696651\n",
      "[1, 80]: loss: 0.22664515115320683\n",
      "[2, 5]: loss: 0.14142974209971726\n",
      "[2, 10]: loss: 0.0890284557826817\n",
      "[2, 15]: loss: 0.058617650764063\n",
      "[2, 20]: loss: 0.05807719239965081\n",
      "[2, 25]: loss: 0.0939241386950016\n",
      "[2, 30]: loss: 0.07093409867957234\n",
      "[2, 35]: loss: 0.03596185124479234\n",
      "[2, 40]: loss: 0.03235502587631345\n",
      "[2, 45]: loss: 0.05314838158665225\n",
      "[2, 50]: loss: 0.07368477433919907\n",
      "[2, 55]: loss: 0.17808803217485547\n",
      "[2, 60]: loss: 0.013880330719985068\n",
      "[2, 65]: loss: 0.04675220511853695\n",
      "[2, 70]: loss: 0.10307575773913413\n",
      "[2, 75]: loss: 0.16030364856123924\n",
      "[2, 80]: loss: 0.1309213638305664\n",
      "[3, 5]: loss: 0.01861150690820068\n",
      "[3, 10]: loss: 0.08190271840430796\n",
      "[3, 15]: loss: 0.3527380619198084\n",
      "[3, 20]: loss: 0.06711566215381026\n",
      "[3, 25]: loss: 0.07177261123433709\n",
      "[3, 30]: loss: 0.20231825206428766\n",
      "[3, 35]: loss: 0.15468419715762138\n",
      "[3, 40]: loss: 0.07355700642801821\n",
      "[3, 45]: loss: 0.09369268221780658\n",
      "[3, 50]: loss: 0.11771745653823018\n",
      "[3, 55]: loss: 0.09123472636565566\n",
      "[3, 60]: loss: 0.03000545484246686\n",
      "[3, 65]: loss: 0.07234027003869414\n",
      "[3, 70]: loss: 0.024053119705058634\n",
      "[3, 75]: loss: 0.1836909162811935\n",
      "[3, 80]: loss: 0.07722354074940085\n",
      "[4, 5]: loss: 0.05173361476045102\n",
      "[4, 10]: loss: 0.08729834819678217\n",
      "[4, 15]: loss: 0.025971784722059965\n",
      "[4, 20]: loss: 0.011826361762359738\n",
      "[4, 25]: loss: 0.08997068647295237\n",
      "[4, 30]: loss: 0.015185831289272755\n",
      "[4, 35]: loss: 0.020113201579079032\n",
      "[4, 40]: loss: 0.0664199274033308\n",
      "[4, 45]: loss: 0.016963465721346438\n",
      "[4, 50]: loss: 0.04973379382863641\n",
      "[4, 55]: loss: 0.01551792526151985\n",
      "[4, 60]: loss: 0.040751109598204494\n",
      "[4, 65]: loss: 0.03876494156429544\n",
      "[4, 70]: loss: 0.023429308726917952\n",
      "[4, 75]: loss: 0.024073894252069294\n",
      "[4, 80]: loss: 0.04324865364469588\n",
      "[5, 5]: loss: 0.014346780837513506\n",
      "[5, 10]: loss: 0.09096871758811176\n",
      "[5, 15]: loss: 0.0788175892084837\n",
      "[5, 20]: loss: 0.05594431678764522\n",
      "[5, 25]: loss: 0.22713463450782\n",
      "[5, 30]: loss: 0.09278866276144981\n",
      "[5, 35]: loss: 0.10214586480287835\n",
      "[5, 40]: loss: 0.14712910586968064\n",
      "[5, 45]: loss: 0.09643783885985613\n",
      "[5, 50]: loss: 0.06882017431780696\n",
      "[5, 55]: loss: 0.08273127535358071\n",
      "[5, 60]: loss: 0.1439252719283104\n",
      "[5, 65]: loss: 0.0762207544175908\n",
      "[5, 70]: loss: 0.06839042494539171\n",
      "[5, 75]: loss: 0.08895894628949463\n",
      "[5, 80]: loss: 0.07432576711289585\n",
      "[6, 5]: loss: 0.0711923266062513\n",
      "[6, 10]: loss: 0.02746585919521749\n",
      "[6, 15]: loss: 0.06100940809119493\n",
      "[6, 20]: loss: 0.043382662581279874\n",
      "[6, 25]: loss: 0.04547843697946519\n",
      "[6, 30]: loss: 0.018035357061307877\n",
      "[6, 35]: loss: 0.0410901855211705\n",
      "[6, 40]: loss: 0.04518529126653448\n",
      "[6, 45]: loss: 0.03589069610461593\n",
      "[6, 50]: loss: 0.030395551439141855\n",
      "[6, 55]: loss: 0.04470883234171197\n",
      "[6, 60]: loss: 0.011406073113903403\n",
      "[6, 65]: loss: 0.018201824510470033\n",
      "[6, 70]: loss: 0.03147098235785961\n",
      "[6, 75]: loss: 0.01750821855966933\n",
      "[6, 80]: loss: 0.05661804892588407\n",
      "[7, 5]: loss: 0.010102672036737204\n",
      "[7, 10]: loss: 0.012701195024419576\n",
      "[7, 15]: loss: 0.039215135388076305\n",
      "[7, 20]: loss: 0.011811082600615919\n",
      "[7, 25]: loss: 0.017539713997393847\n",
      "[7, 30]: loss: 0.05651910579763353\n",
      "[7, 35]: loss: 0.016623316274490207\n",
      "[7, 40]: loss: 0.06288321781903505\n",
      "[7, 45]: loss: 0.05090153217315674\n",
      "[7, 50]: loss: 0.01814788719639182\n",
      "[7, 55]: loss: 0.024491062853485346\n",
      "[7, 60]: loss: 0.014939269050955772\n",
      "[7, 65]: loss: 0.020777122874278575\n",
      "[7, 70]: loss: 0.04887786158360541\n",
      "[7, 75]: loss: 0.0041288145002909005\n",
      "[7, 80]: loss: 0.01202875905437395\n",
      "[8, 5]: loss: 0.016084659902844578\n",
      "[8, 10]: loss: 0.014558591996319592\n",
      "[8, 15]: loss: 0.006327999813947827\n",
      "[8, 20]: loss: 0.005637286580167711\n",
      "[8, 25]: loss: 0.006753173016477376\n",
      "[8, 30]: loss: 0.007316209084820002\n",
      "[8, 35]: loss: 0.01892817864427343\n",
      "[8, 40]: loss: 0.03587783360853791\n",
      "[8, 45]: loss: 0.01948596941656433\n",
      "[8, 50]: loss: 0.022692503407597542\n",
      "[8, 55]: loss: 0.026270840666256845\n",
      "[8, 60]: loss: 0.010887999495025724\n",
      "[8, 65]: loss: 0.03693014720920473\n",
      "[8, 70]: loss: 0.01691695291083306\n",
      "[8, 75]: loss: 0.027188050793483853\n",
      "[8, 80]: loss: 0.0040765001613181084\n",
      "[9, 5]: loss: 0.006973246781853959\n",
      "[9, 10]: loss: 0.01450521130755078\n",
      "[9, 15]: loss: 0.0057801515504252166\n",
      "[9, 20]: loss: 0.023904219269752502\n",
      "[9, 25]: loss: 0.013512818142771721\n",
      "[9, 30]: loss: 0.0186389644513838\n",
      "[9, 35]: loss: 0.008244736614869907\n",
      "[9, 40]: loss: 0.014692481840029359\n",
      "[9, 45]: loss: 0.009004730149172246\n",
      "[9, 50]: loss: 0.005178392399102449\n",
      "[9, 55]: loss: 0.00971882522571832\n",
      "[9, 60]: loss: 0.008716177253518254\n",
      "[9, 65]: loss: 0.005744867899920791\n",
      "[9, 70]: loss: 0.008320387642015703\n",
      "[9, 75]: loss: 0.005634072818793356\n",
      "[9, 80]: loss: 0.010611817633616738\n",
      "[10, 5]: loss: 0.010900966575718485\n",
      "[10, 10]: loss: 0.007221979787573218\n",
      "[10, 15]: loss: 0.00562526257999707\n",
      "[10, 20]: loss: 0.0050881046045105904\n",
      "[10, 25]: loss: 0.005055426241597161\n",
      "[10, 30]: loss: 0.013938155141659081\n",
      "[10, 35]: loss: 0.03266940382309258\n",
      "[10, 40]: loss: 0.017324257642030716\n",
      "[10, 45]: loss: 0.01015767629723996\n",
      "[10, 50]: loss: 0.004507254168856889\n",
      "[10, 55]: loss: 0.03257968823891133\n",
      "[10, 60]: loss: 0.0251801562262699\n",
      "[10, 65]: loss: 0.037298183160601184\n",
      "[10, 70]: loss: 0.021319161634892225\n",
      "[10, 75]: loss: 0.020515782991424203\n",
      "[10, 80]: loss: 0.00878600828582421\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 8: 0.4835\n",
      "Random Sampling Iteration 9\n",
      "[1, 5]: loss: 0.12751386465970427\n",
      "[1, 10]: loss: 0.06766890121798497\n",
      "[1, 15]: loss: 0.6675558462738991\n",
      "[1, 20]: loss: 0.539167121052742\n",
      "[1, 25]: loss: 0.11167872068472207\n",
      "[1, 30]: loss: 0.46748599875718355\n",
      "[1, 35]: loss: 0.18750704638659954\n",
      "[1, 40]: loss: 0.3142856298945844\n",
      "[1, 45]: loss: 0.4807077271398157\n",
      "[1, 50]: loss: 0.3230484016239643\n",
      "[1, 55]: loss: 0.47806659573689103\n",
      "[1, 60]: loss: 0.1736164717003703\n",
      "[1, 65]: loss: 0.15628594998270273\n",
      "[1, 70]: loss: 0.8687076005153358\n",
      "[1, 75]: loss: 0.2196407555602491\n",
      "[1, 80]: loss: 0.5615322501398623\n",
      "[1, 85]: loss: 0.16481957072392106\n",
      "[2, 5]: loss: 0.08736357232555747\n",
      "[2, 10]: loss: 0.0947305029258132\n",
      "[2, 15]: loss: 0.1866551050916314\n",
      "[2, 20]: loss: 0.09137338330037892\n",
      "[2, 25]: loss: 0.1411185401957482\n",
      "[2, 30]: loss: 0.11699320178013295\n",
      "[2, 35]: loss: 0.18545851530507207\n",
      "[2, 40]: loss: 0.05013173626502976\n",
      "[2, 45]: loss: 0.10779653926147148\n",
      "[2, 50]: loss: 0.030767363496124744\n",
      "[2, 55]: loss: 0.05958977108821273\n",
      "[2, 60]: loss: 0.13009127951227129\n",
      "[2, 65]: loss: 0.09663848159834743\n",
      "[2, 70]: loss: 0.12365416763350368\n",
      "[2, 75]: loss: 0.179626545868814\n",
      "[2, 80]: loss: 0.13119772961363196\n",
      "[2, 85]: loss: 0.12815437838435173\n",
      "[3, 5]: loss: 0.056270866421982646\n",
      "[3, 10]: loss: 0.10052031697705388\n",
      "[3, 15]: loss: 0.07278829370625317\n",
      "[3, 20]: loss: 0.05729146976955235\n",
      "[3, 25]: loss: 0.04055198049172759\n",
      "[3, 30]: loss: 0.03532472113147378\n",
      "[3, 35]: loss: 0.10724280745489523\n",
      "[3, 40]: loss: 0.02076666336506605\n",
      "[3, 45]: loss: 0.040578453335911036\n",
      "[3, 50]: loss: 0.03615337633527815\n",
      "[3, 55]: loss: 0.01097054488491267\n",
      "[3, 60]: loss: 0.07591560459695756\n",
      "[3, 65]: loss: 0.020514125004410744\n",
      "[3, 70]: loss: 0.03954639547737315\n",
      "[3, 75]: loss: 0.02142933552386239\n",
      "[3, 80]: loss: 0.03909173794090748\n",
      "[3, 85]: loss: 0.03396324359346181\n",
      "[4, 5]: loss: 0.009770757314981893\n",
      "[4, 10]: loss: 0.015613687690347433\n",
      "[4, 15]: loss: 0.03617620229488239\n",
      "[4, 20]: loss: 0.016872166888788342\n",
      "[4, 25]: loss: 0.019244845723733306\n",
      "[4, 30]: loss: 0.009294425690313801\n",
      "[4, 35]: loss: 0.028190035314764827\n",
      "[4, 40]: loss: 0.005522502062376589\n",
      "[4, 45]: loss: 0.01949989193235524\n",
      "[4, 50]: loss: 0.012911385390907526\n",
      "[4, 55]: loss: 0.010903578076977283\n",
      "[4, 60]: loss: 0.009615351562388241\n",
      "[4, 65]: loss: 0.003589893283788115\n",
      "[4, 70]: loss: 0.008052193385083228\n",
      "[4, 75]: loss: 0.005955055239610374\n",
      "[4, 80]: loss: 0.010811561427544802\n",
      "[4, 85]: loss: 0.055918563914019614\n",
      "[5, 5]: loss: 0.005552276619710028\n",
      "[5, 10]: loss: 0.027290201382129453\n",
      "[5, 15]: loss: 0.017099077405873686\n",
      "[5, 20]: loss: 0.014236068353056908\n",
      "[5, 25]: loss: 0.012177550845080987\n",
      "[5, 30]: loss: 0.006152018089778721\n",
      "[5, 35]: loss: 0.07512756675714627\n",
      "[5, 40]: loss: 0.005956193708698265\n",
      "[5, 45]: loss: 0.04855819448130205\n",
      "[5, 50]: loss: 0.010648948955349624\n",
      "[5, 55]: loss: 0.011041976948035881\n",
      "[5, 60]: loss: 0.0939799303887412\n",
      "[5, 65]: loss: 0.05933003989048302\n",
      "[5, 70]: loss: 0.08275887463241816\n",
      "[5, 75]: loss: 0.04711588268401101\n",
      "[5, 80]: loss: 0.027972555719316006\n",
      "[5, 85]: loss: 0.02892857533879578\n",
      "[6, 5]: loss: 0.014820406417129561\n",
      "[6, 10]: loss: 0.022265542043896858\n",
      "[6, 15]: loss: 0.10330500270356424\n",
      "[6, 20]: loss: 0.029677539598196745\n",
      "[6, 25]: loss: 0.018657383741810918\n",
      "[6, 30]: loss: 0.009665352350566536\n",
      "[6, 35]: loss: 0.01761509769130498\n",
      "[6, 40]: loss: 0.05447768699377775\n",
      "[6, 45]: loss: 0.14804022072348744\n",
      "[6, 50]: loss: 0.027567958284635097\n",
      "[6, 55]: loss: 0.060417022701585665\n",
      "[6, 60]: loss: 0.014083870890317485\n",
      "[6, 65]: loss: 0.009794370998861268\n",
      "[6, 70]: loss: 0.0422609735687729\n",
      "[6, 75]: loss: 0.011286892055068165\n",
      "[6, 80]: loss: 0.03384471172466874\n",
      "[6, 85]: loss: 0.017914934433065355\n",
      "[7, 5]: loss: 0.008758574986131862\n",
      "[7, 10]: loss: 0.02444213250419125\n",
      "[7, 15]: loss: 0.015078014053869992\n",
      "[7, 20]: loss: 0.004723847523564473\n",
      "[7, 25]: loss: 0.010352063341997564\n",
      "[7, 30]: loss: 0.009305094339651987\n",
      "[7, 35]: loss: 0.010475029179360718\n",
      "[7, 40]: loss: 0.003558506097760983\n",
      "[7, 45]: loss: 0.01315693580545485\n",
      "[7, 50]: loss: 0.01206164329778403\n",
      "[7, 55]: loss: 0.02064755590981804\n",
      "[7, 60]: loss: 0.00870993104763329\n",
      "[7, 65]: loss: 0.010497945011593401\n",
      "[7, 70]: loss: 0.009614396243705414\n",
      "[7, 75]: loss: 0.0034990261774510145\n",
      "[7, 80]: loss: 0.004843314687605016\n",
      "[7, 85]: loss: 0.025186444749124348\n",
      "[8, 5]: loss: 0.009098434238694608\n",
      "[8, 10]: loss: 0.012231135304318741\n",
      "[8, 15]: loss: 0.005600298347417265\n",
      "[8, 20]: loss: 0.005854622111655772\n",
      "[8, 25]: loss: 0.003254186303820461\n",
      "[8, 30]: loss: 0.020203097199555486\n",
      "[8, 35]: loss: 0.0033941546353162266\n",
      "[8, 40]: loss: 0.014368699208716862\n",
      "[8, 45]: loss: 0.004502048250287771\n",
      "[8, 50]: loss: 0.01654393959324807\n",
      "[8, 55]: loss: 0.0046922885812819\n",
      "[8, 60]: loss: 0.011509208823554218\n",
      "[8, 65]: loss: 0.006422137870686129\n",
      "[8, 70]: loss: 0.011081578442826867\n",
      "[8, 75]: loss: 0.005218349397182465\n",
      "[8, 80]: loss: 0.00864128817920573\n",
      "[8, 85]: loss: 0.01817244474659674\n",
      "[9, 5]: loss: 0.008716660449863411\n",
      "[9, 10]: loss: 0.016908868332393467\n",
      "[9, 15]: loss: 0.005896836461033672\n",
      "[9, 20]: loss: 0.014630197925725952\n",
      "[9, 25]: loss: 0.005769546449300833\n",
      "[9, 30]: loss: 0.004151361412368715\n",
      "[9, 35]: loss: 0.0015599125472363085\n",
      "[9, 40]: loss: 0.01807279814966023\n",
      "[9, 45]: loss: 0.020508196263108402\n",
      "[9, 50]: loss: 0.0034867542271967977\n",
      "[9, 55]: loss: 0.03325090990983881\n",
      "[9, 60]: loss: 0.008860288275172934\n",
      "[9, 65]: loss: 0.008865272742696106\n",
      "[9, 70]: loss: 0.006793981170631014\n",
      "[9, 75]: loss: 0.003734765385161154\n",
      "[9, 80]: loss: 0.002707318533794023\n",
      "[9, 85]: loss: 0.0036271724093239754\n",
      "[10, 5]: loss: 0.008745967323193327\n",
      "[10, 10]: loss: 0.014663617621408775\n",
      "[10, 15]: loss: 0.003880022733937949\n",
      "[10, 20]: loss: 0.005450747354188934\n",
      "[10, 25]: loss: 0.003774800708924886\n",
      "[10, 30]: loss: 0.01089625884196721\n",
      "[10, 35]: loss: 0.013129305734764785\n",
      "[10, 40]: loss: 0.005909423882258125\n",
      "[10, 45]: loss: 0.0022698921529809013\n",
      "[10, 50]: loss: 0.005408930213889107\n",
      "[10, 55]: loss: 0.019594384648371488\n",
      "[10, 60]: loss: 0.004610350850271061\n",
      "[10, 65]: loss: 0.004519282636465505\n",
      "[10, 70]: loss: 0.019815839012153447\n",
      "[10, 75]: loss: 0.0039142907480709255\n",
      "[10, 80]: loss: 0.01114963585860096\n",
      "[10, 85]: loss: 0.007943534525111318\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 9: 0.4926\n",
      "Random Sampling Iteration 10\n",
      "[1, 5]: loss: 0.051276514772325754\n",
      "[1, 10]: loss: 0.0803982563666068\n",
      "[1, 15]: loss: 0.3000013316050172\n",
      "[1, 20]: loss: 0.09456076714559458\n",
      "[1, 25]: loss: 0.24979067884851247\n",
      "[1, 30]: loss: 0.330507455393672\n",
      "[1, 35]: loss: 0.2916016145609319\n",
      "[1, 40]: loss: 0.3220114461146295\n",
      "[1, 45]: loss: 0.2689801211236045\n",
      "[1, 50]: loss: 0.3030900042504072\n",
      "[1, 55]: loss: 0.5588482618331909\n",
      "[1, 60]: loss: 0.3425214309245348\n",
      "[1, 65]: loss: 0.3415532847866416\n",
      "[1, 70]: loss: 0.29453127551823854\n",
      "[1, 75]: loss: 0.43739306135103106\n",
      "[1, 80]: loss: 0.2080982392653823\n",
      "[1, 85]: loss: 0.3683387874625623\n",
      "[1, 90]: loss: 0.5615179799497128\n",
      "[2, 5]: loss: 0.06266214582137764\n",
      "[2, 10]: loss: 0.05958208325318992\n",
      "[2, 15]: loss: 0.10841575940139592\n",
      "[2, 20]: loss: 0.04779344191774726\n",
      "[2, 25]: loss: 0.1731640938669443\n",
      "[2, 30]: loss: 0.2599558038637042\n",
      "[2, 35]: loss: 0.09452350944047794\n",
      "[2, 40]: loss: 0.04908198048360646\n",
      "[2, 45]: loss: 0.19962013512849808\n",
      "[2, 50]: loss: 0.03530869027599692\n",
      "[2, 55]: loss: 0.11998819629661739\n",
      "[2, 60]: loss: 0.10519863571971655\n",
      "[2, 65]: loss: 0.03544163447804749\n",
      "[2, 70]: loss: 0.07206123927608132\n",
      "[2, 75]: loss: 0.07822538132313639\n",
      "[2, 80]: loss: 0.10081154754152521\n",
      "[2, 85]: loss: 0.09909559786319733\n",
      "[2, 90]: loss: 0.04868508782237768\n",
      "[3, 5]: loss: 0.046437204524409026\n",
      "[3, 10]: loss: 0.03326196683337912\n",
      "[3, 15]: loss: 0.08620912814512849\n",
      "[3, 20]: loss: 0.010903969698119909\n",
      "[3, 25]: loss: 0.024146257317624986\n",
      "[3, 30]: loss: 0.034194717300124466\n",
      "[3, 35]: loss: 0.00670721143251285\n",
      "[3, 40]: loss: 0.025851654616417363\n",
      "[3, 45]: loss: 0.03055813629180193\n",
      "[3, 50]: loss: 0.04244063701480627\n",
      "[3, 55]: loss: 0.012308270874200389\n",
      "[3, 60]: loss: 0.024987473501823843\n",
      "[3, 65]: loss: 0.07019349792972207\n",
      "[3, 70]: loss: 0.008450731867924333\n",
      "[3, 75]: loss: 0.019313098047859967\n",
      "[3, 80]: loss: 0.04567756032338366\n",
      "[3, 85]: loss: 0.03227507654810324\n",
      "[3, 90]: loss: 0.01735157021903433\n",
      "[4, 5]: loss: 0.011311769718304276\n",
      "[4, 10]: loss: 0.008905692782718688\n",
      "[4, 15]: loss: 0.008121235994622111\n",
      "[4, 20]: loss: 0.012843057484133169\n",
      "[4, 25]: loss: 0.028601523634279147\n",
      "[4, 30]: loss: 0.02499594930850435\n",
      "[4, 35]: loss: 0.0308572183130309\n",
      "[4, 40]: loss: 0.006799223221605644\n",
      "[4, 45]: loss: 0.011891556321643293\n",
      "[4, 50]: loss: 0.002536229440011084\n",
      "[4, 55]: loss: 0.02423846050805878\n",
      "[4, 60]: loss: 0.020072047933354042\n",
      "[4, 65]: loss: 0.010578739253105596\n",
      "[4, 70]: loss: 0.0030266011308413\n",
      "[4, 75]: loss: 0.039922461146488786\n",
      "[4, 80]: loss: 0.016846089158207178\n",
      "[4, 85]: loss: 0.012561723560793325\n",
      "[4, 90]: loss: 0.03962398777366616\n",
      "[5, 5]: loss: 0.006805110606364906\n",
      "[5, 10]: loss: 0.0072536199586465955\n",
      "[5, 15]: loss: 0.020597850903868675\n",
      "[5, 20]: loss: 0.03750895419216249\n",
      "[5, 25]: loss: 0.006303407019004226\n",
      "[5, 30]: loss: 0.01629221424809657\n",
      "[5, 35]: loss: 0.009985644603148103\n",
      "[5, 40]: loss: 0.01738869905238971\n",
      "[5, 45]: loss: 0.011829685419797897\n",
      "[5, 50]: loss: 0.012735607277136296\n",
      "[5, 55]: loss: 0.007474452082533389\n",
      "[5, 60]: loss: 0.02675034574349411\n",
      "[5, 65]: loss: 0.03293302498059347\n",
      "[5, 70]: loss: 0.013543662353185937\n",
      "[5, 75]: loss: 0.006001703659421764\n",
      "[5, 80]: loss: 0.08015884162159637\n",
      "[5, 85]: loss: 0.08902162889717147\n",
      "[5, 90]: loss: 0.008488250474329107\n",
      "[6, 5]: loss: 0.009848613000940531\n",
      "[6, 10]: loss: 0.06641488731838763\n",
      "[6, 15]: loss: 0.055601270229090005\n",
      "[6, 20]: loss: 0.010141299950191751\n",
      "[6, 25]: loss: 0.038575553451664746\n",
      "[6, 30]: loss: 0.012594435189384967\n",
      "[6, 35]: loss: 0.033098718035034835\n",
      "[6, 40]: loss: 0.008073835488175973\n",
      "[6, 45]: loss: 0.042181806202279404\n",
      "[6, 50]: loss: 0.008437401906121522\n",
      "[6, 55]: loss: 0.005699578730855137\n",
      "[6, 60]: loss: 0.002559685002779588\n",
      "[6, 65]: loss: 0.021859332380699925\n",
      "[6, 70]: loss: 0.05836350878234953\n",
      "[6, 75]: loss: 0.0434674957068637\n",
      "[6, 80]: loss: 0.0062330828222911805\n",
      "[6, 85]: loss: 0.002802880640956573\n",
      "[6, 90]: loss: 0.006949327180336695\n",
      "[7, 5]: loss: 0.020922406343743205\n",
      "[7, 10]: loss: 0.013389984189416282\n",
      "[7, 15]: loss: 0.007851773960283026\n",
      "[7, 20]: loss: 0.003784874774282798\n",
      "[7, 25]: loss: 0.0029260216397233307\n",
      "[7, 30]: loss: 0.020819758297875524\n",
      "[7, 35]: loss: 0.004686653119279072\n",
      "[7, 40]: loss: 0.010178644326515496\n",
      "[7, 45]: loss: 0.008248138939961791\n",
      "[7, 50]: loss: 0.004454446025192738\n",
      "[7, 55]: loss: 0.009229104121914133\n",
      "[7, 60]: loss: 0.02676754485582933\n",
      "[7, 65]: loss: 0.015503404225455597\n",
      "[7, 70]: loss: 0.012510932050645351\n",
      "[7, 75]: loss: 0.004464651719899848\n",
      "[7, 80]: loss: 0.006054199446225539\n",
      "[7, 85]: loss: 0.004638375306967646\n",
      "[7, 90]: loss: 0.009204724250594154\n",
      "[8, 5]: loss: 0.006019784050295129\n",
      "[8, 10]: loss: 0.00632029529515421\n",
      "[8, 15]: loss: 0.007192462027887814\n",
      "[8, 20]: loss: 0.0627903628628701\n",
      "[8, 25]: loss: 0.005174137826543301\n",
      "[8, 30]: loss: 0.0047275752876885235\n",
      "[8, 35]: loss: 0.019237973087001592\n",
      "[8, 40]: loss: 0.013869526927010156\n",
      "[8, 45]: loss: 0.006199137918883935\n",
      "[8, 50]: loss: 0.002645787928486243\n",
      "[8, 55]: loss: 0.02041392878163606\n",
      "[8, 60]: loss: 0.004926727066049352\n",
      "[8, 65]: loss: 0.0060332523280521855\n",
      "[8, 70]: loss: 0.0025143137463601306\n",
      "[8, 75]: loss: 0.005980178160825744\n",
      "[8, 80]: loss: 0.029189161956310272\n",
      "[8, 85]: loss: 0.007149254175601527\n",
      "[8, 90]: loss: 0.0020605589015758596\n",
      "[9, 5]: loss: 0.00616226319107227\n",
      "[9, 10]: loss: 0.006535087042720988\n",
      "[9, 15]: loss: 0.006983095372561365\n",
      "[9, 20]: loss: 0.0041122652182821184\n",
      "[9, 25]: loss: 0.01424738226342015\n",
      "[9, 30]: loss: 0.003565841747331433\n",
      "[9, 35]: loss: 0.0037997670006006956\n",
      "[9, 40]: loss: 0.008990274407551624\n",
      "[9, 45]: loss: 0.002216006556409411\n",
      "[9, 50]: loss: 0.003373923886101693\n",
      "[9, 55]: loss: 0.002648986061103642\n",
      "[9, 60]: loss: 0.003251214075135067\n",
      "[9, 65]: loss: 0.0035015475878026336\n",
      "[9, 70]: loss: 0.0057961878046626225\n",
      "[9, 75]: loss: 0.008303259863168932\n",
      "[9, 80]: loss: 0.002084531672153389\n",
      "[9, 85]: loss: 0.003513999457936734\n",
      "[9, 90]: loss: 0.006183688317833003\n",
      "[10, 5]: loss: 0.003280239616287872\n",
      "[10, 10]: loss: 0.004929882532451302\n",
      "[10, 15]: loss: 0.005659561822540127\n",
      "[10, 20]: loss: 0.0028936031594639644\n",
      "[10, 25]: loss: 0.0021809647732879966\n",
      "[10, 30]: loss: 0.023618772538611665\n",
      "[10, 35]: loss: 0.0038402785939979367\n",
      "[10, 40]: loss: 0.004416658019181341\n",
      "[10, 45]: loss: 0.0038954481424298137\n",
      "[10, 50]: loss: 0.003575674847525079\n",
      "[10, 55]: loss: 0.013323705177754164\n",
      "[10, 60]: loss: 0.0024797546066110954\n",
      "[10, 65]: loss: 0.004863049442064948\n",
      "[10, 70]: loss: 0.004653425537981093\n",
      "[10, 75]: loss: 0.0034664231789065525\n",
      "[10, 80]: loss: 0.0029041480884188786\n",
      "[10, 85]: loss: 0.005384213800425641\n",
      "[10, 90]: loss: 0.005040490301325917\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 10: 0.4968\n",
      "Random Sampling Iteration 11\n",
      "[1, 5]: loss: 0.13124447944574058\n",
      "[1, 10]: loss: 0.5798338185995817\n",
      "[1, 15]: loss: 0.18099164112936705\n",
      "[1, 20]: loss: 0.3800168540328741\n",
      "[1, 25]: loss: 0.22578902449458838\n",
      "[1, 30]: loss: 0.7085348537075333\n",
      "[1, 35]: loss: 0.260267075675074\n",
      "[1, 40]: loss: 0.09390100417658687\n",
      "[1, 45]: loss: 0.20786145702004433\n",
      "[1, 50]: loss: 0.16270965547300875\n",
      "[1, 55]: loss: 0.13066919497214258\n",
      "[1, 60]: loss: 0.17425381671637297\n",
      "[1, 65]: loss: 0.30252919159829617\n",
      "[1, 70]: loss: 0.2097253092797473\n",
      "[1, 75]: loss: 0.389230165630579\n",
      "[1, 80]: loss: 0.5717052321415395\n",
      "[1, 85]: loss: 0.3046614952618256\n",
      "[1, 90]: loss: 0.4561375379562378\n",
      "[1, 95]: loss: 0.20726935053244233\n",
      "[1, 100]: loss: 0.4782599192112684\n",
      "[2, 5]: loss: 0.12775728525593877\n",
      "[2, 10]: loss: 0.08505965024232864\n",
      "[2, 15]: loss: 0.0792577681131661\n",
      "[2, 20]: loss: 0.04869234631769359\n",
      "[2, 25]: loss: 0.03420904732774943\n",
      "[2, 30]: loss: 0.07618340943008661\n",
      "[2, 35]: loss: 0.07752637386147399\n",
      "[2, 40]: loss: 0.027102463063783944\n",
      "[2, 45]: loss: 0.07658815151080489\n",
      "[2, 50]: loss: 0.08980480104219168\n",
      "[2, 55]: loss: 0.09776121168397367\n",
      "[2, 60]: loss: 0.04446199722588062\n",
      "[2, 65]: loss: 0.1043871397851035\n",
      "[2, 70]: loss: 0.09857414331054315\n",
      "[2, 75]: loss: 0.021447940729558468\n",
      "[2, 80]: loss: 0.0730971047305502\n",
      "[2, 85]: loss: 0.0476469931891188\n",
      "[2, 90]: loss: 0.029510742286220193\n",
      "[2, 95]: loss: 0.08501204405911267\n",
      "[2, 100]: loss: 0.1204973568674177\n",
      "[3, 5]: loss: 0.06685685057891533\n",
      "[3, 10]: loss: 0.06006202707067132\n",
      "[3, 15]: loss: 0.022333229542709887\n",
      "[3, 20]: loss: 0.042610173841239884\n",
      "[3, 25]: loss: 0.009843361680395901\n",
      "[3, 30]: loss: 0.04399324831319973\n",
      "[3, 35]: loss: 0.02915056818164885\n",
      "[3, 40]: loss: 0.026982725830748677\n",
      "[3, 45]: loss: 0.013914935232605785\n",
      "[3, 50]: loss: 0.126893213018775\n",
      "[3, 55]: loss: 0.03901155712082982\n",
      "[3, 60]: loss: 0.018302364565897733\n",
      "[3, 65]: loss: 0.0520477385725826\n",
      "[3, 70]: loss: 0.017809160344768316\n",
      "[3, 75]: loss: 0.03191407211124897\n",
      "[3, 80]: loss: 0.03487469849642366\n",
      "[3, 85]: loss: 0.05876772984629497\n",
      "[3, 90]: loss: 0.016670512733981013\n",
      "[3, 95]: loss: 0.00713371328311041\n",
      "[3, 100]: loss: 0.013115835725329816\n",
      "[4, 5]: loss: 0.005018363852286711\n",
      "[4, 10]: loss: 0.009087257087230682\n",
      "[4, 15]: loss: 0.010529138729907572\n",
      "[4, 20]: loss: 0.00882141619513277\n",
      "[4, 25]: loss: 0.0048193623078987\n",
      "[4, 30]: loss: 0.004169386433204636\n",
      "[4, 35]: loss: 0.007538325633504428\n",
      "[4, 40]: loss: 0.004819102803594433\n",
      "[4, 45]: loss: 0.002952067618025467\n",
      "[4, 50]: loss: 0.002275010076118633\n",
      "[4, 55]: loss: 0.005909561907174066\n",
      "[4, 60]: loss: 0.004904174362309277\n",
      "[4, 65]: loss: 0.06657617895689327\n",
      "[4, 70]: loss: 0.021323348337318748\n",
      "[4, 75]: loss: 0.04205006849952042\n",
      "[4, 80]: loss: 0.02396399906137958\n",
      "[4, 85]: loss: 0.025932200747774914\n",
      "[4, 90]: loss: 0.04413048620335758\n",
      "[4, 95]: loss: 0.03586796042509377\n",
      "[4, 100]: loss: 0.005227552188443951\n",
      "[5, 5]: loss: 0.007070096442475915\n",
      "[5, 10]: loss: 0.005622976561426185\n",
      "[5, 15]: loss: 0.006750566331902519\n",
      "[5, 20]: loss: 0.006254271662328392\n",
      "[5, 25]: loss: 0.00781344750430435\n",
      "[5, 30]: loss: 0.0358407900785096\n",
      "[5, 35]: loss: 0.003920575210941024\n",
      "[5, 40]: loss: 0.00932981482037576\n",
      "[5, 45]: loss: 0.002696078212466091\n",
      "[5, 50]: loss: 0.017628694360610098\n",
      "[5, 55]: loss: 0.012472997244913131\n",
      "[5, 60]: loss: 0.01211961021181196\n",
      "[5, 65]: loss: 0.005001461395295337\n",
      "[5, 70]: loss: 0.005072481464594603\n",
      "[5, 75]: loss: 0.021808879653690383\n",
      "[5, 80]: loss: 0.020690491830464453\n",
      "[5, 85]: loss: 0.014858699141768739\n",
      "[5, 90]: loss: 0.010078592225909233\n",
      "[5, 95]: loss: 0.00604358140117256\n",
      "[5, 100]: loss: 0.015776102954987437\n",
      "[6, 5]: loss: 0.004337014528573491\n",
      "[6, 10]: loss: 0.006170330620079767\n",
      "[6, 15]: loss: 0.016600516319158487\n",
      "[6, 20]: loss: 0.03509310935623944\n",
      "[6, 25]: loss: 0.00932429262320511\n",
      "[6, 30]: loss: 0.012179330566141289\n",
      "[6, 35]: loss: 0.0031811418884899467\n",
      "[6, 40]: loss: 0.012277161848032847\n",
      "[6, 45]: loss: 0.008116780300042592\n",
      "[6, 50]: loss: 0.003871988315950148\n",
      "[6, 55]: loss: 0.007696806467720307\n",
      "[6, 60]: loss: 0.012789980908564758\n",
      "[6, 65]: loss: 0.0048627521900925785\n",
      "[6, 70]: loss: 0.004020550797577016\n",
      "[6, 75]: loss: 0.01936460449360311\n",
      "[6, 80]: loss: 0.07806352938496275\n",
      "[6, 85]: loss: 0.012730462156469002\n",
      "[6, 90]: loss: 0.0019272291247034445\n",
      "[6, 95]: loss: 0.007417101500323042\n",
      "[6, 100]: loss: 0.0030466541065834463\n",
      "[7, 5]: loss: 0.004818357730982825\n",
      "[7, 10]: loss: 0.006955820077564567\n",
      "[7, 15]: loss: 0.0035855497990269214\n",
      "[7, 20]: loss: 0.006189325329614803\n",
      "[7, 25]: loss: 0.004557643143925816\n",
      "[7, 30]: loss: 0.003799400001298636\n",
      "[7, 35]: loss: 0.003985773219028488\n",
      "[7, 40]: loss: 0.008840589580358937\n",
      "[7, 45]: loss: 0.0031315080050262623\n",
      "[7, 50]: loss: 0.0032923486578511074\n",
      "[7, 55]: loss: 0.0018525970954215154\n",
      "[7, 60]: loss: 0.0013294351665535942\n",
      "[7, 65]: loss: 0.017857989441836253\n",
      "[7, 70]: loss: 0.009743597780470736\n",
      "[7, 75]: loss: 0.004444278471055441\n",
      "[7, 80]: loss: 0.002579013802460395\n",
      "[7, 85]: loss: 0.0030073093075770885\n",
      "[7, 90]: loss: 0.012472713729948737\n",
      "[7, 95]: loss: 0.013090396227198653\n",
      "[7, 100]: loss: 0.0028048963213223033\n",
      "[8, 5]: loss: 0.004352303134510294\n",
      "[8, 10]: loss: 0.012623434959095903\n",
      "[8, 15]: loss: 0.002627790643600747\n",
      "[8, 20]: loss: 0.005041639466071501\n",
      "[8, 25]: loss: 0.00453784572891891\n",
      "[8, 30]: loss: 0.01079274368021288\n",
      "[8, 35]: loss: 0.0016641627662465908\n",
      "[8, 40]: loss: 0.005939052993198857\n",
      "[8, 45]: loss: 0.04289069612423191\n",
      "[8, 50]: loss: 0.006350196315906942\n",
      "[8, 55]: loss: 0.007124288575141691\n",
      "[8, 60]: loss: 0.004233065279549919\n",
      "[8, 65]: loss: 0.002991568762809038\n",
      "[8, 70]: loss: 0.0021135627466719598\n",
      "[8, 75]: loss: 0.005080422110040672\n",
      "[8, 80]: loss: 0.006483908815425821\n",
      "[8, 85]: loss: 0.01002823251474183\n",
      "[8, 90]: loss: 0.021990126930177212\n",
      "[8, 95]: loss: 0.0053218131652101874\n",
      "[8, 100]: loss: 0.0017901443243317772\n",
      "[9, 5]: loss: 0.004765165416756645\n",
      "[9, 10]: loss: 0.0047848986578173935\n",
      "[9, 15]: loss: 0.004059823550051078\n",
      "[9, 20]: loss: 0.0019199407834094018\n",
      "[9, 25]: loss: 0.0015162566414801404\n",
      "[9, 30]: loss: 0.008592276077251881\n",
      "[9, 35]: loss: 0.003996666971943341\n",
      "[9, 40]: loss: 0.001259846831089817\n",
      "[9, 45]: loss: 0.0021884977613808587\n",
      "[9, 50]: loss: 0.028425665601389483\n",
      "[9, 55]: loss: 0.004756278045533691\n",
      "[9, 60]: loss: 0.001484771360992454\n",
      "[9, 65]: loss: 0.0010524304052523803\n",
      "[9, 70]: loss: 0.0027263029987807386\n",
      "[9, 75]: loss: 0.016693263518391177\n",
      "[9, 80]: loss: 0.001675766114203725\n",
      "[9, 85]: loss: 0.006091983726946637\n",
      "[9, 90]: loss: 0.004081834951648489\n",
      "[9, 95]: loss: 0.0025005733405123465\n",
      "[9, 100]: loss: 0.01166316180024296\n",
      "[10, 5]: loss: 0.003847718035103753\n",
      "[10, 10]: loss: 0.002837477019056678\n",
      "[10, 15]: loss: 0.0028623223770409822\n",
      "[10, 20]: loss: 0.001726680184219731\n",
      "[10, 25]: loss: 0.0032030287802626844\n",
      "[10, 30]: loss: 0.002030795905739069\n",
      "[10, 35]: loss: 0.0027891673089470714\n",
      "[10, 40]: loss: 0.0041679823443701025\n",
      "[10, 45]: loss: 0.004142253485042602\n",
      "[10, 50]: loss: 0.0030569054288207553\n",
      "[10, 55]: loss: 0.004157591072726063\n",
      "[10, 60]: loss: 0.002169276398490183\n",
      "[10, 65]: loss: 0.0032209562341449782\n",
      "[10, 70]: loss: 0.002044603788817767\n",
      "[10, 75]: loss: 0.0014585513708880171\n",
      "[10, 80]: loss: 0.0012652007208089344\n",
      "[10, 85]: loss: 0.005828098117490299\n",
      "[10, 90]: loss: 0.011189418921276229\n",
      "[10, 95]: loss: 0.0020241662496118806\n",
      "[10, 100]: loss: 0.01219771264004521\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 11: 0.5136\n",
      "Random Sampling Iteration 12\n",
      "[1, 5]: loss: 0.024229293514508754\n",
      "[1, 10]: loss: 0.030107977363513783\n",
      "[1, 15]: loss: 0.39061875990591943\n",
      "[1, 20]: loss: 0.1781503155361861\n",
      "[1, 25]: loss: 0.5728217270225286\n",
      "[1, 30]: loss: 0.44778588972985744\n",
      "[1, 35]: loss: 0.6886290861293674\n",
      "[1, 40]: loss: 0.18163069151341915\n",
      "[1, 45]: loss: 0.3713054161053151\n",
      "[1, 50]: loss: 0.19462961296085268\n",
      "[1, 55]: loss: 0.12395164393819869\n",
      "[1, 60]: loss: 0.1710953307338059\n",
      "[1, 65]: loss: 0.3194592256040778\n",
      "[1, 70]: loss: 0.39234839123673737\n",
      "[1, 75]: loss: 0.3484071569982916\n",
      "[1, 80]: loss: 0.26289806328713894\n",
      "[1, 85]: loss: 0.1849897583015263\n",
      "[1, 90]: loss: 0.12894673785194755\n",
      "[1, 95]: loss: 0.4032326810993254\n",
      "[1, 100]: loss: 0.20349860738497227\n",
      "[1, 105]: loss: 0.38338823802769184\n",
      "[2, 5]: loss: 0.10214722994714975\n",
      "[2, 10]: loss: 0.051683268888155\n",
      "[2, 15]: loss: 0.152046381495893\n",
      "[2, 20]: loss: 0.09761132195126265\n",
      "[2, 25]: loss: 0.10652512684464455\n",
      "[2, 30]: loss: 0.08648272231221199\n",
      "[2, 35]: loss: 0.16041155043058097\n",
      "[2, 40]: loss: 0.08714823564514518\n",
      "[2, 45]: loss: 0.05359291780041531\n",
      "[2, 50]: loss: 0.05499113106634468\n",
      "[2, 55]: loss: 0.040250670397654176\n",
      "[2, 60]: loss: 0.048749406123533845\n",
      "[2, 65]: loss: 0.12276136223226786\n",
      "[2, 70]: loss: 0.14161697402596474\n",
      "[2, 75]: loss: 0.04910746112000197\n",
      "[2, 80]: loss: 0.09210233273915946\n",
      "[2, 85]: loss: 0.09161301492713392\n",
      "[2, 90]: loss: 0.19772769487462938\n",
      "[2, 95]: loss: 0.043412249331595376\n",
      "[2, 100]: loss: 0.05057227308861911\n",
      "[2, 105]: loss: 0.16411281656473875\n",
      "[3, 5]: loss: 0.03397723170928657\n",
      "[3, 10]: loss: 0.02208322713704547\n",
      "[3, 15]: loss: 0.09753575967624784\n",
      "[3, 20]: loss: 0.05606233040452935\n",
      "[3, 25]: loss: 0.1370239183306694\n",
      "[3, 30]: loss: 0.09592529843212105\n",
      "[3, 35]: loss: 0.007595485774800181\n",
      "[3, 40]: loss: 0.05208660289645195\n",
      "[3, 45]: loss: 0.014219349657651037\n",
      "[3, 50]: loss: 0.09809665754437447\n",
      "[3, 55]: loss: 0.028226332098711282\n",
      "[3, 60]: loss: 0.019506206328514963\n",
      "[3, 65]: loss: 0.06896864681039006\n",
      "[3, 70]: loss: 0.04281069245189428\n",
      "[3, 75]: loss: 0.05345827550627291\n",
      "[3, 80]: loss: 0.050751110597047955\n",
      "[3, 85]: loss: 0.13033910025842488\n",
      "[3, 90]: loss: 0.15278570470400155\n",
      "[3, 95]: loss: 0.02274233422940597\n",
      "[3, 100]: loss: 0.11094266269356012\n",
      "[3, 105]: loss: 0.017514461069367826\n",
      "[4, 5]: loss: 0.01749441324500367\n",
      "[4, 10]: loss: 0.018348137236898765\n",
      "[4, 15]: loss: 0.04724877921398729\n",
      "[4, 20]: loss: 0.0034254560014232993\n",
      "[4, 25]: loss: 0.006700097292196006\n",
      "[4, 30]: loss: 0.037283252575434744\n",
      "[4, 35]: loss: 0.00840951302961912\n",
      "[4, 40]: loss: 0.011240658815950155\n",
      "[4, 45]: loss: 0.008160874160239473\n",
      "[4, 50]: loss: 0.04318457082263194\n",
      "[4, 55]: loss: 0.010529419087106362\n",
      "[4, 60]: loss: 0.04103811081586173\n",
      "[4, 65]: loss: 0.008192776644136757\n",
      "[4, 70]: loss: 0.007557577075203881\n",
      "[4, 75]: loss: 0.03792664536740631\n",
      "[4, 80]: loss: 0.044898155611008406\n",
      "[4, 85]: loss: 0.005905289435759187\n",
      "[4, 90]: loss: 0.06094338488765061\n",
      "[4, 95]: loss: 0.002780169128527632\n",
      "[4, 100]: loss: 0.02807724435115233\n",
      "[4, 105]: loss: 0.021197507274337113\n",
      "[5, 5]: loss: 0.01161900203442201\n",
      "[5, 10]: loss: 0.06156939081847668\n",
      "[5, 15]: loss: 0.03409922955324873\n",
      "[5, 20]: loss: 0.07694180931139272\n",
      "[5, 25]: loss: 0.08508467290084809\n",
      "[5, 30]: loss: 0.017693616187898442\n",
      "[5, 35]: loss: 0.020954112813342363\n",
      "[5, 40]: loss: 0.08035282045602798\n",
      "[5, 45]: loss: 0.013292597490362823\n",
      "[5, 50]: loss: 0.21132648806087673\n",
      "[5, 55]: loss: 0.030775373801589012\n",
      "[5, 60]: loss: 0.02731044148094952\n",
      "[5, 65]: loss: 0.050348698801826686\n",
      "[5, 70]: loss: 0.014307914883829653\n",
      "[5, 75]: loss: 0.041640490177087486\n",
      "[5, 80]: loss: 0.057174165500327945\n",
      "[5, 85]: loss: 0.0576272034086287\n",
      "[5, 90]: loss: 0.12392384669510648\n",
      "[5, 95]: loss: 0.3383647894952446\n",
      "[5, 100]: loss: 0.012738709396217018\n",
      "[5, 105]: loss: 0.011649360865703784\n",
      "[6, 5]: loss: 0.02358449634630233\n",
      "[6, 10]: loss: 0.22137440706137568\n",
      "[6, 15]: loss: 0.02927779362653382\n",
      "[6, 20]: loss: 0.034657010168302804\n",
      "[6, 25]: loss: 0.07448596344329417\n",
      "[6, 30]: loss: 0.029416695280815475\n",
      "[6, 35]: loss: 0.013635614130180329\n",
      "[6, 40]: loss: 0.023314411286264658\n",
      "[6, 45]: loss: 0.015061667072586715\n",
      "[6, 50]: loss: 0.14895216037984937\n",
      "[6, 55]: loss: 0.04524454372585751\n",
      "[6, 60]: loss: 0.01783659792272374\n",
      "[6, 65]: loss: 0.06164903147146106\n",
      "[6, 70]: loss: 0.03080871037673205\n",
      "[6, 75]: loss: 0.042523980780970305\n",
      "[6, 80]: loss: 0.07525085919769481\n",
      "[6, 85]: loss: 0.022497238591313362\n",
      "[6, 90]: loss: 0.031479278579354286\n",
      "[6, 95]: loss: 0.0285838878917275\n",
      "[6, 100]: loss: 0.015897771285381168\n",
      "[6, 105]: loss: 0.018094275263138115\n",
      "[7, 5]: loss: 0.005530161899514496\n",
      "[7, 10]: loss: 0.01658600367954932\n",
      "[7, 15]: loss: 0.0046023774484638125\n",
      "[7, 20]: loss: 0.0027101394080091268\n",
      "[7, 25]: loss: 0.009095664921915159\n",
      "[7, 30]: loss: 0.006785540434066206\n",
      "[7, 35]: loss: 0.01767791621387005\n",
      "[7, 40]: loss: 0.0036872309574391693\n",
      "[7, 45]: loss: 0.005382297094911337\n",
      "[7, 50]: loss: 0.04648004868067801\n",
      "[7, 55]: loss: 0.014999048886238597\n",
      "[7, 60]: loss: 0.01252324803499505\n",
      "[7, 65]: loss: 0.0062207972223404795\n",
      "[7, 70]: loss: 0.009169181430479512\n",
      "[7, 75]: loss: 0.050416924874298275\n",
      "[7, 80]: loss: 0.004264820861862972\n",
      "[7, 85]: loss: 0.011530951305758208\n",
      "[7, 90]: loss: 0.016264643345493823\n",
      "[7, 95]: loss: 0.010929103911621496\n",
      "[7, 100]: loss: 0.02947942345053889\n",
      "[7, 105]: loss: 0.012382766813971102\n",
      "[8, 5]: loss: 0.008114721189485863\n",
      "[8, 10]: loss: 0.035299883951665834\n",
      "[8, 15]: loss: 0.01042747963219881\n",
      "[8, 20]: loss: 0.00571273360401392\n",
      "[8, 25]: loss: 0.018991557299159467\n",
      "[8, 30]: loss: 0.0049482406466268\n",
      "[8, 35]: loss: 0.004816483189642895\n",
      "[8, 40]: loss: 0.0029806294696754776\n",
      "[8, 45]: loss: 0.005379026719310787\n",
      "[8, 50]: loss: 0.024130968318786472\n",
      "[8, 55]: loss: 0.012626854469999671\n",
      "[8, 60]: loss: 0.03131655417382717\n",
      "[8, 65]: loss: 0.006627939495956525\n",
      "[8, 70]: loss: 0.007420598674798384\n",
      "[8, 75]: loss: 0.0069080511457286775\n",
      "[8, 80]: loss: 0.007676680397707969\n",
      "[8, 85]: loss: 0.0025455754657741636\n",
      "[8, 90]: loss: 0.014584988355636597\n",
      "[8, 95]: loss: 0.014215962604794186\n",
      "[8, 100]: loss: 0.005430883058579639\n",
      "[8, 105]: loss: 0.0018942521637654863\n",
      "[9, 5]: loss: 0.0033373498808941804\n",
      "[9, 10]: loss: 0.002864061971195042\n",
      "[9, 15]: loss: 0.003840373072307557\n",
      "[9, 20]: loss: 0.006487908365670592\n",
      "[9, 25]: loss: 0.003712967096362263\n",
      "[9, 30]: loss: 0.015591050891089253\n",
      "[9, 35]: loss: 0.007274420066096354\n",
      "[9, 40]: loss: 0.018071950500598177\n",
      "[9, 45]: loss: 0.006585901021026075\n",
      "[9, 50]: loss: 0.0065554956017876975\n",
      "[9, 55]: loss: 0.004989370994735509\n",
      "[9, 60]: loss: 0.004190526626189239\n",
      "[9, 65]: loss: 0.002245161660539452\n",
      "[9, 70]: loss: 0.009034987422637641\n",
      "[9, 75]: loss: 0.0035588102764450014\n",
      "[9, 80]: loss: 0.0033580652670934796\n",
      "[9, 85]: loss: 0.004620343563146889\n",
      "[9, 90]: loss: 0.00592828071967233\n",
      "[9, 95]: loss: 0.003125531322439201\n",
      "[9, 100]: loss: 0.007428907658322714\n",
      "[9, 105]: loss: 0.008086340123554692\n",
      "[10, 5]: loss: 0.007759499771054834\n",
      "[10, 10]: loss: 0.00535052441409789\n",
      "[10, 15]: loss: 0.017827194576966576\n",
      "[10, 20]: loss: 0.005567126252572052\n",
      "[10, 25]: loss: 0.005550436588237062\n",
      "[10, 30]: loss: 0.004396372765768319\n",
      "[10, 35]: loss: 0.0075064376287627965\n",
      "[10, 40]: loss: 0.005748099196352996\n",
      "[10, 45]: loss: 0.006359082428389229\n",
      "[10, 50]: loss: 0.01129256427520886\n",
      "[10, 55]: loss: 0.00392126344377175\n",
      "[10, 60]: loss: 0.008699012076249346\n",
      "[10, 65]: loss: 0.003467759925115388\n",
      "[10, 70]: loss: 0.006877706215163926\n",
      "[10, 75]: loss: 0.0034142167714890093\n",
      "[10, 80]: loss: 0.010003145929658785\n",
      "[10, 85]: loss: 0.004713658941909671\n",
      "[10, 90]: loss: 0.008771042703301646\n",
      "[10, 95]: loss: 0.010209823100012727\n",
      "[10, 100]: loss: 0.004899569059489295\n",
      "[10, 105]: loss: 0.02111334884830285\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 12: 0.5102\n",
      "Random Sampling Iteration 13\n",
      "[1, 5]: loss: 0.13401979161426425\n",
      "[1, 10]: loss: 0.1268334326450713\n",
      "[1, 15]: loss: 0.38273585558636114\n",
      "[1, 20]: loss: 0.1188857015222311\n",
      "[1, 25]: loss: 0.1114609797950834\n",
      "[1, 30]: loss: 0.04556414904072881\n",
      "[1, 35]: loss: 0.19098930049221963\n",
      "[1, 40]: loss: 0.15807938016951084\n",
      "[1, 45]: loss: 0.23012233668123372\n",
      "[1, 50]: loss: 0.7220715491566807\n",
      "[1, 55]: loss: 0.6260184766724706\n",
      "[1, 60]: loss: 0.5125353862531483\n",
      "[1, 65]: loss: 0.2142326559405774\n",
      "[1, 70]: loss: 0.18509425478987396\n",
      "[1, 75]: loss: 0.42218284122645855\n",
      "[1, 80]: loss: 0.2057393193244934\n",
      "[1, 85]: loss: 0.39070496999192983\n",
      "[1, 90]: loss: 0.4077345780096948\n",
      "[1, 95]: loss: 0.4177753096446395\n",
      "[1, 100]: loss: 0.44406310154590756\n",
      "[1, 105]: loss: 0.26298188092187047\n",
      "[1, 110]: loss: 0.47039415733888745\n",
      "[2, 5]: loss: 0.10449026791320648\n",
      "[2, 10]: loss: 0.03785323607735336\n",
      "[2, 15]: loss: 0.04651565640233457\n",
      "[2, 20]: loss: 0.016316806490067393\n",
      "[2, 25]: loss: 0.04415106773376465\n",
      "[2, 30]: loss: 0.030466185649856925\n",
      "[2, 35]: loss: 0.061851740814745426\n",
      "[2, 40]: loss: 0.10823272750712931\n",
      "[2, 45]: loss: 0.05301772058010101\n",
      "[2, 50]: loss: 0.028680759365670383\n",
      "[2, 55]: loss: 0.048294427106156945\n",
      "[2, 60]: loss: 0.0288333393400535\n",
      "[2, 65]: loss: 0.04559279256500304\n",
      "[2, 70]: loss: 0.041096129280049354\n",
      "[2, 75]: loss: 0.03470329858828336\n",
      "[2, 80]: loss: 0.04491461114957929\n",
      "[2, 85]: loss: 0.04300540976691991\n",
      "[2, 90]: loss: 0.04714266862720251\n",
      "[2, 95]: loss: 0.057675881776958704\n",
      "[2, 100]: loss: 0.11292251129634678\n",
      "[2, 105]: loss: 0.1028950628824532\n",
      "[2, 110]: loss: 0.019193721818737686\n",
      "[3, 5]: loss: 0.010239112074486911\n",
      "[3, 10]: loss: 0.02677965143811889\n",
      "[3, 15]: loss: 0.023223901691380888\n",
      "[3, 20]: loss: 0.12198792022536509\n",
      "[3, 25]: loss: 0.038116868352517486\n",
      "[3, 30]: loss: 0.04284901270875707\n",
      "[3, 35]: loss: 0.017054531490430236\n",
      "[3, 40]: loss: 0.04017530963756144\n",
      "[3, 45]: loss: 0.05842291301814839\n",
      "[3, 50]: loss: 0.021070454851724207\n",
      "[3, 55]: loss: 0.15339894729549997\n",
      "[3, 60]: loss: 0.005792147479951382\n",
      "[3, 65]: loss: 0.009497604623902589\n",
      "[3, 70]: loss: 0.028221607441082597\n",
      "[3, 75]: loss: 0.039182941953185946\n",
      "[3, 80]: loss: 0.034700729593168944\n",
      "[3, 85]: loss: 0.041791442257817835\n",
      "[3, 90]: loss: 0.04509353474713862\n",
      "[3, 95]: loss: 0.09630590537562966\n",
      "[3, 100]: loss: 0.0196146737143863\n",
      "[3, 105]: loss: 0.05132933193817735\n",
      "[3, 110]: loss: 0.033174750627949834\n",
      "[4, 5]: loss: 0.03334320298745297\n",
      "[4, 10]: loss: 0.0053481181093957275\n",
      "[4, 15]: loss: 0.011172578088007867\n",
      "[4, 20]: loss: 0.015871815965510905\n",
      "[4, 25]: loss: 0.041160476990626194\n",
      "[4, 30]: loss: 0.011412994877900928\n",
      "[4, 35]: loss: 0.05320837826002389\n",
      "[4, 40]: loss: 0.012948079849593341\n",
      "[4, 45]: loss: 0.020957439992344007\n",
      "[4, 50]: loss: 0.01498349366011098\n",
      "[4, 55]: loss: 0.05142955994233489\n",
      "[4, 60]: loss: 0.024784525856375694\n",
      "[4, 65]: loss: 0.05491361624444835\n",
      "[4, 70]: loss: 0.035322478448506445\n",
      "[4, 75]: loss: 0.02534058778837789\n",
      "[4, 80]: loss: 0.06766576378140599\n",
      "[4, 85]: loss: 0.04897387191886082\n",
      "[4, 90]: loss: 0.021006082330131903\n",
      "[4, 95]: loss: 0.06875165546080098\n",
      "[4, 100]: loss: 0.029609269928187132\n",
      "[4, 105]: loss: 0.08378593169618398\n",
      "[4, 110]: loss: 0.013001167681068182\n",
      "[5, 5]: loss: 0.04926156881265342\n",
      "[5, 10]: loss: 0.08776196202961728\n",
      "[5, 15]: loss: 0.015738002955913544\n",
      "[5, 20]: loss: 0.03351323527749628\n",
      "[5, 25]: loss: 0.07524018303956836\n",
      "[5, 30]: loss: 0.056914414308266714\n",
      "[5, 35]: loss: 0.08560988833778538\n",
      "[5, 40]: loss: 0.019806114432867616\n",
      "[5, 45]: loss: 0.05499194638105109\n",
      "[5, 50]: loss: 0.010956185113172978\n",
      "[5, 55]: loss: 0.02233929099747911\n",
      "[5, 60]: loss: 0.0780911737238057\n",
      "[5, 65]: loss: 0.037918639834970236\n",
      "[5, 70]: loss: 0.06211762048769742\n",
      "[5, 75]: loss: 0.056641942472197115\n",
      "[5, 80]: loss: 0.07028486604394857\n",
      "[5, 85]: loss: 0.018092787300702184\n",
      "[5, 90]: loss: 0.018904333468526602\n",
      "[5, 95]: loss: 0.036893081036396325\n",
      "[5, 100]: loss: 0.039526032662251964\n",
      "[5, 105]: loss: 0.02645708341151476\n",
      "[5, 110]: loss: 0.024641024880111217\n",
      "[6, 5]: loss: 0.03449628027374274\n",
      "[6, 10]: loss: 0.016898387926630676\n",
      "[6, 15]: loss: 0.0321981124143349\n",
      "[6, 20]: loss: 0.02015797168132849\n",
      "[6, 25]: loss: 0.005426538642495871\n",
      "[6, 30]: loss: 0.03651361493393779\n",
      "[6, 35]: loss: 0.04322106014296878\n",
      "[6, 40]: loss: 0.039626852551009506\n",
      "[6, 45]: loss: 0.011273807554971427\n",
      "[6, 50]: loss: 0.04829344048630446\n",
      "[6, 55]: loss: 0.025607332383515313\n",
      "[6, 60]: loss: 0.013738261346588843\n",
      "[6, 65]: loss: 0.01411153533263132\n",
      "[6, 70]: loss: 0.007266431610332802\n",
      "[6, 75]: loss: 0.03503967571305111\n",
      "[6, 80]: loss: 0.02464755962137133\n",
      "[6, 85]: loss: 0.011078535710112192\n",
      "[6, 90]: loss: 0.01818817415187368\n",
      "[6, 95]: loss: 0.013042172300629318\n",
      "[6, 100]: loss: 0.01362710859393701\n",
      "[6, 105]: loss: 0.004867639858275652\n",
      "[6, 110]: loss: 0.03158919423003681\n",
      "[7, 5]: loss: 0.016687050578184426\n",
      "[7, 10]: loss: 0.0049897866701940075\n",
      "[7, 15]: loss: 0.0061466480838134885\n",
      "[7, 20]: loss: 0.012605061696376652\n",
      "[7, 25]: loss: 0.008692026080098003\n",
      "[7, 30]: loss: 0.03917310509132221\n",
      "[7, 35]: loss: 0.00588729941227939\n",
      "[7, 40]: loss: 0.018838269694242626\n",
      "[7, 45]: loss: 0.013984729317598976\n",
      "[7, 50]: loss: 0.0025038964522536844\n",
      "[7, 55]: loss: 0.02216078239143826\n",
      "[7, 60]: loss: 0.0024744458205532283\n",
      "[7, 65]: loss: 0.005159714550245553\n",
      "[7, 70]: loss: 0.004342255226220004\n",
      "[7, 75]: loss: 0.008796751906629652\n",
      "[7, 80]: loss: 0.028695294618955813\n",
      "[7, 85]: loss: 0.012052988662617281\n",
      "[7, 90]: loss: 0.005649209575494751\n",
      "[7, 95]: loss: 0.010931829863693565\n",
      "[7, 100]: loss: 0.0040375715179834515\n",
      "[7, 105]: loss: 0.014483491540886462\n",
      "[7, 110]: loss: 0.024445577371807303\n",
      "[8, 5]: loss: 0.021825912757776678\n",
      "[8, 10]: loss: 0.0019372946117073298\n",
      "[8, 15]: loss: 0.015947975334711373\n",
      "[8, 20]: loss: 0.013594291966001038\n",
      "[8, 25]: loss: 0.010405987050035037\n",
      "[8, 30]: loss: 0.012612209393410012\n",
      "[8, 35]: loss: 0.016368870274163783\n",
      "[8, 40]: loss: 0.004133390248171054\n",
      "[8, 45]: loss: 0.020544526487356052\n",
      "[8, 50]: loss: 0.010038468521088362\n",
      "[8, 55]: loss: 0.007154417020501569\n",
      "[8, 60]: loss: 0.006168736319523305\n",
      "[8, 65]: loss: 0.017150869069155306\n",
      "[8, 70]: loss: 0.009422893126611598\n",
      "[8, 75]: loss: 0.00427841252530925\n",
      "[8, 80]: loss: 0.011190860823262483\n",
      "[8, 85]: loss: 0.006807005484006368\n",
      "[8, 90]: loss: 0.0017242970498045906\n",
      "[8, 95]: loss: 0.0032901814847718924\n",
      "[8, 100]: loss: 0.005314221642038319\n",
      "[8, 105]: loss: 0.002386451553320512\n",
      "[8, 110]: loss: 0.01154375815531239\n",
      "[9, 5]: loss: 0.0261315863172058\n",
      "[9, 10]: loss: 0.0018658503540791571\n",
      "[9, 15]: loss: 0.021667311724741012\n",
      "[9, 20]: loss: 0.008466027531540021\n",
      "[9, 25]: loss: 0.007814748300006613\n",
      "[9, 30]: loss: 0.005297339586832095\n",
      "[9, 35]: loss: 0.012577561610669363\n",
      "[9, 40]: loss: 0.008895654682419263\n",
      "[9, 45]: loss: 0.003823824052233249\n",
      "[9, 50]: loss: 0.01705955175566487\n",
      "[9, 55]: loss: 0.030233623139793053\n",
      "[9, 60]: loss: 0.0024066932528512552\n",
      "[9, 65]: loss: 0.039616578607819974\n",
      "[9, 70]: loss: 0.0018161642183258664\n",
      "[9, 75]: loss: 0.007138925764593296\n",
      "[9, 80]: loss: 0.004487987112952396\n",
      "[9, 85]: loss: 0.0026819131308002397\n",
      "[9, 90]: loss: 0.0038277991552604362\n",
      "[9, 95]: loss: 0.010764881270006299\n",
      "[9, 100]: loss: 0.007613501482410356\n",
      "[9, 105]: loss: 0.0040093707939377055\n",
      "[9, 110]: loss: 0.002876896149246022\n",
      "[10, 5]: loss: 0.003344760654726997\n",
      "[10, 10]: loss: 0.011581259546801448\n",
      "[10, 15]: loss: 0.0028802146553061903\n",
      "[10, 20]: loss: 0.0014534623478539288\n",
      "[10, 25]: loss: 0.00288728010491468\n",
      "[10, 30]: loss: 0.010076924692839384\n",
      "[10, 35]: loss: 0.002663186562131159\n",
      "[10, 40]: loss: 0.0012518177172751166\n",
      "[10, 45]: loss: 0.003022264951141551\n",
      "[10, 50]: loss: 0.007927366779767908\n",
      "[10, 55]: loss: 0.004405430285260081\n",
      "[10, 60]: loss: 0.004335903147875797\n",
      "[10, 65]: loss: 0.0023883676185505465\n",
      "[10, 70]: loss: 0.0030496792460326105\n",
      "[10, 75]: loss: 0.006053332042938564\n",
      "[10, 80]: loss: 0.022732836761861108\n",
      "[10, 85]: loss: 0.03220248316938523\n",
      "[10, 90]: loss: 0.0010157822325709276\n",
      "[10, 95]: loss: 0.005155536371603375\n",
      "[10, 100]: loss: 0.001790220798284281\n",
      "[10, 105]: loss: 0.005231144037679769\n",
      "[10, 110]: loss: 0.01236713380785659\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 13: 0.5169\n",
      "Random Sampling Iteration 14\n",
      "[1, 5]: loss: 0.11964248008735012\n",
      "[1, 10]: loss: 0.2807966403197497\n",
      "[1, 15]: loss: 0.17698655044659972\n",
      "[1, 20]: loss: 0.10317615687381476\n",
      "[1, 25]: loss: 0.35840050876140594\n",
      "[1, 30]: loss: 0.4820864626672119\n",
      "[1, 35]: loss: 0.4957305663265288\n",
      "[1, 40]: loss: 0.24428690504282713\n",
      "[1, 45]: loss: 0.04770502477185801\n",
      "[1, 50]: loss: 0.15504809975391254\n",
      "[1, 55]: loss: 0.5161541936104186\n",
      "[1, 60]: loss: 0.1608325126580894\n",
      "[1, 65]: loss: 0.1416245549917221\n",
      "[1, 70]: loss: 0.2902216427028179\n",
      "[1, 75]: loss: 0.524399385554716\n",
      "[1, 80]: loss: 0.16598288901150227\n",
      "[1, 85]: loss: 0.3433628808706999\n",
      "[1, 90]: loss: 0.16259763902053237\n",
      "[1, 95]: loss: 0.18910950527060777\n",
      "[1, 100]: loss: 0.40271753119304776\n",
      "[1, 105]: loss: 0.1829705024138093\n",
      "[1, 110]: loss: 0.15473000472411513\n",
      "[1, 115]: loss: 0.5500746998004615\n",
      "[2, 5]: loss: 0.05705589524586685\n",
      "[2, 10]: loss: 0.05598995659966022\n",
      "[2, 15]: loss: 0.028806749265640974\n",
      "[2, 20]: loss: 0.10598592413589358\n",
      "[2, 25]: loss: 0.045400609727948904\n",
      "[2, 30]: loss: 0.042115614167414606\n",
      "[2, 35]: loss: 0.11025299708126113\n",
      "[2, 40]: loss: 0.02367233345285058\n",
      "[2, 45]: loss: 0.05543520100763999\n",
      "[2, 50]: loss: 0.1421687393449247\n",
      "[2, 55]: loss: 0.07317880951450206\n",
      "[2, 60]: loss: 0.040636824182001874\n",
      "[2, 65]: loss: 0.03721993835642934\n",
      "[2, 70]: loss: 0.03989075368735939\n",
      "[2, 75]: loss: 0.10206253337673843\n",
      "[2, 80]: loss: 0.020502801635302603\n",
      "[2, 85]: loss: 0.056953020161017776\n",
      "[2, 90]: loss: 0.055290453135967255\n",
      "[2, 95]: loss: 0.07666691532358527\n",
      "[2, 100]: loss: 0.024999289715196937\n",
      "[2, 105]: loss: 0.04292600363260135\n",
      "[2, 110]: loss: 0.01714682235615328\n",
      "[2, 115]: loss: 0.026223787484923378\n",
      "[3, 5]: loss: 0.03432814416009933\n",
      "[3, 10]: loss: 0.059406411775853485\n",
      "[3, 15]: loss: 0.0580333867110312\n",
      "[3, 20]: loss: 0.020468661794438958\n",
      "[3, 25]: loss: 0.024177391751436517\n",
      "[3, 30]: loss: 0.01362751261331141\n",
      "[3, 35]: loss: 0.034590307099279016\n",
      "[3, 40]: loss: 0.024424638715572655\n",
      "[3, 45]: loss: 0.026549484085990116\n",
      "[3, 50]: loss: 0.06862034852383658\n",
      "[3, 55]: loss: 0.008033014761167578\n",
      "[3, 60]: loss: 0.022404303890652955\n",
      "[3, 65]: loss: 0.10555861139437184\n",
      "[3, 70]: loss: 0.011635630944510922\n",
      "[3, 75]: loss: 0.03395085423835553\n",
      "[3, 80]: loss: 0.08718065253924578\n",
      "[3, 85]: loss: 0.008945600464357994\n",
      "[3, 90]: loss: 0.010531270512728952\n",
      "[3, 95]: loss: 0.015924281848128885\n",
      "[3, 100]: loss: 0.06462765880860388\n",
      "[3, 105]: loss: 0.11339539871551096\n",
      "[3, 110]: loss: 0.004380595579277724\n",
      "[3, 115]: loss: 0.1738945059478283\n",
      "[4, 5]: loss: 0.046370248950552195\n",
      "[4, 10]: loss: 0.05069915938656777\n",
      "[4, 15]: loss: 0.015181593014858663\n",
      "[4, 20]: loss: 0.010554656095337123\n",
      "[4, 25]: loss: 0.07540989108383656\n",
      "[4, 30]: loss: 0.027840375900268555\n",
      "[4, 35]: loss: 0.026387724064989015\n",
      "[4, 40]: loss: 0.008360962630831636\n",
      "[4, 45]: loss: 0.09060370770748705\n",
      "[4, 50]: loss: 0.012665665446547791\n",
      "[4, 55]: loss: 0.014122561668045819\n",
      "[4, 60]: loss: 0.06836315902182832\n",
      "[4, 65]: loss: 0.03990380873437971\n",
      "[4, 70]: loss: 0.013819995685480535\n",
      "[4, 75]: loss: 0.03464516659732908\n",
      "[4, 80]: loss: 0.05084686659392901\n",
      "[4, 85]: loss: 0.052032409934327006\n",
      "[4, 90]: loss: 0.06057153790607117\n",
      "[4, 95]: loss: 0.050110796524677426\n",
      "[4, 100]: loss: 0.008515989116858691\n",
      "[4, 105]: loss: 0.03135435446165502\n",
      "[4, 110]: loss: 0.011611780151724815\n",
      "[4, 115]: loss: 0.018564973142929375\n",
      "[5, 5]: loss: 0.01113621360855177\n",
      "[5, 10]: loss: 0.021663236315362155\n",
      "[5, 15]: loss: 0.012034794344799593\n",
      "[5, 20]: loss: 0.009646466205595061\n",
      "[5, 25]: loss: 0.004983857863408048\n",
      "[5, 30]: loss: 0.009011368703795597\n",
      "[5, 35]: loss: 0.036287365190219134\n",
      "[5, 40]: loss: 0.02044618030777201\n",
      "[5, 45]: loss: 0.008174358634278178\n",
      "[5, 50]: loss: 0.007231342780869454\n",
      "[5, 55]: loss: 0.03811700215737801\n",
      "[5, 60]: loss: 0.014698738523293287\n",
      "[5, 65]: loss: 0.00836368951422628\n",
      "[5, 70]: loss: 0.007981375056260731\n",
      "[5, 75]: loss: 0.008071503893006593\n",
      "[5, 80]: loss: 0.013414230546914041\n",
      "[5, 85]: loss: 0.025832478771917522\n",
      "[5, 90]: loss: 0.11810827421140857\n",
      "[5, 95]: loss: 0.0036550369986798614\n",
      "[5, 100]: loss: 0.006576669111382216\n",
      "[5, 105]: loss: 0.10825240990379825\n",
      "[5, 110]: loss: 0.01355534273898229\n",
      "[5, 115]: loss: 0.0521538516622968\n",
      "[6, 5]: loss: 0.009639404990593903\n",
      "[6, 10]: loss: 0.005847649008501321\n",
      "[6, 15]: loss: 0.019018588354811072\n",
      "[6, 20]: loss: 0.009462858637562022\n",
      "[6, 25]: loss: 0.004905044217593968\n",
      "[6, 30]: loss: 0.0231084033875959\n",
      "[6, 35]: loss: 0.05080933522549458\n",
      "[6, 40]: loss: 0.010768554580863565\n",
      "[6, 45]: loss: 0.05848155314743053\n",
      "[6, 50]: loss: 0.03597843294846825\n",
      "[6, 55]: loss: 0.01785156928235665\n",
      "[6, 60]: loss: 0.038151448796270415\n",
      "[6, 65]: loss: 0.02663409343222156\n",
      "[6, 70]: loss: 0.011791466677095741\n",
      "[6, 75]: loss: 0.05222875653998926\n",
      "[6, 80]: loss: 0.012133657757658511\n",
      "[6, 85]: loss: 0.005684201285475865\n",
      "[6, 90]: loss: 0.01120710365648847\n",
      "[6, 95]: loss: 0.021210162885836326\n",
      "[6, 100]: loss: 0.003681474830955267\n",
      "[6, 105]: loss: 0.0038711867382517084\n",
      "[6, 110]: loss: 0.014993528864579275\n",
      "[6, 115]: loss: 0.012981034567928873\n",
      "[7, 5]: loss: 0.039966115553397685\n",
      "[7, 10]: loss: 0.0028664890633081086\n",
      "[7, 15]: loss: 0.008089561808446888\n",
      "[7, 20]: loss: 0.002335021024919115\n",
      "[7, 25]: loss: 0.010563416210061405\n",
      "[7, 30]: loss: 0.015651139605324715\n",
      "[7, 35]: loss: 0.002304699650267139\n",
      "[7, 40]: loss: 0.015244976661051624\n",
      "[7, 45]: loss: 0.007924194389488548\n",
      "[7, 50]: loss: 0.0019288980402052402\n",
      "[7, 55]: loss: 0.005777347425464541\n",
      "[7, 60]: loss: 0.018200236969278194\n",
      "[7, 65]: loss: 0.0086219021177385\n",
      "[7, 70]: loss: 0.0061482355813495815\n",
      "[7, 75]: loss: 0.005123009177623317\n",
      "[7, 80]: loss: 0.019989106280263513\n",
      "[7, 85]: loss: 0.02706350982771255\n",
      "[7, 90]: loss: 0.00614561204565689\n",
      "[7, 95]: loss: 0.00529995042597875\n",
      "[7, 100]: loss: 0.006148253713035956\n",
      "[7, 105]: loss: 0.005077457753941417\n",
      "[7, 110]: loss: 0.005375993074267171\n",
      "[7, 115]: loss: 0.0071057837340049446\n",
      "[8, 5]: loss: 0.03552806495281402\n",
      "[8, 10]: loss: 0.01571072889782954\n",
      "[8, 15]: loss: 0.016355645755538717\n",
      "[8, 20]: loss: 0.00847729854285717\n",
      "[8, 25]: loss: 0.005182113221962936\n",
      "[8, 30]: loss: 0.004891737509751692\n",
      "[8, 35]: loss: 0.005358804351999424\n",
      "[8, 40]: loss: 0.01079966191900894\n",
      "[8, 45]: loss: 0.0035099787564831786\n",
      "[8, 50]: loss: 0.010692226031096652\n",
      "[8, 55]: loss: 0.0030546941998181865\n",
      "[8, 60]: loss: 0.007020959848887287\n",
      "[8, 65]: loss: 0.015732308522274252\n",
      "[8, 70]: loss: 0.014825152058620006\n",
      "[8, 75]: loss: 0.003502645340631716\n",
      "[8, 80]: loss: 0.004081170191057026\n",
      "[8, 85]: loss: 0.00772511720424518\n",
      "[8, 90]: loss: 0.0048086397218867205\n",
      "[8, 95]: loss: 0.016033785039326176\n",
      "[8, 100]: loss: 0.005230906885117292\n",
      "[8, 105]: loss: 0.00672591058537364\n",
      "[8, 110]: loss: 0.0152603343303781\n",
      "[8, 115]: loss: 0.0038744992343708873\n",
      "[9, 5]: loss: 0.0014274455952545395\n",
      "[9, 10]: loss: 0.013688436636584811\n",
      "[9, 15]: loss: 0.000877075748576317\n",
      "[9, 20]: loss: 0.0017428055580239743\n",
      "[9, 25]: loss: 0.012191830981464591\n",
      "[9, 30]: loss: 0.0068466895463643596\n",
      "[9, 35]: loss: 0.0022375473054125905\n",
      "[9, 40]: loss: 0.018940757465315983\n",
      "[9, 45]: loss: 0.0010197280425927602\n",
      "[9, 50]: loss: 0.002126305247657001\n",
      "[9, 55]: loss: 0.006761333890608512\n",
      "[9, 60]: loss: 0.022934081207495183\n",
      "[9, 65]: loss: 0.012791053100954741\n",
      "[9, 70]: loss: 0.009360961463244166\n",
      "[9, 75]: loss: 0.002903635206166655\n",
      "[9, 80]: loss: 0.008710266032721847\n",
      "[9, 85]: loss: 0.006027475195878651\n",
      "[9, 90]: loss: 0.006051589647540823\n",
      "[9, 95]: loss: 0.002689088250917848\n",
      "[9, 100]: loss: 0.0025122916558757424\n",
      "[9, 105]: loss: 0.005326074257027358\n",
      "[9, 110]: loss: 0.0022272532078204677\n",
      "[9, 115]: loss: 0.0025674439675640315\n",
      "[10, 5]: loss: 0.0068297324469313025\n",
      "[10, 10]: loss: 0.003204122964234557\n",
      "[10, 15]: loss: 0.042440559933311306\n",
      "[10, 20]: loss: 0.0021258649503579363\n",
      "[10, 25]: loss: 0.0029871360020479187\n",
      "[10, 30]: loss: 0.0032228306081378832\n",
      "[10, 35]: loss: 0.009799582039704546\n",
      "[10, 40]: loss: 0.018659452471183613\n",
      "[10, 45]: loss: 0.0033644864161033183\n",
      "[10, 50]: loss: 0.03374574442568701\n",
      "[10, 55]: loss: 0.006861230387585238\n",
      "[10, 60]: loss: 0.02008972072508186\n",
      "[10, 65]: loss: 0.00158629357611062\n",
      "[10, 70]: loss: 0.02361606076010503\n",
      "[10, 75]: loss: 0.004138919903198257\n",
      "[10, 80]: loss: 0.007161588000599295\n",
      "[10, 85]: loss: 0.00870281238167081\n",
      "[10, 90]: loss: 0.005567418236751109\n",
      "[10, 95]: loss: 0.009468637843383476\n",
      "[10, 100]: loss: 0.00224795141548384\n",
      "[10, 105]: loss: 0.006094396987464279\n",
      "[10, 110]: loss: 0.005912507200264372\n",
      "[10, 115]: loss: 0.009039457436301745\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 14: 0.5105\n",
      "Random Sampling Iteration 15\n",
      "[1, 5]: loss: 0.18567296536639333\n",
      "[1, 10]: loss: 0.06744243355933577\n",
      "[1, 15]: loss: 0.22100047761341557\n",
      "[1, 20]: loss: 0.3155240686610341\n",
      "[1, 25]: loss: 0.17252636409830302\n",
      "[1, 30]: loss: 0.14141729613766074\n",
      "[1, 35]: loss: 0.4291195394471288\n",
      "[1, 40]: loss: 0.29076665127649903\n",
      "[1, 45]: loss: 0.2769120155717246\n",
      "[1, 50]: loss: 0.13083224464207888\n",
      "[1, 55]: loss: 0.025115504337009043\n",
      "[1, 60]: loss: 0.2224478404968977\n",
      "[1, 65]: loss: 0.3424194095423445\n",
      "[1, 70]: loss: 0.2727504407521337\n",
      "[1, 75]: loss: 0.5873144380748272\n",
      "[1, 80]: loss: 0.39603243954479694\n",
      "[1, 85]: loss: 0.2576868472388014\n",
      "[1, 90]: loss: 0.20271486917044967\n",
      "[1, 95]: loss: 0.14590876223519444\n",
      "[1, 100]: loss: 0.7114839479327202\n",
      "[1, 105]: loss: 0.0271041706437245\n",
      "[1, 110]: loss: 0.3724406885448843\n",
      "[1, 115]: loss: 0.12172093259869143\n",
      "[1, 120]: loss: 0.20540516264736652\n",
      "[1, 125]: loss: 0.016171469789696857\n",
      "[2, 5]: loss: 0.03588706045411527\n",
      "[2, 10]: loss: 0.060844113351777196\n",
      "[2, 15]: loss: 0.0669224328303244\n",
      "[2, 20]: loss: 0.02367589477216825\n",
      "[2, 25]: loss: 0.19713336462154984\n",
      "[2, 30]: loss: 0.10004940693033859\n",
      "[2, 35]: loss: 0.03566804155707359\n",
      "[2, 40]: loss: 0.030118279275484383\n",
      "[2, 45]: loss: 0.02148518804460764\n",
      "[2, 50]: loss: 0.17138093913672492\n",
      "[2, 55]: loss: 0.07670328282983974\n",
      "[2, 60]: loss: 0.1260886003728956\n",
      "[2, 65]: loss: 0.10519953328184783\n",
      "[2, 70]: loss: 0.1816256670281291\n",
      "[2, 75]: loss: 0.03338300541508943\n",
      "[2, 80]: loss: 0.13596949959173799\n",
      "[2, 85]: loss: 0.01342414622195065\n",
      "[2, 90]: loss: 0.049930268607567996\n",
      "[2, 95]: loss: 0.1304408795258496\n",
      "[2, 100]: loss: 0.06982136715669185\n",
      "[2, 105]: loss: 0.013360159122385085\n",
      "[2, 110]: loss: 0.05999320384580642\n",
      "[2, 115]: loss: 0.04373471438884735\n",
      "[2, 120]: loss: 0.02451582089997828\n",
      "[2, 125]: loss: 0.019366758642718196\n",
      "[3, 5]: loss: 0.02320496301399544\n",
      "[3, 10]: loss: 0.01740510448871646\n",
      "[3, 15]: loss: 0.011797056999057531\n",
      "[3, 20]: loss: 0.02759223442990333\n",
      "[3, 25]: loss: 0.010121404950041324\n",
      "[3, 30]: loss: 0.06636089425592218\n",
      "[3, 35]: loss: 0.01297870807320578\n",
      "[3, 40]: loss: 0.006471099419286475\n",
      "[3, 45]: loss: 0.03403959478600882\n",
      "[3, 50]: loss: 0.05485951041919179\n",
      "[3, 55]: loss: 0.014678543317131698\n",
      "[3, 60]: loss: 0.015579332015477121\n",
      "[3, 65]: loss: 0.09368133268435486\n",
      "[3, 70]: loss: 0.08445598953403533\n",
      "[3, 75]: loss: 0.07993275253102183\n",
      "[3, 80]: loss: 0.030751261627301574\n",
      "[3, 85]: loss: 0.04092575146933086\n",
      "[3, 90]: loss: 0.03047860460355878\n",
      "[3, 95]: loss: 0.05155384651152417\n",
      "[3, 100]: loss: 0.05596306663937867\n",
      "[3, 105]: loss: 0.03890212398255244\n",
      "[3, 110]: loss: 0.02122547969338484\n",
      "[3, 115]: loss: 0.015738727161078714\n",
      "[3, 120]: loss: 0.03976789675652981\n",
      "[3, 125]: loss: 0.027761868142988533\n",
      "[4, 5]: loss: 0.03347503651457373\n",
      "[4, 10]: loss: 0.01890369539614767\n",
      "[4, 15]: loss: 0.01529995264718309\n",
      "[4, 20]: loss: 0.024254115101939533\n",
      "[4, 25]: loss: 0.005009824963053688\n",
      "[4, 30]: loss: 0.013291305454913527\n",
      "[4, 35]: loss: 0.0026554558862699196\n",
      "[4, 40]: loss: 0.019684662111103535\n",
      "[4, 45]: loss: 0.021053670468972996\n",
      "[4, 50]: loss: 0.011999034963082522\n",
      "[4, 55]: loss: 0.005347029320546426\n",
      "[4, 60]: loss: 0.016371298959711567\n",
      "[4, 65]: loss: 0.009029448847286403\n",
      "[4, 70]: loss: 0.04893411252851365\n",
      "[4, 75]: loss: 0.011937215647776611\n",
      "[4, 80]: loss: 0.02427456114673987\n",
      "[4, 85]: loss: 0.004322908527683467\n",
      "[4, 90]: loss: 0.023695793061051518\n",
      "[4, 95]: loss: 0.005511543349712156\n",
      "[4, 100]: loss: 0.041799141545197926\n",
      "[4, 105]: loss: 0.0037370440186350606\n",
      "[4, 110]: loss: 0.07701158520649187\n",
      "[4, 115]: loss: 0.02989159553544596\n",
      "[4, 120]: loss: 0.048674273915821686\n",
      "[4, 125]: loss: 0.008595802763011307\n",
      "[5, 5]: loss: 0.02839269011747092\n",
      "[5, 10]: loss: 0.037374365332652815\n",
      "[5, 15]: loss: 0.020395758299855515\n",
      "[5, 20]: loss: 0.00394994905218482\n",
      "[5, 25]: loss: 0.005290557368425652\n",
      "[5, 30]: loss: 0.043311065062880516\n",
      "[5, 35]: loss: 0.007064705394441262\n",
      "[5, 40]: loss: 0.02320394798880443\n",
      "[5, 45]: loss: 0.027707815868780017\n",
      "[5, 50]: loss: 0.030201084387954324\n",
      "[5, 55]: loss: 0.07403068733401597\n",
      "[5, 60]: loss: 0.0205913579557091\n",
      "[5, 65]: loss: 0.033254852634854615\n",
      "[5, 70]: loss: 0.007698175875702873\n",
      "[5, 75]: loss: 0.01545248756883666\n",
      "[5, 80]: loss: 0.016377893422031775\n",
      "[5, 85]: loss: 0.009298663528170437\n",
      "[5, 90]: loss: 0.07669717560929712\n",
      "[5, 95]: loss: 0.01802654511993751\n",
      "[5, 100]: loss: 0.014501273952191696\n",
      "[5, 105]: loss: 0.01900119864149019\n",
      "[5, 110]: loss: 0.03888741762784775\n",
      "[5, 115]: loss: 0.006513541695312597\n",
      "[5, 120]: loss: 0.009013193470309488\n",
      "[5, 125]: loss: 0.00597269632271491\n",
      "[6, 5]: loss: 0.034419822943164036\n",
      "[6, 10]: loss: 0.0054765548557043076\n",
      "[6, 15]: loss: 0.01048623796668835\n",
      "[6, 20]: loss: 0.0691095952388423\n",
      "[6, 25]: loss: 0.002869198186090216\n",
      "[6, 30]: loss: 0.028748717973940074\n",
      "[6, 35]: loss: 0.007268957706401125\n",
      "[6, 40]: loss: 0.003070537088206038\n",
      "[6, 45]: loss: 0.016119034960865974\n",
      "[6, 50]: loss: 0.0024900654971133918\n",
      "[6, 55]: loss: 0.011686182231642306\n",
      "[6, 60]: loss: 0.00537599713425152\n",
      "[6, 65]: loss: 0.008279620378743857\n",
      "[6, 70]: loss: 0.029647622141055763\n",
      "[6, 75]: loss: 0.004994315764633939\n",
      "[6, 80]: loss: 0.010360643733292818\n",
      "[6, 85]: loss: 0.013893598079448566\n",
      "[6, 90]: loss: 0.0054802919330541044\n",
      "[6, 95]: loss: 0.00957940514490474\n",
      "[6, 100]: loss: 0.004916045123536605\n",
      "[6, 105]: loss: 0.005396320440922864\n",
      "[6, 110]: loss: 0.007375432396656834\n",
      "[6, 115]: loss: 0.003777981866733171\n",
      "[6, 120]: loss: 0.010681097861379385\n",
      "[6, 125]: loss: 0.01992823457112536\n",
      "[7, 5]: loss: 0.005188563984120265\n",
      "[7, 10]: loss: 0.006968933565076441\n",
      "[7, 15]: loss: 0.005384407384553924\n",
      "[7, 20]: loss: 0.007548420624516439\n",
      "[7, 25]: loss: 0.0033565555495442823\n",
      "[7, 30]: loss: 0.00247461989056319\n",
      "[7, 35]: loss: 0.010892746242461726\n",
      "[7, 40]: loss: 0.004629642906365916\n",
      "[7, 45]: loss: 0.005562961479881778\n",
      "[7, 50]: loss: 0.009559554760926403\n",
      "[7, 55]: loss: 0.0316258464299608\n",
      "[7, 60]: loss: 0.002497696754289791\n",
      "[7, 65]: loss: 0.0036778493085876107\n",
      "[7, 70]: loss: 0.013079909753287211\n",
      "[7, 75]: loss: 0.010856025794055313\n",
      "[7, 80]: loss: 0.007511478790547699\n",
      "[7, 85]: loss: 0.012180431935121305\n",
      "[7, 90]: loss: 0.005005701656045858\n",
      "[7, 95]: loss: 0.0024686573815415613\n",
      "[7, 100]: loss: 0.011360384814906865\n",
      "[7, 105]: loss: 0.02649882304831408\n",
      "[7, 110]: loss: 0.01609622483374551\n",
      "[7, 115]: loss: 0.004854929444263689\n",
      "[7, 120]: loss: 0.0027524707838892937\n",
      "[7, 125]: loss: 0.029944556212285534\n",
      "[8, 5]: loss: 0.00845574941195082\n",
      "[8, 10]: loss: 0.039537853153888136\n",
      "[8, 15]: loss: 0.006482432698248886\n",
      "[8, 20]: loss: 0.0070016580139053985\n",
      "[8, 25]: loss: 0.002893090946599841\n",
      "[8, 30]: loss: 0.015129321109270677\n",
      "[8, 35]: loss: 0.005133631217177026\n",
      "[8, 40]: loss: 0.008545376767870039\n",
      "[8, 45]: loss: 0.010669826879166067\n",
      "[8, 50]: loss: 0.0013339095312403515\n",
      "[8, 55]: loss: 0.006396302691427991\n",
      "[8, 60]: loss: 0.017637808487052098\n",
      "[8, 65]: loss: 0.029722283885348588\n",
      "[8, 70]: loss: 0.00516159349353984\n",
      "[8, 75]: loss: 0.0025129255664069206\n",
      "[8, 80]: loss: 0.0037158245613682084\n",
      "[8, 85]: loss: 0.0021763116310467012\n",
      "[8, 90]: loss: 0.009789247123990208\n",
      "[8, 95]: loss: 0.006689773974358104\n",
      "[8, 100]: loss: 0.0027144131890963763\n",
      "[8, 105]: loss: 0.00311634513491299\n",
      "[8, 110]: loss: 0.011777189021813683\n",
      "[8, 115]: loss: 0.0021929628928774036\n",
      "[8, 120]: loss: 0.012091788259567693\n",
      "[8, 125]: loss: 0.003963419218052877\n",
      "[9, 5]: loss: 0.005038649287598673\n",
      "[9, 10]: loss: 0.04022176575381309\n",
      "[9, 15]: loss: 0.0030140809976728633\n",
      "[9, 20]: loss: 0.0021128213265910745\n",
      "[9, 25]: loss: 0.0020705318893305957\n",
      "[9, 30]: loss: 0.0024782305699773133\n",
      "[9, 35]: loss: 0.002976318064611405\n",
      "[9, 40]: loss: 0.004506578872678801\n",
      "[9, 45]: loss: 0.00533897103741765\n",
      "[9, 50]: loss: 0.002577587030827999\n",
      "[9, 55]: loss: 0.005580371740506962\n",
      "[9, 60]: loss: 0.003997532592620701\n",
      "[9, 65]: loss: 0.003177106089424342\n",
      "[9, 70]: loss: 0.0053585424611810595\n",
      "[9, 75]: loss: 0.004313319848733954\n",
      "[9, 80]: loss: 0.0028756858373526484\n",
      "[9, 85]: loss: 0.006612482509808615\n",
      "[9, 90]: loss: 0.001317253216257086\n",
      "[9, 95]: loss: 0.0015230181088554673\n",
      "[9, 100]: loss: 0.007143657589040231\n",
      "[9, 105]: loss: 0.0025850960664683953\n",
      "[9, 110]: loss: 0.0038894211902515963\n",
      "[9, 115]: loss: 0.0020553269459924195\n",
      "[9, 120]: loss: 0.003882375138346106\n",
      "[9, 125]: loss: 0.0009914944748743437\n",
      "[10, 5]: loss: 0.0022983369563007727\n",
      "[10, 10]: loss: 0.001310234369157115\n",
      "[10, 15]: loss: 0.010692449159250828\n",
      "[10, 20]: loss: 0.003975550847826526\n",
      "[10, 25]: loss: 0.0058410627316334285\n",
      "[10, 30]: loss: 0.003230641821573954\n",
      "[10, 35]: loss: 0.001499354184488766\n",
      "[10, 40]: loss: 0.017501236623502336\n",
      "[10, 45]: loss: 0.005937584617640823\n",
      "[10, 50]: loss: 0.0017583607041160576\n",
      "[10, 55]: loss: 0.00965323306445498\n",
      "[10, 60]: loss: 0.01798597796005197\n",
      "[10, 65]: loss: 0.009683774007498869\n",
      "[10, 70]: loss: 0.0059542485905694775\n",
      "[10, 75]: loss: 0.013044772033026675\n",
      "[10, 80]: loss: 0.0023557241074740887\n",
      "[10, 85]: loss: 0.005649477247061441\n",
      "[10, 90]: loss: 0.0029873784733354114\n",
      "[10, 95]: loss: 0.0011290801921859384\n",
      "[10, 100]: loss: 0.005122919457789976\n",
      "[10, 105]: loss: 0.004181223179330118\n",
      "[10, 110]: loss: 0.015706210368080065\n",
      "[10, 115]: loss: 0.006019441381795332\n",
      "[10, 120]: loss: 0.0035096649153274484\n",
      "[10, 125]: loss: 0.056206077308161184\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 15: 0.519\n",
      "Random Sampling Iteration 16\n",
      "[1, 5]: loss: 0.3265646934596589\n",
      "[1, 10]: loss: 0.07827955042012036\n",
      "[1, 15]: loss: 0.1675114500685595\n",
      "[1, 20]: loss: 0.2273918911232613\n",
      "[1, 25]: loss: 0.18817346217110753\n",
      "[1, 30]: loss: 0.36892710279789753\n",
      "[1, 35]: loss: 0.21690433472394943\n",
      "[1, 40]: loss: 0.2093977325130254\n",
      "[1, 45]: loss: 0.07107598532456905\n",
      "[1, 50]: loss: 0.14317558333277702\n",
      "[1, 55]: loss: 0.20859135943464935\n",
      "[1, 60]: loss: 0.3041664455085993\n",
      "[1, 65]: loss: 0.04310788004659116\n",
      "[1, 70]: loss: 0.2516362437745556\n",
      "[1, 75]: loss: 0.2584731983952224\n",
      "[1, 80]: loss: 0.3957143034785986\n",
      "[1, 85]: loss: 0.3979315939359367\n",
      "[1, 90]: loss: 0.18750587836257182\n",
      "[1, 95]: loss: 0.06950300734024495\n",
      "[1, 100]: loss: 0.3046004220377654\n",
      "[1, 105]: loss: 0.25874622724950314\n",
      "[1, 110]: loss: 0.21432019257918\n",
      "[1, 115]: loss: 0.2799826371483505\n",
      "[1, 120]: loss: 0.14202791906427592\n",
      "[1, 125]: loss: 0.5576688898727298\n",
      "[1, 130]: loss: 0.11589377280324697\n",
      "[2, 5]: loss: 0.11874357424676418\n",
      "[2, 10]: loss: 0.07767696678638458\n",
      "[2, 15]: loss: 0.021986161795211956\n",
      "[2, 20]: loss: 0.09858552529476583\n",
      "[2, 25]: loss: 0.2441649518441409\n",
      "[2, 30]: loss: 0.029906538489740342\n",
      "[2, 35]: loss: 0.02078508085105568\n",
      "[2, 40]: loss: 0.07088044285774231\n",
      "[2, 45]: loss: 0.014297686648205854\n",
      "[2, 50]: loss: 0.01571785708074458\n",
      "[2, 55]: loss: 0.09352864720858634\n",
      "[2, 60]: loss: 0.15623645763844252\n",
      "[2, 65]: loss: 0.07285882707219571\n",
      "[2, 70]: loss: 0.11894128751009703\n",
      "[2, 75]: loss: 0.10389312433835585\n",
      "[2, 80]: loss: 0.019850041484460235\n",
      "[2, 85]: loss: 0.018907438148744404\n",
      "[2, 90]: loss: 0.019522471411619335\n",
      "[2, 95]: loss: 0.09422816149890423\n",
      "[2, 100]: loss: 0.09798841259907931\n",
      "[2, 105]: loss: 0.07123732648324221\n",
      "[2, 110]: loss: 0.06858588980685454\n",
      "[2, 115]: loss: 0.06302492623217404\n",
      "[2, 120]: loss: 0.016775392432464287\n",
      "[2, 125]: loss: 0.013746128650382161\n",
      "[2, 130]: loss: 0.02363303687889129\n",
      "[3, 5]: loss: 0.007411437953123823\n",
      "[3, 10]: loss: 0.11468901840271428\n",
      "[3, 15]: loss: 0.0072171119099948555\n",
      "[3, 20]: loss: 0.06606284159352072\n",
      "[3, 25]: loss: 0.01470602973131463\n",
      "[3, 30]: loss: 0.023704182007350028\n",
      "[3, 35]: loss: 0.06668041775992606\n",
      "[3, 40]: loss: 0.016546955826925114\n",
      "[3, 45]: loss: 0.041238284669816494\n",
      "[3, 50]: loss: 0.032736928173108026\n",
      "[3, 55]: loss: 0.010552953172009438\n",
      "[3, 60]: loss: 0.018004634883254766\n",
      "[3, 65]: loss: 0.02291436307132244\n",
      "[3, 70]: loss: 0.026774096360895783\n",
      "[3, 75]: loss: 0.008833133761072531\n",
      "[3, 80]: loss: 0.059432704700157046\n",
      "[3, 85]: loss: 0.04801762476563454\n",
      "[3, 90]: loss: 0.01994237763574347\n",
      "[3, 95]: loss: 0.045599246863275766\n",
      "[3, 100]: loss: 0.005441610876005143\n",
      "[3, 105]: loss: 0.01950256183044985\n",
      "[3, 110]: loss: 0.07912175610545091\n",
      "[3, 115]: loss: 0.01628154597710818\n",
      "[3, 120]: loss: 0.013015329837799072\n",
      "[3, 125]: loss: 0.02399983291979879\n",
      "[3, 130]: loss: 0.04194749190355651\n",
      "[4, 5]: loss: 0.012246052894624881\n",
      "[4, 10]: loss: 0.030669757863506675\n",
      "[4, 15]: loss: 0.0569530013599433\n",
      "[4, 20]: loss: 0.030271792318671942\n",
      "[4, 25]: loss: 0.020208895002724603\n",
      "[4, 30]: loss: 0.008017752406885847\n",
      "[4, 35]: loss: 0.014961322238377761\n",
      "[4, 40]: loss: 0.005267567932605743\n",
      "[4, 45]: loss: 0.004079010686837137\n",
      "[4, 50]: loss: 0.014395784877706319\n",
      "[4, 55]: loss: 0.02489565331779886\n",
      "[4, 60]: loss: 0.03219998961139936\n",
      "[4, 65]: loss: 0.005757218561484478\n",
      "[4, 70]: loss: 0.01443113059940515\n",
      "[4, 75]: loss: 0.00880672299535945\n",
      "[4, 80]: loss: 0.010609946737531573\n",
      "[4, 85]: loss: 0.0776375352870673\n",
      "[4, 90]: loss: 0.015314584641600959\n",
      "[4, 95]: loss: 0.010958802435197867\n",
      "[4, 100]: loss: 0.013245574227767065\n",
      "[4, 105]: loss: 0.016884346594451927\n",
      "[4, 110]: loss: 0.01868723941151984\n",
      "[4, 115]: loss: 0.020742627209983766\n",
      "[4, 120]: loss: 0.007154561986681074\n",
      "[4, 125]: loss: 0.010818644355822471\n",
      "[4, 130]: loss: 0.00952716865867842\n",
      "[5, 5]: loss: 0.010040652450697962\n",
      "[5, 10]: loss: 0.009130396785621997\n",
      "[5, 15]: loss: 0.009784886467969045\n",
      "[5, 20]: loss: 0.003860837983665988\n",
      "[5, 25]: loss: 0.006399207366484916\n",
      "[5, 30]: loss: 0.005072158272014349\n",
      "[5, 35]: loss: 0.03333463457238395\n",
      "[5, 40]: loss: 0.00965843113954179\n",
      "[5, 45]: loss: 0.014424037290154956\n",
      "[5, 50]: loss: 0.005387714118114673\n",
      "[5, 55]: loss: 0.0362687218002975\n",
      "[5, 60]: loss: 0.01880713494028896\n",
      "[5, 65]: loss: 0.009558989011566155\n",
      "[5, 70]: loss: 0.004828393779462203\n",
      "[5, 75]: loss: 0.007012957503320649\n",
      "[5, 80]: loss: 0.00926709856139496\n",
      "[5, 85]: loss: 0.012907042808365077\n",
      "[5, 90]: loss: 0.0017304939319728874\n",
      "[5, 95]: loss: 0.00480659632012248\n",
      "[5, 100]: loss: 0.0073452342840027995\n",
      "[5, 105]: loss: 0.01802204441628419\n",
      "[5, 110]: loss: 0.0037473283391591394\n",
      "[5, 115]: loss: 0.022953674877498997\n",
      "[5, 120]: loss: 0.0062071627471596\n",
      "[5, 125]: loss: 0.004631141142453998\n",
      "[5, 130]: loss: 0.005363678923458792\n",
      "[6, 5]: loss: 0.009704453346785158\n",
      "[6, 10]: loss: 0.0039024175639497116\n",
      "[6, 15]: loss: 0.008820342016406357\n",
      "[6, 20]: loss: 0.008834549389575841\n",
      "[6, 25]: loss: 0.002910272523877211\n",
      "[6, 30]: loss: 0.0036751177394762635\n",
      "[6, 35]: loss: 0.0012785619182977825\n",
      "[6, 40]: loss: 0.0031079774962563533\n",
      "[6, 45]: loss: 0.00854850032919785\n",
      "[6, 50]: loss: 0.002520485875720624\n",
      "[6, 55]: loss: 0.02284957966185175\n",
      "[6, 60]: loss: 0.013173538940463914\n",
      "[6, 65]: loss: 0.027334560218150727\n",
      "[6, 70]: loss: 0.01986713777296245\n",
      "[6, 75]: loss: 0.009280074067646638\n",
      "[6, 80]: loss: 0.007603224374179263\n",
      "[6, 85]: loss: 0.0026424785974086262\n",
      "[6, 90]: loss: 0.0008178404204954859\n",
      "[6, 95]: loss: 0.0036921316204825416\n",
      "[6, 100]: loss: 0.00725438937661238\n",
      "[6, 105]: loss: 0.0012295118067413568\n",
      "[6, 110]: loss: 0.025308313342975453\n",
      "[6, 115]: loss: 0.004838860149902757\n",
      "[6, 120]: loss: 0.005980230373097584\n",
      "[6, 125]: loss: 0.00353660342443618\n",
      "[6, 130]: loss: 0.002221230257418938\n",
      "[7, 5]: loss: 0.0012981994877918623\n",
      "[7, 10]: loss: 0.010288356454111636\n",
      "[7, 15]: loss: 0.014080207300139591\n",
      "[7, 20]: loss: 0.0028591569243872073\n",
      "[7, 25]: loss: 0.008695744691067375\n",
      "[7, 30]: loss: 0.0032891182345338166\n",
      "[7, 35]: loss: 0.002192115382058546\n",
      "[7, 40]: loss: 0.0034508108801674098\n",
      "[7, 45]: loss: 0.004542154667433351\n",
      "[7, 50]: loss: 0.0037636348279193044\n",
      "[7, 55]: loss: 0.0034253919002367184\n",
      "[7, 60]: loss: 0.0021876626342418604\n",
      "[7, 65]: loss: 0.02537339185801102\n",
      "[7, 70]: loss: 0.0031229095984599553\n",
      "[7, 75]: loss: 0.0032571290503256023\n",
      "[7, 80]: loss: 0.003728946059709415\n",
      "[7, 85]: loss: 0.0014826362203166354\n",
      "[7, 90]: loss: 0.007936550580780022\n",
      "[7, 95]: loss: 0.005640306437271647\n",
      "[7, 100]: loss: 0.003195564309862675\n",
      "[7, 105]: loss: 0.00363494768680539\n",
      "[7, 110]: loss: 0.004171233478700742\n",
      "[7, 115]: loss: 0.0018325226574233966\n",
      "[7, 120]: loss: 0.0020537017480819486\n",
      "[7, 125]: loss: 0.02389474428491667\n",
      "[7, 130]: loss: 0.0021923167951172218\n",
      "[8, 5]: loss: 0.009064596495591104\n",
      "[8, 10]: loss: 0.00825716182589531\n",
      "[8, 15]: loss: 0.010612393547489773\n",
      "[8, 20]: loss: 0.007163085225329269\n",
      "[8, 25]: loss: 0.0011503722380439285\n",
      "[8, 30]: loss: 0.00358451767169754\n",
      "[8, 35]: loss: 0.0013979512586956844\n",
      "[8, 40]: loss: 0.0016497933756909333\n",
      "[8, 45]: loss: 0.0030898733530193567\n",
      "[8, 50]: loss: 0.007844348685466684\n",
      "[8, 55]: loss: 0.0033749772192095406\n",
      "[8, 60]: loss: 0.0049168942096002866\n",
      "[8, 65]: loss: 0.0022748168266844004\n",
      "[8, 70]: loss: 0.005438197709736414\n",
      "[8, 75]: loss: 0.001509573106886819\n",
      "[8, 80]: loss: 0.005263916566036642\n",
      "[8, 85]: loss: 0.011488004671264207\n",
      "[8, 90]: loss: 0.002644373002112843\n",
      "[8, 95]: loss: 0.008672274467244279\n",
      "[8, 100]: loss: 0.013054354509222321\n",
      "[8, 105]: loss: 0.001814286209992133\n",
      "[8, 110]: loss: 0.0077196698694024235\n",
      "[8, 115]: loss: 0.003746656177099794\n",
      "[8, 120]: loss: 0.0033821517499745823\n",
      "[8, 125]: loss: 0.001005140989946085\n",
      "[8, 130]: loss: 0.004032004449982196\n",
      "[9, 5]: loss: 0.0013667532621184364\n",
      "[9, 10]: loss: 0.002467244972649496\n",
      "[9, 15]: loss: 0.0010415524084237404\n",
      "[9, 20]: loss: 0.0034272754201083444\n",
      "[9, 25]: loss: 0.0025643511908128858\n",
      "[9, 30]: loss: 0.0098859687714139\n",
      "[9, 35]: loss: 0.007915245838375995\n",
      "[9, 40]: loss: 0.003391022728465032\n",
      "[9, 45]: loss: 0.001769823968061246\n",
      "[9, 50]: loss: 0.00356168993312167\n",
      "[9, 55]: loss: 0.010144976840820163\n",
      "[9, 60]: loss: 0.0036007369199069217\n",
      "[9, 65]: loss: 0.0021576527469733264\n",
      "[9, 70]: loss: 0.00615129669313319\n",
      "[9, 75]: loss: 0.006438764903577976\n",
      "[9, 80]: loss: 0.006838689900177997\n",
      "[9, 85]: loss: 0.02196824599377578\n",
      "[9, 90]: loss: 0.002762899355730042\n",
      "[9, 95]: loss: 0.0063782395591260865\n",
      "[9, 100]: loss: 0.003189593175193295\n",
      "[9, 105]: loss: 0.0021802130213472992\n",
      "[9, 110]: loss: 0.003389313511434011\n",
      "[9, 115]: loss: 0.0011401894662412815\n",
      "[9, 120]: loss: 0.0008412088718614541\n",
      "[9, 125]: loss: 0.003667474229587242\n",
      "[9, 130]: loss: 0.008799492912658025\n",
      "[10, 5]: loss: 0.0017370184887113282\n",
      "[10, 10]: loss: 0.003633120832091663\n",
      "[10, 15]: loss: 0.00238113909290405\n",
      "[10, 20]: loss: 0.002424192745820619\n",
      "[10, 25]: loss: 0.001284444035263732\n",
      "[10, 30]: loss: 0.0016899338079383597\n",
      "[10, 35]: loss: 0.0011874146293848753\n",
      "[10, 40]: loss: 0.0006294902432273375\n",
      "[10, 45]: loss: 0.00547858138452284\n",
      "[10, 50]: loss: 0.005365799232095014\n",
      "[10, 55]: loss: 0.0031924566719681025\n",
      "[10, 60]: loss: 0.0012617115462489892\n",
      "[10, 65]: loss: 0.004385560838272795\n",
      "[10, 70]: loss: 0.0017615751276025549\n",
      "[10, 75]: loss: 0.004355045050033368\n",
      "[10, 80]: loss: 0.0015757308065076359\n",
      "[10, 85]: loss: 0.005016107737901621\n",
      "[10, 90]: loss: 0.006256335589569062\n",
      "[10, 95]: loss: 0.0006301612665993161\n",
      "[10, 100]: loss: 0.00501715258724289\n",
      "[10, 105]: loss: 0.010158403067180188\n",
      "[10, 110]: loss: 0.001689563976469799\n",
      "[10, 115]: loss: 0.0016184350461116992\n",
      "[10, 120]: loss: 0.010174955532420427\n",
      "[10, 125]: loss: 0.0004441611636138987\n",
      "[10, 130]: loss: 0.0037314957080525346\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 16: 0.5325\n",
      "Random Sampling Iteration 17\n",
      "[1, 5]: loss: 0.24419322681205813\n",
      "[1, 10]: loss: 0.18054906593170017\n",
      "[1, 15]: loss: 0.13498115731636062\n",
      "[1, 20]: loss: 0.43402336427243426\n",
      "[1, 25]: loss: 0.30532641243189573\n",
      "[1, 30]: loss: 0.0676155851688236\n",
      "[1, 35]: loss: 0.31261754035949707\n",
      "[1, 40]: loss: 0.1934904265217483\n",
      "[1, 45]: loss: 0.12297097942791879\n",
      "[1, 50]: loss: 0.15329045191174373\n",
      "[1, 55]: loss: 0.22620781239675125\n",
      "[1, 60]: loss: 0.35057411243906245\n",
      "[1, 65]: loss: 0.6147072007879615\n",
      "[1, 70]: loss: 0.11318189557641745\n",
      "[1, 75]: loss: 0.10240792005788535\n",
      "[1, 80]: loss: 0.31035980954766273\n",
      "[1, 85]: loss: 0.21053279924672097\n",
      "[1, 90]: loss: 0.19832144037354738\n",
      "[1, 95]: loss: 0.17702593258582056\n",
      "[1, 100]: loss: 0.15212391270324588\n",
      "[1, 105]: loss: 0.31104841781780124\n",
      "[1, 110]: loss: 0.26769478479400277\n",
      "[1, 115]: loss: 0.07109267089981586\n",
      "[1, 120]: loss: 0.3739863352384418\n",
      "[1, 125]: loss: 0.14426085352897644\n",
      "[1, 130]: loss: 0.18712902802508324\n",
      "[1, 135]: loss: 0.13137172622373328\n",
      "[2, 5]: loss: 0.10164385195821524\n",
      "[2, 10]: loss: 0.04730630049016327\n",
      "[2, 15]: loss: 0.02613202843349427\n",
      "[2, 20]: loss: 0.04637789027765393\n",
      "[2, 25]: loss: 0.08829095214605331\n",
      "[2, 30]: loss: 0.08977074478752911\n",
      "[2, 35]: loss: 0.011858509038574994\n",
      "[2, 40]: loss: 0.1625108520674985\n",
      "[2, 45]: loss: 0.043677562702214345\n",
      "[2, 50]: loss: 0.008943791995989159\n",
      "[2, 55]: loss: 0.06626619733287953\n",
      "[2, 60]: loss: 0.0344679644331336\n",
      "[2, 65]: loss: 0.019790802558418363\n",
      "[2, 70]: loss: 0.011885121697559953\n",
      "[2, 75]: loss: 0.026439275010488927\n",
      "[2, 80]: loss: 0.012643465801374987\n",
      "[2, 85]: loss: 0.19160246208775789\n",
      "[2, 90]: loss: 0.04016757314093411\n",
      "[2, 95]: loss: 0.14293277729302645\n",
      "[2, 100]: loss: 0.01663242661743425\n",
      "[2, 105]: loss: 0.07283968175761402\n",
      "[2, 110]: loss: 0.10771691997069865\n",
      "[2, 115]: loss: 0.033684223832096905\n",
      "[2, 120]: loss: 0.022935347631573677\n",
      "[2, 125]: loss: 0.1359211376402527\n",
      "[2, 130]: loss: 0.05371048499364406\n",
      "[2, 135]: loss: 0.07950700377114117\n",
      "[3, 5]: loss: 0.05178206879645586\n",
      "[3, 10]: loss: 0.01758027297910303\n",
      "[3, 15]: loss: 0.046146889857482165\n",
      "[3, 20]: loss: 0.020196918165311217\n",
      "[3, 25]: loss: 0.006479341711383313\n",
      "[3, 30]: loss: 0.009901109064230695\n",
      "[3, 35]: loss: 0.006883514346554875\n",
      "[3, 40]: loss: 0.010232422588160262\n",
      "[3, 45]: loss: 0.059072912350529805\n",
      "[3, 50]: loss: 0.06574149278458208\n",
      "[3, 55]: loss: 0.04623927013017237\n",
      "[3, 60]: loss: 0.18813483929261565\n",
      "[3, 65]: loss: 0.08582435094285756\n",
      "[3, 70]: loss: 0.05141479603480548\n",
      "[3, 75]: loss: 0.012611543876118958\n",
      "[3, 80]: loss: 0.012092969816876575\n",
      "[3, 85]: loss: 0.0060997778055025265\n",
      "[3, 90]: loss: 0.10044155683135614\n",
      "[3, 95]: loss: 0.019680368073750287\n",
      "[3, 100]: loss: 0.028025143314152956\n",
      "[3, 105]: loss: 0.017583340682904236\n",
      "[3, 110]: loss: 0.010084487410495058\n",
      "[3, 115]: loss: 0.03989822429139167\n",
      "[3, 120]: loss: 0.020773032039869577\n",
      "[3, 125]: loss: 0.01037546992301941\n",
      "[3, 130]: loss: 0.07920528165413998\n",
      "[3, 135]: loss: 0.019589772142353468\n",
      "[4, 5]: loss: 0.01674873981392011\n",
      "[4, 10]: loss: 0.00428712455322966\n",
      "[4, 15]: loss: 0.0371423636097461\n",
      "[4, 20]: loss: 0.05430051631992683\n",
      "[4, 25]: loss: 0.02696411474607885\n",
      "[4, 30]: loss: 0.004987743479432538\n",
      "[4, 35]: loss: 0.022996999410679564\n",
      "[4, 40]: loss: 0.006846169213531539\n",
      "[4, 45]: loss: 0.05713594672852196\n",
      "[4, 50]: loss: 0.0558059886097908\n",
      "[4, 55]: loss: 0.010543677199166268\n",
      "[4, 60]: loss: 0.0230531825509388\n",
      "[4, 65]: loss: 0.0138814581441693\n",
      "[4, 70]: loss: 0.07285339676309377\n",
      "[4, 75]: loss: 0.007959185357321985\n",
      "[4, 80]: loss: 0.0170952670450788\n",
      "[4, 85]: loss: 0.03153273745556362\n",
      "[4, 90]: loss: 0.010627204086631536\n",
      "[4, 95]: loss: 0.030766930954996496\n",
      "[4, 100]: loss: 0.004253690043697134\n",
      "[4, 105]: loss: 0.020106544630834833\n",
      "[4, 110]: loss: 0.026709173660492525\n",
      "[4, 115]: loss: 0.027618587715551257\n",
      "[4, 120]: loss: 0.009186626790324226\n",
      "[4, 125]: loss: 0.006476952603406971\n",
      "[4, 130]: loss: 0.04535751446383074\n",
      "[4, 135]: loss: 0.010008719196775928\n",
      "[5, 5]: loss: 0.0050227047759108245\n",
      "[5, 10]: loss: 0.010393837030278519\n",
      "[5, 15]: loss: 0.016280962714517955\n",
      "[5, 20]: loss: 0.04023640832747333\n",
      "[5, 25]: loss: 0.011386469639546704\n",
      "[5, 30]: loss: 0.0025089716800721362\n",
      "[5, 35]: loss: 0.008983994834125042\n",
      "[5, 40]: loss: 0.01041119359433651\n",
      "[5, 45]: loss: 0.0032887531560845673\n",
      "[5, 50]: loss: 0.0017158830014523119\n",
      "[5, 55]: loss: 0.013521120243240148\n",
      "[5, 60]: loss: 0.002317615260835737\n",
      "[5, 65]: loss: 0.003561628509487491\n",
      "[5, 70]: loss: 0.017955810439161723\n",
      "[5, 75]: loss: 0.008196433365810663\n",
      "[5, 80]: loss: 0.029502404900995316\n",
      "[5, 85]: loss: 0.011565608831006102\n",
      "[5, 90]: loss: 0.05606193852145225\n",
      "[5, 95]: loss: 0.016722502215998247\n",
      "[5, 100]: loss: 0.019649100431706756\n",
      "[5, 105]: loss: 0.036144096418865956\n",
      "[5, 110]: loss: 0.009042087578563951\n",
      "[5, 115]: loss: 0.09121476828295272\n",
      "[5, 120]: loss: 0.10085914596857037\n",
      "[5, 125]: loss: 0.011464289709692821\n",
      "[5, 130]: loss: 0.06532217049971223\n",
      "[5, 135]: loss: 0.005374550848500803\n",
      "[6, 5]: loss: 0.03126817991142161\n",
      "[6, 10]: loss: 0.005405470335972495\n",
      "[6, 15]: loss: 0.045266273693414405\n",
      "[6, 20]: loss: 0.005801501276437193\n",
      "[6, 25]: loss: 0.006097673875046894\n",
      "[6, 30]: loss: 0.030598557670600712\n",
      "[6, 35]: loss: 0.0385575966356555\n",
      "[6, 40]: loss: 0.07535771558468696\n",
      "[6, 45]: loss: 0.028151833248557523\n",
      "[6, 50]: loss: 0.1278736884414684\n",
      "[6, 55]: loss: 0.08701763943827245\n",
      "[6, 60]: loss: 0.008932818527682684\n",
      "[6, 65]: loss: 0.06056351351435296\n",
      "[6, 70]: loss: 0.014017315465025604\n",
      "[6, 75]: loss: 0.007396204382530414\n",
      "[6, 80]: loss: 0.0021591854019789025\n",
      "[6, 85]: loss: 0.013459973990393337\n",
      "[6, 90]: loss: 0.009174690305371769\n",
      "[6, 95]: loss: 0.02123089606175199\n",
      "[6, 100]: loss: 0.011024160718079656\n",
      "[6, 105]: loss: 0.021864267793716863\n",
      "[6, 110]: loss: 0.0028583738894667476\n",
      "[6, 115]: loss: 0.006386117449437734\n",
      "[6, 120]: loss: 0.014612241619033739\n",
      "[6, 125]: loss: 0.009117906214669347\n",
      "[6, 130]: loss: 0.01888660319673363\n",
      "[6, 135]: loss: 0.016213990005780943\n",
      "[7, 5]: loss: 0.005261456753942184\n",
      "[7, 10]: loss: 0.002340707811526954\n",
      "[7, 15]: loss: 0.010839401489647571\n",
      "[7, 20]: loss: 0.01707214022462722\n",
      "[7, 25]: loss: 0.015761245231260546\n",
      "[7, 30]: loss: 0.002659606107044965\n",
      "[7, 35]: loss: 0.011239830579143018\n",
      "[7, 40]: loss: 0.04351469117682427\n",
      "[7, 45]: loss: 0.005000930352252908\n",
      "[7, 50]: loss: 0.007769478252157569\n",
      "[7, 55]: loss: 0.0076447605970315635\n",
      "[7, 60]: loss: 0.0029407656111288816\n",
      "[7, 65]: loss: 0.002301548214745708\n",
      "[7, 70]: loss: 0.004182216063782107\n",
      "[7, 75]: loss: 0.013787398696877062\n",
      "[7, 80]: loss: 0.013311113420058973\n",
      "[7, 85]: loss: 0.01577384158008499\n",
      "[7, 90]: loss: 0.0024376509099965915\n",
      "[7, 95]: loss: 0.0012555340072140098\n",
      "[7, 100]: loss: 0.021092079230584204\n",
      "[7, 105]: loss: 0.015232711055432446\n",
      "[7, 110]: loss: 0.004245348089170875\n",
      "[7, 115]: loss: 0.002855793689377606\n",
      "[7, 120]: loss: 0.016756578392232768\n",
      "[7, 125]: loss: 0.0014193952229106799\n",
      "[7, 130]: loss: 0.0015458365087397397\n",
      "[7, 135]: loss: 0.004752017339342274\n",
      "[8, 5]: loss: 0.007676844747038558\n",
      "[8, 10]: loss: 0.00816724234027788\n",
      "[8, 15]: loss: 0.008468394575174898\n",
      "[8, 20]: loss: 0.008563672046875581\n",
      "[8, 25]: loss: 0.0018000595446210355\n",
      "[8, 30]: loss: 0.011700855098752072\n",
      "[8, 35]: loss: 0.006341599961160682\n",
      "[8, 40]: loss: 0.0025773028974072076\n",
      "[8, 45]: loss: 0.002049787755822763\n",
      "[8, 50]: loss: 0.005000813631340861\n",
      "[8, 55]: loss: 0.0011031593894585967\n",
      "[8, 60]: loss: 0.0016032406711019576\n",
      "[8, 65]: loss: 0.002954616014903877\n",
      "[8, 70]: loss: 0.01447379615274258\n",
      "[8, 75]: loss: 0.0026009750727098435\n",
      "[8, 80]: loss: 0.0023416544645442627\n",
      "[8, 85]: loss: 0.0013843850756529719\n",
      "[8, 90]: loss: 0.0018287325510755181\n",
      "[8, 95]: loss: 0.030494978360366076\n",
      "[8, 100]: loss: 0.006631325799389742\n",
      "[8, 105]: loss: 0.004384727275464684\n",
      "[8, 110]: loss: 0.016567798098549247\n",
      "[8, 115]: loss: 0.007468777053873055\n",
      "[8, 120]: loss: 0.003500194252410438\n",
      "[8, 125]: loss: 0.003639255039161071\n",
      "[8, 130]: loss: 0.0035454228636808693\n",
      "[8, 135]: loss: 0.003210941460565664\n",
      "[9, 5]: loss: 0.001809995230360073\n",
      "[9, 10]: loss: 0.0009131851838901639\n",
      "[9, 15]: loss: 0.0017813851300161332\n",
      "[9, 20]: loss: 0.002702071869862266\n",
      "[9, 25]: loss: 0.0035806492724077543\n",
      "[9, 30]: loss: 0.008319069747813046\n",
      "[9, 35]: loss: 0.0018127519433619455\n",
      "[9, 40]: loss: 0.0077219445083756\n",
      "[9, 45]: loss: 0.00501394469756633\n",
      "[9, 50]: loss: 0.002013550682022469\n",
      "[9, 55]: loss: 0.004648298967367737\n",
      "[9, 60]: loss: 0.0031518093164777383\n",
      "[9, 65]: loss: 0.00805677840980934\n",
      "[9, 70]: loss: 0.00750342445098795\n",
      "[9, 75]: loss: 0.0061992575210751966\n",
      "[9, 80]: loss: 0.003540205070748925\n",
      "[9, 85]: loss: 0.004925623332383111\n",
      "[9, 90]: loss: 0.004578780266456306\n",
      "[9, 95]: loss: 0.0022388252182281576\n",
      "[9, 100]: loss: 0.0005670950558851473\n",
      "[9, 105]: loss: 0.0022925168450456113\n",
      "[9, 110]: loss: 0.0018421306303935125\n",
      "[9, 115]: loss: 0.002732908891630359\n",
      "[9, 120]: loss: 0.002332671072508674\n",
      "[9, 125]: loss: 0.004479022027226165\n",
      "[9, 130]: loss: 0.0009270651789847761\n",
      "[9, 135]: loss: 0.0005488058923219796\n",
      "[10, 5]: loss: 0.009483114939939696\n",
      "[10, 10]: loss: 0.0012701456616923679\n",
      "[10, 15]: loss: 0.0044995376956649125\n",
      "[10, 20]: loss: 0.0038926635588723\n",
      "[10, 25]: loss: 0.0015685901162214577\n",
      "[10, 30]: loss: 0.0015758946246933192\n",
      "[10, 35]: loss: 0.011836582358228043\n",
      "[10, 40]: loss: 0.010163543134694919\n",
      "[10, 45]: loss: 0.0013352105524973013\n",
      "[10, 50]: loss: 0.005533982126507908\n",
      "[10, 55]: loss: 0.002290714517585002\n",
      "[10, 60]: loss: 0.004174323752522469\n",
      "[10, 65]: loss: 0.0024785015120869502\n",
      "[10, 70]: loss: 0.0017356315220240504\n",
      "[10, 75]: loss: 0.0017196884291479364\n",
      "[10, 80]: loss: 0.0139064820468775\n",
      "[10, 85]: loss: 0.005379156194976531\n",
      "[10, 90]: loss: 0.0022649927414022386\n",
      "[10, 95]: loss: 0.0037583482189802453\n",
      "[10, 100]: loss: 0.02352116185647901\n",
      "[10, 105]: loss: 0.012857972837082343\n",
      "[10, 110]: loss: 0.001540537388791563\n",
      "[10, 115]: loss: 0.005150246863195207\n",
      "[10, 120]: loss: 0.003975512016040739\n",
      "[10, 125]: loss: 0.0017394464666722342\n",
      "[10, 130]: loss: 0.005821619415655732\n",
      "[10, 135]: loss: 0.0014027090219315141\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 17: 0.5281\n",
      "Random Sampling Iteration 18\n",
      "[1, 5]: loss: 0.039024963552947156\n",
      "[1, 10]: loss: 0.006920696519955527\n",
      "[1, 15]: loss: 0.43866836465895176\n",
      "[1, 20]: loss: 0.20948682163725607\n",
      "[1, 25]: loss: 0.11273878742940724\n",
      "[1, 30]: loss: 0.282069462351501\n",
      "[1, 35]: loss: 0.16305517905857414\n",
      "[1, 40]: loss: 0.05740639730356634\n",
      "[1, 45]: loss: 0.2173230736516416\n",
      "[1, 50]: loss: 0.23582320637069643\n",
      "[1, 55]: loss: 0.19050356931984425\n",
      "[1, 60]: loss: 0.06376968811673578\n",
      "[1, 65]: loss: 0.281917427200824\n",
      "[1, 70]: loss: 0.11706688394770026\n",
      "[1, 75]: loss: 0.14512823428958654\n",
      "[1, 80]: loss: 0.357476313598454\n",
      "[1, 85]: loss: 0.10721009306143969\n",
      "[1, 90]: loss: 0.3901643205899745\n",
      "[1, 95]: loss: 0.1182106554042548\n",
      "[1, 100]: loss: 0.3438457199954428\n",
      "[1, 105]: loss: 0.45121208974160254\n",
      "[1, 110]: loss: 0.6069008794002002\n",
      "[1, 115]: loss: 0.29146855417639017\n",
      "[1, 120]: loss: 0.2999928495846689\n",
      "[1, 125]: loss: 0.08685362758114934\n",
      "[1, 130]: loss: 0.14653171111422125\n",
      "[1, 135]: loss: 0.3679774471092969\n",
      "[1, 140]: loss: 0.2917907703667879\n",
      "[2, 5]: loss: 0.018112388905137777\n",
      "[2, 10]: loss: 0.014895132364472374\n",
      "[2, 15]: loss: 0.047899239987600595\n",
      "[2, 20]: loss: 0.007541080005466938\n",
      "[2, 25]: loss: 0.03083034884184599\n",
      "[2, 30]: loss: 0.02665721086668782\n",
      "[2, 35]: loss: 0.08196651030448265\n",
      "[2, 40]: loss: 0.03855418332386762\n",
      "[2, 45]: loss: 0.041210834635421634\n",
      "[2, 50]: loss: 0.10011433716863394\n",
      "[2, 55]: loss: 0.1045893557020463\n",
      "[2, 60]: loss: 0.06598582130391151\n",
      "[2, 65]: loss: 0.08691335504408926\n",
      "[2, 70]: loss: 0.06399863856495358\n",
      "[2, 75]: loss: 0.16515221633017063\n",
      "[2, 80]: loss: 0.0320338235469535\n",
      "[2, 85]: loss: 0.0467143984278664\n",
      "[2, 90]: loss: 0.029109032067935914\n",
      "[2, 95]: loss: 0.034984306432306767\n",
      "[2, 100]: loss: 0.017018938495311886\n",
      "[2, 105]: loss: 0.03285468026297167\n",
      "[2, 110]: loss: 0.01494971095235087\n",
      "[2, 115]: loss: 0.018924185933428816\n",
      "[2, 120]: loss: 0.016409890435170382\n",
      "[2, 125]: loss: 0.11330454019844183\n",
      "[2, 130]: loss: 0.03285614971537143\n",
      "[2, 135]: loss: 0.011885530257131904\n",
      "[2, 140]: loss: 0.021485182107426226\n",
      "[3, 5]: loss: 0.004432427682331763\n",
      "[3, 10]: loss: 0.009189418007736094\n",
      "[3, 15]: loss: 0.03615767379233148\n",
      "[3, 20]: loss: 0.02459069326869212\n",
      "[3, 25]: loss: 0.006010982160660205\n",
      "[3, 30]: loss: 0.016729648923501372\n",
      "[3, 35]: loss: 0.010653857316356152\n",
      "[3, 40]: loss: 0.006191184344061185\n",
      "[3, 45]: loss: 0.059627905662637204\n",
      "[3, 50]: loss: 0.0032476341002620757\n",
      "[3, 55]: loss: 0.019446061283815652\n",
      "[3, 60]: loss: 0.01553361225523986\n",
      "[3, 65]: loss: 0.02015013201162219\n",
      "[3, 70]: loss: 0.012356015184195712\n",
      "[3, 75]: loss: 0.01012783611076884\n",
      "[3, 80]: loss: 0.005256021715467796\n",
      "[3, 85]: loss: 0.009197512379614636\n",
      "[3, 90]: loss: 0.007706062708166428\n",
      "[3, 95]: loss: 0.04522025894402759\n",
      "[3, 100]: loss: 0.012181945581687614\n",
      "[3, 105]: loss: 0.04440434119896963\n",
      "[3, 110]: loss: 0.009524471854092553\n",
      "[3, 115]: loss: 0.03006862815527711\n",
      "[3, 120]: loss: 0.015239642496453598\n",
      "[3, 125]: loss: 0.02730715817597229\n",
      "[3, 130]: loss: 0.014263278106227517\n",
      "[3, 135]: loss: 0.01048899462330155\n",
      "[3, 140]: loss: 0.05652268702397123\n",
      "[4, 5]: loss: 0.008658527163788676\n",
      "[4, 10]: loss: 0.039639842172618955\n",
      "[4, 15]: loss: 0.0036592687101801857\n",
      "[4, 20]: loss: 0.028397324997058604\n",
      "[4, 25]: loss: 0.0026294475537724793\n",
      "[4, 30]: loss: 0.058767862094100565\n",
      "[4, 35]: loss: 0.015315843491407577\n",
      "[4, 40]: loss: 0.010335768820368685\n",
      "[4, 45]: loss: 0.01378308240964543\n",
      "[4, 50]: loss: 0.011539522500243038\n",
      "[4, 55]: loss: 0.009888150845654309\n",
      "[4, 60]: loss: 0.0909906760789454\n",
      "[4, 65]: loss: 0.01039805737673305\n",
      "[4, 70]: loss: 0.01134928634564858\n",
      "[4, 75]: loss: 0.013228665877250023\n",
      "[4, 80]: loss: 0.05983867919712793\n",
      "[4, 85]: loss: 0.010180573182879016\n",
      "[4, 90]: loss: 0.01800351963902358\n",
      "[4, 95]: loss: 0.07231935628806241\n",
      "[4, 100]: loss: 0.03215469908900559\n",
      "[4, 105]: loss: 0.059707124542910606\n",
      "[4, 110]: loss: 0.012492276495322585\n",
      "[4, 115]: loss: 0.0665321733104065\n",
      "[4, 120]: loss: 0.046986260800622404\n",
      "[4, 125]: loss: 0.017743931617587805\n",
      "[4, 130]: loss: 0.19876643101451918\n",
      "[4, 135]: loss: 0.04770365642616525\n",
      "[4, 140]: loss: 0.006659872364252806\n",
      "[5, 5]: loss: 0.007970034901518375\n",
      "[5, 10]: loss: 0.06400106882210821\n",
      "[5, 15]: loss: 0.03849698312114924\n",
      "[5, 20]: loss: 0.01945129595696926\n",
      "[5, 25]: loss: 0.11704732483485714\n",
      "[5, 30]: loss: 0.022773621371015906\n",
      "[5, 35]: loss: 0.07189307740190998\n",
      "[5, 40]: loss: 0.057600235362770036\n",
      "[5, 45]: loss: 0.019460299983620644\n",
      "[5, 50]: loss: 0.08650942402891815\n",
      "[5, 55]: loss: 0.030728231242392212\n",
      "[5, 60]: loss: 0.015861248517467175\n",
      "[5, 65]: loss: 0.02956038183765486\n",
      "[5, 70]: loss: 0.0731208871002309\n",
      "[5, 75]: loss: 0.06333546648966148\n",
      "[5, 80]: loss: 0.10281727381516248\n",
      "[5, 85]: loss: 0.14209101421874948\n",
      "[5, 90]: loss: 0.0166454617283307\n",
      "[5, 95]: loss: 0.02797078226285521\n",
      "[5, 100]: loss: 0.009233622986357659\n",
      "[5, 105]: loss: 0.017623243998968974\n",
      "[5, 110]: loss: 0.15832398959901184\n",
      "[5, 115]: loss: 0.01835387889877893\n",
      "[5, 120]: loss: 0.08728278637863696\n",
      "[5, 125]: loss: 0.08386018060264178\n",
      "[5, 130]: loss: 0.06657716480549425\n",
      "[5, 135]: loss: 0.1339339000696782\n",
      "[5, 140]: loss: 0.019237425789469853\n",
      "[6, 5]: loss: 0.03684214345412329\n",
      "[6, 10]: loss: 0.012354166363365948\n",
      "[6, 15]: loss: 0.015430082770762965\n",
      "[6, 20]: loss: 0.04305254393693758\n",
      "[6, 25]: loss: 0.03877101780381054\n",
      "[6, 30]: loss: 0.06208202266134322\n",
      "[6, 35]: loss: 0.030151893675792962\n",
      "[6, 40]: loss: 0.017496427317382768\n",
      "[6, 45]: loss: 0.024044001300353557\n",
      "[6, 50]: loss: 0.018577540758997202\n",
      "[6, 55]: loss: 0.03609686568961479\n",
      "[6, 60]: loss: 0.021509041311219335\n",
      "[6, 65]: loss: 0.009224213637935463\n",
      "[6, 70]: loss: 0.018814025854226202\n",
      "[6, 75]: loss: 0.0024177796512958594\n",
      "[6, 80]: loss: 0.06731461966410279\n",
      "[6, 85]: loss: 0.010001323942560703\n",
      "[6, 90]: loss: 0.03928046626970172\n",
      "[6, 95]: loss: 0.0878898958035279\n",
      "[6, 100]: loss: 0.057769956663833\n",
      "[6, 105]: loss: 0.022321670898236334\n",
      "[6, 110]: loss: 0.01498598710168153\n",
      "[6, 115]: loss: 0.031827437109313905\n",
      "[6, 120]: loss: 0.049714879860403016\n",
      "[6, 125]: loss: 0.026122042443603277\n",
      "[6, 130]: loss: 0.006007910473272204\n",
      "[6, 135]: loss: 0.011362440200173296\n",
      "[6, 140]: loss: 0.14996125796460547\n",
      "[7, 5]: loss: 0.007022865494946018\n",
      "[7, 10]: loss: 0.011253791133640334\n",
      "[7, 15]: loss: 0.026689279591664672\n",
      "[7, 20]: loss: 0.03405855338496622\n",
      "[7, 25]: loss: 0.00821096557774581\n",
      "[7, 30]: loss: 0.01993947639130056\n",
      "[7, 35]: loss: 0.002852090787200723\n",
      "[7, 40]: loss: 0.021847121795872226\n",
      "[7, 45]: loss: 0.0023902641551103443\n",
      "[7, 50]: loss: 0.02787844872364076\n",
      "[7, 55]: loss: 0.009830099792452529\n",
      "[7, 60]: loss: 0.004659353886381723\n",
      "[7, 65]: loss: 0.018643074057763442\n",
      "[7, 70]: loss: 0.006926757472683676\n",
      "[7, 75]: loss: 0.0031359094427898526\n",
      "[7, 80]: loss: 0.0012222381446918007\n",
      "[7, 85]: loss: 0.005066572310170159\n",
      "[7, 90]: loss: 0.011344367288984358\n",
      "[7, 95]: loss: 0.016448831469460856\n",
      "[7, 100]: loss: 0.0037110645789653063\n",
      "[7, 105]: loss: 0.002286613322212361\n",
      "[7, 110]: loss: 0.012838877097237855\n",
      "[7, 115]: loss: 0.0072054663905873895\n",
      "[7, 120]: loss: 0.004389807698316872\n",
      "[7, 125]: loss: 0.0053939260833431035\n",
      "[7, 130]: loss: 0.0060155313112773\n",
      "[7, 135]: loss: 0.10019002010812983\n",
      "[7, 140]: loss: 0.020533415547106415\n",
      "[8, 5]: loss: 0.008661416446557269\n",
      "[8, 10]: loss: 0.01034232199890539\n",
      "[8, 15]: loss: 0.012145662069087848\n",
      "[8, 20]: loss: 0.02761331183137372\n",
      "[8, 25]: loss: 0.006571736914338544\n",
      "[8, 30]: loss: 0.00946000924159307\n",
      "[8, 35]: loss: 0.013066652012639679\n",
      "[8, 40]: loss: 0.029152477498428198\n",
      "[8, 45]: loss: 0.0034291925403522328\n",
      "[8, 50]: loss: 0.01298596482956782\n",
      "[8, 55]: loss: 0.007861309917643666\n",
      "[8, 60]: loss: 0.01815285364864394\n",
      "[8, 65]: loss: 0.014587449899408966\n",
      "[8, 70]: loss: 0.019387348933378235\n",
      "[8, 75]: loss: 0.004471898078918457\n",
      "[8, 80]: loss: 0.011951726002735086\n",
      "[8, 85]: loss: 0.007406865610391833\n",
      "[8, 90]: loss: 0.013962728262413293\n",
      "[8, 95]: loss: 0.011425412958487868\n",
      "[8, 100]: loss: 0.0028355803951853886\n",
      "[8, 105]: loss: 0.011697249196004122\n",
      "[8, 110]: loss: 0.0062613087429781444\n",
      "[8, 115]: loss: 0.020776112971361727\n",
      "[8, 120]: loss: 0.004056113510159776\n",
      "[8, 125]: loss: 0.006478660230641253\n",
      "[8, 130]: loss: 0.011611960333539173\n",
      "[8, 135]: loss: 0.0011316566378809512\n",
      "[8, 140]: loss: 0.006221992647624575\n",
      "[9, 5]: loss: 0.013283483101986349\n",
      "[9, 10]: loss: 0.005191884745727293\n",
      "[9, 15]: loss: 0.004634881574020255\n",
      "[9, 20]: loss: 0.020450772120966576\n",
      "[9, 25]: loss: 0.0017141960706794634\n",
      "[9, 30]: loss: 0.004110513167688623\n",
      "[9, 35]: loss: 0.00872983320732601\n",
      "[9, 40]: loss: 0.007550075824838132\n",
      "[9, 45]: loss: 0.0015896277473075315\n",
      "[9, 50]: loss: 0.004293452686397359\n",
      "[9, 55]: loss: 0.0035243825404904783\n",
      "[9, 60]: loss: 0.005712244579626713\n",
      "[9, 65]: loss: 0.00939563411520794\n",
      "[9, 70]: loss: 0.004009956086520106\n",
      "[9, 75]: loss: 0.00506901991320774\n",
      "[9, 80]: loss: 0.0066165543685201555\n",
      "[9, 85]: loss: 0.0008878332009771839\n",
      "[9, 90]: loss: 0.02777899900684133\n",
      "[9, 95]: loss: 0.008713675109902397\n",
      "[9, 100]: loss: 0.005854493385413662\n",
      "[9, 105]: loss: 0.003903367934981361\n",
      "[9, 110]: loss: 0.0031151507719187066\n",
      "[9, 115]: loss: 0.010748718472314067\n",
      "[9, 120]: loss: 0.010259149159537628\n",
      "[9, 125]: loss: 0.01001454296783777\n",
      "[9, 130]: loss: 0.05670927866594866\n",
      "[9, 135]: loss: 0.0033498942357255146\n",
      "[9, 140]: loss: 0.0023818231384211686\n",
      "[10, 5]: loss: 0.002110779983922839\n",
      "[10, 10]: loss: 0.00450357449153671\n",
      "[10, 15]: loss: 0.0150277501961682\n",
      "[10, 20]: loss: 0.0033093531237682328\n",
      "[10, 25]: loss: 0.0027547829085960984\n",
      "[10, 30]: loss: 0.004719591983302962\n",
      "[10, 35]: loss: 0.008147476357407868\n",
      "[10, 40]: loss: 0.009359432471683249\n",
      "[10, 45]: loss: 0.0060664049233309925\n",
      "[10, 50]: loss: 0.02946276805596426\n",
      "[10, 55]: loss: 0.006312616402283311\n",
      "[10, 60]: loss: 0.0013193261038395576\n",
      "[10, 65]: loss: 0.0069117793609621\n",
      "[10, 70]: loss: 0.007468060313840397\n",
      "[10, 75]: loss: 0.0030339715303853154\n",
      "[10, 80]: loss: 0.00858996156603098\n",
      "[10, 85]: loss: 0.008635217251139693\n",
      "[10, 90]: loss: 0.012883727154985536\n",
      "[10, 95]: loss: 0.026124972726393025\n",
      "[10, 100]: loss: 0.0080084421497304\n",
      "[10, 105]: loss: 0.005947604789980687\n",
      "[10, 110]: loss: 0.003271006513386965\n",
      "[10, 115]: loss: 0.03556469263276085\n",
      "[10, 120]: loss: 0.017343867723866424\n",
      "[10, 125]: loss: 0.014954221849620808\n",
      "[10, 130]: loss: 0.021472268068464473\n",
      "[10, 135]: loss: 0.0031466521322727203\n",
      "[10, 140]: loss: 0.005419744877144694\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 18: 0.5355\n",
      "Random Sampling Iteration 19\n",
      "[1, 5]: loss: 0.062082653981633484\n",
      "[1, 10]: loss: 0.14130553224822506\n",
      "[1, 15]: loss: 0.08942443481646478\n",
      "[1, 20]: loss: 0.06809420391800813\n",
      "[1, 25]: loss: 0.7164697792031802\n",
      "[1, 30]: loss: 0.18672331975540146\n",
      "[1, 35]: loss: 0.09598600026220083\n",
      "[1, 40]: loss: 0.5046392363728955\n",
      "[1, 45]: loss: 0.4282916858792305\n",
      "[1, 50]: loss: 0.32846649643033743\n",
      "[1, 55]: loss: 0.19843811681494117\n",
      "[1, 60]: loss: 0.11978449404705316\n",
      "[1, 65]: loss: 0.17411447199992836\n",
      "[1, 70]: loss: 0.3846311615779996\n",
      "[1, 75]: loss: 0.14158330205827951\n",
      "[1, 80]: loss: 0.20878531108610332\n",
      "[1, 85]: loss: 0.36471913754940033\n",
      "[1, 90]: loss: 0.18585908762179315\n",
      "[1, 95]: loss: 0.4266518894582987\n",
      "[1, 100]: loss: 0.10477754974272102\n",
      "[1, 105]: loss: 0.2679993286728859\n",
      "[1, 110]: loss: 0.11688837248948403\n",
      "[1, 115]: loss: 0.22979234589729458\n",
      "[1, 120]: loss: 0.1876320422743447\n",
      "[1, 125]: loss: 0.21985492040403187\n",
      "[1, 130]: loss: 0.14792318269610405\n",
      "[1, 135]: loss: 0.13353486801497638\n",
      "[1, 140]: loss: 0.019510204205289483\n",
      "[1, 145]: loss: 0.08351886481977999\n",
      "[1, 150]: loss: 0.09115350124193355\n",
      "[2, 5]: loss: 0.01519437288516201\n",
      "[2, 10]: loss: 0.0453397260280326\n",
      "[2, 15]: loss: 0.04400220382376574\n",
      "[2, 20]: loss: 0.013250049552880228\n",
      "[2, 25]: loss: 0.04403365024336381\n",
      "[2, 30]: loss: 0.03161695576272905\n",
      "[2, 35]: loss: 0.013309999951161444\n",
      "[2, 40]: loss: 0.05564204455004074\n",
      "[2, 45]: loss: 0.00801972282351926\n",
      "[2, 50]: loss: 0.09727458405541256\n",
      "[2, 55]: loss: 0.04524636987480335\n",
      "[2, 60]: loss: 0.025217122631147504\n",
      "[2, 65]: loss: 0.010846076562302187\n",
      "[2, 70]: loss: 0.0382435314822942\n",
      "[2, 75]: loss: 0.012289994629099965\n",
      "[2, 80]: loss: 0.021597397251753137\n",
      "[2, 85]: loss: 0.0546475873561576\n",
      "[2, 90]: loss: 0.005619885545456782\n",
      "[2, 95]: loss: 0.0557067183253821\n",
      "[2, 100]: loss: 0.020913292071782053\n",
      "[2, 105]: loss: 0.09047195271705277\n",
      "[2, 110]: loss: 0.020017457922222093\n",
      "[2, 115]: loss: 0.010757185344118625\n",
      "[2, 120]: loss: 0.01741113567550201\n",
      "[2, 125]: loss: 0.055068899237085134\n",
      "[2, 130]: loss: 0.018682822032133117\n",
      "[2, 135]: loss: 0.05022368727077264\n",
      "[2, 140]: loss: 0.01733845061971806\n",
      "[2, 145]: loss: 0.00770746152556967\n",
      "[2, 150]: loss: 0.010479073418537155\n",
      "[3, 5]: loss: 0.04613646597135812\n",
      "[3, 10]: loss: 0.008728443994186819\n",
      "[3, 15]: loss: 0.004733221081551164\n",
      "[3, 20]: loss: 0.013469678393448703\n",
      "[3, 25]: loss: 0.013560220701037906\n",
      "[3, 30]: loss: 0.009966427496692631\n",
      "[3, 35]: loss: 0.00416160756140016\n",
      "[3, 40]: loss: 0.05040585808455944\n",
      "[3, 45]: loss: 0.00965250883018598\n",
      "[3, 50]: loss: 0.009254557226086035\n",
      "[3, 55]: loss: 0.004172725064563565\n",
      "[3, 60]: loss: 0.022381575079634786\n",
      "[3, 65]: loss: 0.005394398613134399\n",
      "[3, 70]: loss: 0.018652434278919827\n",
      "[3, 75]: loss: 0.03190768993226811\n",
      "[3, 80]: loss: 0.0034743323631118983\n",
      "[3, 85]: loss: 0.020475513752899133\n",
      "[3, 90]: loss: 0.016875574132427573\n",
      "[3, 95]: loss: 0.012671653646975756\n",
      "[3, 100]: loss: 0.024787016212940216\n",
      "[3, 105]: loss: 0.007662953656108584\n",
      "[3, 110]: loss: 0.007750430217129178\n",
      "[3, 115]: loss: 0.022553377115400508\n",
      "[3, 120]: loss: 0.0061510357336374\n",
      "[3, 125]: loss: 0.005565028201090172\n",
      "[3, 130]: loss: 0.021596541220787913\n",
      "[3, 135]: loss: 0.02146167994942516\n",
      "[3, 140]: loss: 0.005960886453976855\n",
      "[3, 145]: loss: 0.04366818050039001\n",
      "[3, 150]: loss: 0.0031813684763619676\n",
      "[4, 5]: loss: 0.00688790125423111\n",
      "[4, 10]: loss: 0.005776127058197744\n",
      "[4, 15]: loss: 0.005487827264005318\n",
      "[4, 20]: loss: 0.016484869178384542\n",
      "[4, 25]: loss: 0.010471676956512965\n",
      "[4, 30]: loss: 0.002830702709616162\n",
      "[4, 35]: loss: 0.0027924427413381636\n",
      "[4, 40]: loss: 0.033993374672718346\n",
      "[4, 45]: loss: 0.00689996566507034\n",
      "[4, 50]: loss: 0.0789475958008552\n",
      "[4, 55]: loss: 0.050163703454018105\n",
      "[4, 60]: loss: 0.003019051000592299\n",
      "[4, 65]: loss: 0.022952266794163734\n",
      "[4, 70]: loss: 0.0019488844263833016\n",
      "[4, 75]: loss: 0.022052381478715688\n",
      "[4, 80]: loss: 0.016319454298354685\n",
      "[4, 85]: loss: 0.011171175305207726\n",
      "[4, 90]: loss: 0.0178988074476365\n",
      "[4, 95]: loss: 0.00747398502426222\n",
      "[4, 100]: loss: 0.009583487350028008\n",
      "[4, 105]: loss: 0.058069163700565696\n",
      "[4, 110]: loss: 0.012442125458619557\n",
      "[4, 115]: loss: 0.006874386570416391\n",
      "[4, 120]: loss: 0.03954881467507221\n",
      "[4, 125]: loss: 0.003903292235918343\n",
      "[4, 130]: loss: 0.01762033910199534\n",
      "[4, 135]: loss: 0.01406808738829568\n",
      "[4, 140]: loss: 0.055395106464857236\n",
      "[4, 145]: loss: 0.09469569800421596\n",
      "[4, 150]: loss: 0.008555538719519973\n",
      "[5, 5]: loss: 0.005981120571959764\n",
      "[5, 10]: loss: 0.021822858572704718\n",
      "[5, 15]: loss: 0.050395113532431424\n",
      "[5, 20]: loss: 0.014658540836535394\n",
      "[5, 25]: loss: 0.04489340490545146\n",
      "[5, 30]: loss: 0.006024662601703312\n",
      "[5, 35]: loss: 0.009501513835857622\n",
      "[5, 40]: loss: 0.00613465036440175\n",
      "[5, 45]: loss: 0.09914328178274445\n",
      "[5, 50]: loss: 0.0060107241733931005\n",
      "[5, 55]: loss: 0.035079409717582166\n",
      "[5, 60]: loss: 0.007542021194240078\n",
      "[5, 65]: loss: 0.026684367774578277\n",
      "[5, 70]: loss: 0.013229175703600049\n",
      "[5, 75]: loss: 0.1026147186930757\n",
      "[5, 80]: loss: 0.015557862934656441\n",
      "[5, 85]: loss: 0.008600325396400876\n",
      "[5, 90]: loss: 0.0073561775498092175\n",
      "[5, 95]: loss: 0.010971409086778294\n",
      "[5, 100]: loss: 0.02404475523508154\n",
      "[5, 105]: loss: 0.014067432086449116\n",
      "[5, 110]: loss: 0.059699399571400136\n",
      "[5, 115]: loss: 0.05788646031578537\n",
      "[5, 120]: loss: 0.09926231281133369\n",
      "[5, 125]: loss: 0.12009870260953903\n",
      "[5, 130]: loss: 0.19720233845873736\n",
      "[5, 135]: loss: 0.14036204328294843\n",
      "[5, 140]: loss: 0.03531090577598661\n",
      "[5, 145]: loss: 0.19813522743061185\n",
      "[5, 150]: loss: 0.22608136816415936\n",
      "[6, 5]: loss: 0.0744409435428679\n",
      "[6, 10]: loss: 0.040775104658678174\n",
      "[6, 15]: loss: 0.010170702822506428\n",
      "[6, 20]: loss: 0.11051882896572351\n",
      "[6, 25]: loss: 0.06276448641438037\n",
      "[6, 30]: loss: 0.20828609890304506\n",
      "[6, 35]: loss: 0.14374297205358744\n",
      "[6, 40]: loss: 0.029717783560045063\n",
      "[6, 45]: loss: 0.027191420915187337\n",
      "[6, 50]: loss: 0.09182623687956948\n",
      "[6, 55]: loss: 0.018129062722437084\n",
      "[6, 60]: loss: 0.05485305254114792\n",
      "[6, 65]: loss: 0.09974850979051553\n",
      "[6, 70]: loss: 0.04425652278587222\n",
      "[6, 75]: loss: 0.03460233368969057\n",
      "[6, 80]: loss: 0.10000363533617929\n",
      "[6, 85]: loss: 0.01857232210750226\n",
      "[6, 90]: loss: 0.17203340982086957\n",
      "[6, 95]: loss: 0.1694523219484836\n",
      "[6, 100]: loss: 0.047469288459979\n",
      "[6, 105]: loss: 0.077599167911103\n",
      "[6, 110]: loss: 0.08174460474401712\n",
      "[6, 115]: loss: 0.014003863965626806\n",
      "[6, 120]: loss: 0.021744648860476445\n",
      "[6, 125]: loss: 0.029974359553307295\n",
      "[6, 130]: loss: 0.05363240750739351\n",
      "[6, 135]: loss: 0.030777228024817305\n",
      "[6, 140]: loss: 0.09499847286497243\n",
      "[6, 145]: loss: 0.03726564790122211\n",
      "[6, 150]: loss: 0.014091757562709972\n",
      "[7, 5]: loss: 0.015175693319179118\n",
      "[7, 10]: loss: 0.02320116294140462\n",
      "[7, 15]: loss: 0.004989073320757598\n",
      "[7, 20]: loss: 0.002096958749461919\n",
      "[7, 25]: loss: 0.01964670157758519\n",
      "[7, 30]: loss: 0.003710863718879409\n",
      "[7, 35]: loss: 0.004739064665045589\n",
      "[7, 40]: loss: 0.01743920799344778\n",
      "[7, 45]: loss: 0.01894632156472653\n",
      "[7, 50]: loss: 0.021644720574840903\n",
      "[7, 55]: loss: 0.014311568898847327\n",
      "[7, 60]: loss: 0.00516910148144234\n",
      "[7, 65]: loss: 0.027563273208215833\n",
      "[7, 70]: loss: 0.006729018583428115\n",
      "[7, 75]: loss: 0.0062019056640565395\n",
      "[7, 80]: loss: 0.015416073860251345\n",
      "[7, 85]: loss: 0.004358297934231814\n",
      "[7, 90]: loss: 0.010836847999598831\n",
      "[7, 95]: loss: 0.003722327805007808\n",
      "[7, 100]: loss: 0.041281461250036955\n",
      "[7, 105]: loss: 0.008827132201986387\n",
      "[7, 110]: loss: 0.020653329469496384\n",
      "[7, 115]: loss: 0.0022565844119526446\n",
      "[7, 120]: loss: 0.004951691829774063\n",
      "[7, 125]: loss: 0.004002287823823281\n",
      "[7, 130]: loss: 0.027068932715337723\n",
      "[7, 135]: loss: 0.011774740894907154\n",
      "[7, 140]: loss: 0.01546710995171452\n",
      "[7, 145]: loss: 0.09386787906987593\n",
      "[7, 150]: loss: 0.0055671034642728046\n",
      "[8, 5]: loss: 0.007696986589508015\n",
      "[8, 10]: loss: 0.006576212530490011\n",
      "[8, 15]: loss: 0.008586958530941047\n",
      "[8, 20]: loss: 0.01310530782211572\n",
      "[8, 25]: loss: 0.008132660383125767\n",
      "[8, 30]: loss: 0.007447067473549396\n",
      "[8, 35]: loss: 0.009279668505769223\n",
      "[8, 40]: loss: 0.007936964291729964\n",
      "[8, 45]: loss: 0.007110565224138554\n",
      "[8, 50]: loss: 0.00970567436888814\n",
      "[8, 55]: loss: 0.014873693842673674\n",
      "[8, 60]: loss: 0.0033053246152121574\n",
      "[8, 65]: loss: 0.010579012174275704\n",
      "[8, 70]: loss: 0.007210974261397496\n",
      "[8, 75]: loss: 0.00782139104558155\n",
      "[8, 80]: loss: 0.0019402126999921165\n",
      "[8, 85]: loss: 0.007886108243837953\n",
      "[8, 90]: loss: 0.02749151757598156\n",
      "[8, 95]: loss: 0.0030065634637139738\n",
      "[8, 100]: loss: 0.031417190068168566\n",
      "[8, 105]: loss: 0.0024801313484204\n",
      "[8, 110]: loss: 0.004655649478081614\n",
      "[8, 115]: loss: 0.0025508630351396278\n",
      "[8, 120]: loss: 0.008230201376136392\n",
      "[8, 125]: loss: 0.007279659796040505\n",
      "[8, 130]: loss: 0.004210693459754111\n",
      "[8, 135]: loss: 0.006487459526397288\n",
      "[8, 140]: loss: 0.0035261804587207735\n",
      "[8, 145]: loss: 0.01030135960172629\n",
      "[8, 150]: loss: 0.008940482865000376\n",
      "[9, 5]: loss: 0.005132399994181469\n",
      "[9, 10]: loss: 0.011073238798417151\n",
      "[9, 15]: loss: 0.0055656825643382035\n",
      "[9, 20]: loss: 0.0020063944248249754\n",
      "[9, 25]: loss: 0.00453151817782782\n",
      "[9, 30]: loss: 0.0036830885073868558\n",
      "[9, 35]: loss: 0.014241191645851359\n",
      "[9, 40]: loss: 0.006078026635805145\n",
      "[9, 45]: loss: 0.0034908547677332535\n",
      "[9, 50]: loss: 0.006313234436674975\n",
      "[9, 55]: loss: 0.02001882768672658\n",
      "[9, 60]: loss: 0.00323392212158069\n",
      "[9, 65]: loss: 0.01652751100482419\n",
      "[9, 70]: loss: 0.00746137514943257\n",
      "[9, 75]: loss: 0.010382539185229689\n",
      "[9, 80]: loss: 0.007620700096595101\n",
      "[9, 85]: loss: 0.001730329247948248\n",
      "[9, 90]: loss: 0.00233322344138287\n",
      "[9, 95]: loss: 0.046580005320720375\n",
      "[9, 100]: loss: 0.0065409193048253655\n",
      "[9, 105]: loss: 0.007418345463520382\n",
      "[9, 110]: loss: 0.0017594258970348164\n",
      "[9, 115]: loss: 0.004895492544164881\n",
      "[9, 120]: loss: 0.0026307040698156925\n",
      "[9, 125]: loss: 0.011273621217696927\n",
      "[9, 130]: loss: 0.004904177330899984\n",
      "[9, 135]: loss: 0.03794003609800711\n",
      "[9, 140]: loss: 0.03961084660841152\n",
      "[9, 145]: loss: 0.0025543923693476245\n",
      "[9, 150]: loss: 0.0012663579036598094\n",
      "[10, 5]: loss: 0.001753747195834876\n",
      "[10, 10]: loss: 0.0028303788203629665\n",
      "[10, 15]: loss: 0.005538602359592915\n",
      "[10, 20]: loss: 0.005076643057691399\n",
      "[10, 25]: loss: 0.006552770282723941\n",
      "[10, 30]: loss: 0.0021964296101941727\n",
      "[10, 35]: loss: 0.0074122997029917315\n",
      "[10, 40]: loss: 0.016281891483231448\n",
      "[10, 45]: loss: 0.013508879223081749\n",
      "[10, 50]: loss: 0.016232340305577964\n",
      "[10, 55]: loss: 0.012872706836787984\n",
      "[10, 60]: loss: 0.002797878158162348\n",
      "[10, 65]: loss: 0.012923481364850886\n",
      "[10, 70]: loss: 0.008591723104473203\n",
      "[10, 75]: loss: 0.007705318595981225\n",
      "[10, 80]: loss: 0.009305378811404807\n",
      "[10, 85]: loss: 0.04368958162376657\n",
      "[10, 90]: loss: 0.013092096931359265\n",
      "[10, 95]: loss: 0.0022697262102155946\n",
      "[10, 100]: loss: 0.0071030051476554945\n",
      "[10, 105]: loss: 0.003162005312333349\n",
      "[10, 110]: loss: 0.0025229722596122883\n",
      "[10, 115]: loss: 0.0023430232431564946\n",
      "[10, 120]: loss: 0.004623075015842915\n",
      "[10, 125]: loss: 0.002876000653486699\n",
      "[10, 130]: loss: 0.0018075230764225125\n",
      "[10, 135]: loss: 0.008571817132178694\n",
      "[10, 140]: loss: 0.0056729754724074155\n",
      "[10, 145]: loss: 0.0017146150858025067\n",
      "[10, 150]: loss: 0.014709303883137181\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 19: 0.5304\n",
      "Random Sampling Iteration 20\n",
      "[1, 5]: loss: 0.0308242317114491\n",
      "[1, 10]: loss: 0.17298084931098856\n",
      "[1, 15]: loss: 0.05610985076054931\n",
      "[1, 20]: loss: 0.46942912600934505\n",
      "[1, 25]: loss: 0.15173892644816078\n",
      "[1, 30]: loss: 0.16604069550521672\n",
      "[1, 35]: loss: 0.3572855091188103\n",
      "[1, 40]: loss: 0.18495128955692053\n",
      "[1, 45]: loss: 0.19073646565084346\n",
      "[1, 50]: loss: 0.20685263141058385\n",
      "[1, 55]: loss: 0.42300563754542964\n",
      "[1, 60]: loss: 0.35389340721303597\n",
      "[1, 65]: loss: 0.20252507663099095\n",
      "[1, 70]: loss: 0.4368532504886389\n",
      "[1, 75]: loss: 0.24168830242706463\n",
      "[1, 80]: loss: 0.08863741322420537\n",
      "[1, 85]: loss: 0.131035217869794\n",
      "[1, 90]: loss: 0.29785312712192535\n",
      "[1, 95]: loss: 0.2790678122546524\n",
      "[1, 100]: loss: 0.06738603231497109\n",
      "[1, 105]: loss: 0.272521237260662\n",
      "[1, 110]: loss: 0.15273124654777348\n",
      "[1, 115]: loss: 0.1286493507650448\n",
      "[1, 120]: loss: 0.06623002688866109\n",
      "[1, 125]: loss: 0.16526820993749425\n",
      "[1, 130]: loss: 0.24343217167188413\n",
      "[1, 135]: loss: 0.16453598096268252\n",
      "[1, 140]: loss: 0.2564887525513768\n",
      "[1, 145]: loss: 0.032521308083232725\n",
      "[1, 150]: loss: 0.32493058033287525\n",
      "[1, 155]: loss: 0.5547154261730611\n",
      "[2, 5]: loss: 0.05213864566758275\n",
      "[2, 10]: loss: 0.1573668080382049\n",
      "[2, 15]: loss: 0.08489369868766516\n",
      "[2, 20]: loss: 0.10692936318810098\n",
      "[2, 25]: loss: 0.07527238095644861\n",
      "[2, 30]: loss: 0.13537743198685348\n",
      "[2, 35]: loss: 0.1026671746512875\n",
      "[2, 40]: loss: 0.08283967175520957\n",
      "[2, 45]: loss: 0.029282344970852137\n",
      "[2, 50]: loss: 0.026272969000274315\n",
      "[2, 55]: loss: 0.029773931251838803\n",
      "[2, 60]: loss: 0.08931186201516539\n",
      "[2, 65]: loss: 0.07615605075261556\n",
      "[2, 70]: loss: 0.010874743689782917\n",
      "[2, 75]: loss: 0.023998476041015238\n",
      "[2, 80]: loss: 0.04699885720037855\n",
      "[2, 85]: loss: 0.07073438953375444\n",
      "[2, 90]: loss: 0.034739352005999535\n",
      "[2, 95]: loss: 0.028310571506153792\n",
      "[2, 100]: loss: 0.011389206672902219\n",
      "[2, 105]: loss: 0.012626406387425959\n",
      "[2, 110]: loss: 0.07944442216830794\n",
      "[2, 115]: loss: 0.014520207769237459\n",
      "[2, 120]: loss: 0.006612946275708964\n",
      "[2, 125]: loss: 0.06517706182785332\n",
      "[2, 130]: loss: 0.024969960912130773\n",
      "[2, 135]: loss: 0.052883919881423935\n",
      "[2, 140]: loss: 0.011089104693382978\n",
      "[2, 145]: loss: 0.06695748050697148\n",
      "[2, 150]: loss: 0.07510319701395929\n",
      "[2, 155]: loss: 0.021961511665722355\n",
      "[3, 5]: loss: 0.017459916358347982\n",
      "[3, 10]: loss: 0.11293855812982656\n",
      "[3, 15]: loss: 0.052819596719928086\n",
      "[3, 20]: loss: 0.03755368282145355\n",
      "[3, 25]: loss: 0.09195831953547895\n",
      "[3, 30]: loss: 0.21905576344579458\n",
      "[3, 35]: loss: 0.10520805922715226\n",
      "[3, 40]: loss: 0.007508772687288001\n",
      "[3, 45]: loss: 0.042738777643535286\n",
      "[3, 50]: loss: 0.062326509185368195\n",
      "[3, 55]: loss: 0.04517779861635063\n",
      "[3, 60]: loss: 0.04771116201300174\n",
      "[3, 65]: loss: 0.004054021701449528\n",
      "[3, 70]: loss: 0.049465480144135654\n",
      "[3, 75]: loss: 0.02809546299977228\n",
      "[3, 80]: loss: 0.04657252796459943\n",
      "[3, 85]: loss: 0.03451053575554397\n",
      "[3, 90]: loss: 0.04339468615944497\n",
      "[3, 95]: loss: 0.07840610216953792\n",
      "[3, 100]: loss: 0.01638086531602312\n",
      "[3, 105]: loss: 0.00963956919076736\n",
      "[3, 110]: loss: 0.02420804335270077\n",
      "[3, 115]: loss: 0.16592108240001835\n",
      "[3, 120]: loss: 0.026617579977028072\n",
      "[3, 125]: loss: 0.04237940406892449\n",
      "[3, 130]: loss: 0.09875610467861407\n",
      "[3, 135]: loss: 0.08437901781871915\n",
      "[3, 140]: loss: 0.008652502961922437\n",
      "[3, 145]: loss: 0.028025024614180438\n",
      "[3, 150]: loss: 0.06207076604187023\n",
      "[3, 155]: loss: 0.03330551166436635\n",
      "[4, 5]: loss: 0.006621326407184824\n",
      "[4, 10]: loss: 0.03662360549060395\n",
      "[4, 15]: loss: 0.06646134337643161\n",
      "[4, 20]: loss: 0.02286781871225685\n",
      "[4, 25]: loss: 0.006875148901599459\n",
      "[4, 30]: loss: 0.01075612410204485\n",
      "[4, 35]: loss: 0.05530433201784035\n",
      "[4, 40]: loss: 0.01876038810587488\n",
      "[4, 45]: loss: 0.05034345271997154\n",
      "[4, 50]: loss: 0.05034104618243873\n",
      "[4, 55]: loss: 0.005270355177344754\n",
      "[4, 60]: loss: 0.04298677737824619\n",
      "[4, 65]: loss: 0.032282581567415036\n",
      "[4, 70]: loss: 0.011211240373086184\n",
      "[4, 75]: loss: 0.010860051312192809\n",
      "[4, 80]: loss: 0.032204597227973863\n",
      "[4, 85]: loss: 0.032962050710921176\n",
      "[4, 90]: loss: 0.04584547088597901\n",
      "[4, 95]: loss: 0.009601915633538738\n",
      "[4, 100]: loss: 0.021123632715898566\n",
      "[4, 105]: loss: 0.03218164210557006\n",
      "[4, 110]: loss: 0.027389378985390067\n",
      "[4, 115]: loss: 0.0604683926794678\n",
      "[4, 120]: loss: 0.02118561923271045\n",
      "[4, 125]: loss: 0.014465927160927095\n",
      "[4, 130]: loss: 0.012251925276359543\n",
      "[4, 135]: loss: 0.013842676591593772\n",
      "[4, 140]: loss: 0.09259473170277488\n",
      "[4, 145]: loss: 0.020443716013687663\n",
      "[4, 150]: loss: 0.036676858177088434\n",
      "[4, 155]: loss: 0.02102295672739274\n",
      "[5, 5]: loss: 0.012343074922682717\n",
      "[5, 10]: loss: 0.003290635155281052\n",
      "[5, 15]: loss: 0.0173448775021825\n",
      "[5, 20]: loss: 0.03293195489095524\n",
      "[5, 25]: loss: 0.010835673980182037\n",
      "[5, 30]: loss: 0.07696236739866436\n",
      "[5, 35]: loss: 0.02668794826604426\n",
      "[5, 40]: loss: 0.009137690823990852\n",
      "[5, 45]: loss: 0.024861008394509554\n",
      "[5, 50]: loss: 0.03119579321355559\n",
      "[5, 55]: loss: 0.0182454539462924\n",
      "[5, 60]: loss: 0.014076094375923276\n",
      "[5, 65]: loss: 0.005955011700280011\n",
      "[5, 70]: loss: 0.002330810153580387\n",
      "[5, 75]: loss: 0.00998171916580759\n",
      "[5, 80]: loss: 0.02401681029004976\n",
      "[5, 85]: loss: 0.016974001613561995\n",
      "[5, 90]: loss: 0.004812037936062552\n",
      "[5, 95]: loss: 0.01799062779173255\n",
      "[5, 100]: loss: 0.006399566613254137\n",
      "[5, 105]: loss: 0.028854465926997364\n",
      "[5, 110]: loss: 0.009010713270981796\n",
      "[5, 115]: loss: 0.002436577939079143\n",
      "[5, 120]: loss: 0.036591670650523156\n",
      "[5, 125]: loss: 0.07779311931517441\n",
      "[5, 130]: loss: 0.04480329359648749\n",
      "[5, 135]: loss: 0.01116236881352961\n",
      "[5, 140]: loss: 0.057980051074991934\n",
      "[5, 145]: loss: 0.020855415488767903\n",
      "[5, 150]: loss: 0.017514292703708634\n",
      "[5, 155]: loss: 0.015511608507949859\n",
      "[6, 5]: loss: 0.024060770439973567\n",
      "[6, 10]: loss: 0.0031568550039082766\n",
      "[6, 15]: loss: 0.02520180615829304\n",
      "[6, 20]: loss: 0.023617078782990575\n",
      "[6, 25]: loss: 0.016152966214576736\n",
      "[6, 30]: loss: 0.02911024197237566\n",
      "[6, 35]: loss: 0.009448379030800425\n",
      "[6, 40]: loss: 0.025570460496965097\n",
      "[6, 45]: loss: 0.02313993837015005\n",
      "[6, 50]: loss: 0.013257792350486852\n",
      "[6, 55]: loss: 0.0038558303713216446\n",
      "[6, 60]: loss: 0.006513748347060755\n",
      "[6, 65]: loss: 0.0019157745700795203\n",
      "[6, 70]: loss: 0.004777208960149437\n",
      "[6, 75]: loss: 0.015334849595092237\n",
      "[6, 80]: loss: 0.016117518302053213\n",
      "[6, 85]: loss: 0.009439661123906262\n",
      "[6, 90]: loss: 0.039375820004352136\n",
      "[6, 95]: loss: 0.013501857098162873\n",
      "[6, 100]: loss: 0.0021266568182909396\n",
      "[6, 105]: loss: 0.01626261544879526\n",
      "[6, 110]: loss: 0.0031802307421457954\n",
      "[6, 115]: loss: 0.012554747227113694\n",
      "[6, 120]: loss: 0.010037973173893988\n",
      "[6, 125]: loss: 0.008430615707766265\n",
      "[6, 130]: loss: 0.002290773220011033\n",
      "[6, 135]: loss: 0.010990377493726555\n",
      "[6, 140]: loss: 0.006833114071923774\n",
      "[6, 145]: loss: 0.00626846996601671\n",
      "[6, 150]: loss: 0.0014806593098910525\n",
      "[6, 155]: loss: 0.01708562910789624\n",
      "[7, 5]: loss: 0.0024054267050814815\n",
      "[7, 10]: loss: 0.0023500118404626846\n",
      "[7, 15]: loss: 0.01084599964087829\n",
      "[7, 20]: loss: 0.02281253330875188\n",
      "[7, 25]: loss: 0.015110421692952514\n",
      "[7, 30]: loss: 0.006146024301415309\n",
      "[7, 35]: loss: 0.004221464245347306\n",
      "[7, 40]: loss: 0.020930847735144198\n",
      "[7, 45]: loss: 0.00463495054282248\n",
      "[7, 50]: loss: 0.009555041331623215\n",
      "[7, 55]: loss: 0.002070088368782308\n",
      "[7, 60]: loss: 0.002609006070997566\n",
      "[7, 65]: loss: 0.026615950228006113\n",
      "[7, 70]: loss: 0.0026702621198637644\n",
      "[7, 75]: loss: 0.01826651092051179\n",
      "[7, 80]: loss: 0.008458600117592141\n",
      "[7, 85]: loss: 0.017610468363272958\n",
      "[7, 90]: loss: 0.011856645112857223\n",
      "[7, 95]: loss: 0.01805946860986296\n",
      "[7, 100]: loss: 0.0041725855007825885\n",
      "[7, 105]: loss: 0.023825916985515505\n",
      "[7, 110]: loss: 0.01239452906884253\n",
      "[7, 115]: loss: 0.022406496995245107\n",
      "[7, 120]: loss: 0.003699170549225528\n",
      "[7, 125]: loss: 0.003060740389628336\n",
      "[7, 130]: loss: 0.01553089380468009\n",
      "[7, 135]: loss: 0.01567383923975285\n",
      "[7, 140]: loss: 0.019132563182211015\n",
      "[7, 145]: loss: 0.003913296197424643\n",
      "[7, 150]: loss: 0.01217293982335832\n",
      "[7, 155]: loss: 0.014580955932615325\n",
      "[8, 5]: loss: 0.06631023436057149\n",
      "[8, 10]: loss: 0.002102552381984424\n",
      "[8, 15]: loss: 0.013131109502865002\n",
      "[8, 20]: loss: 0.001938713056006236\n",
      "[8, 25]: loss: 0.004766632147948258\n",
      "[8, 30]: loss: 0.008726681018742966\n",
      "[8, 35]: loss: 0.007345383528445382\n",
      "[8, 40]: loss: 0.003644031035946682\n",
      "[8, 45]: loss: 0.018132332246750593\n",
      "[8, 50]: loss: 0.0018353817977185827\n",
      "[8, 55]: loss: 0.008942383301473456\n",
      "[8, 60]: loss: 0.028140628695837222\n",
      "[8, 65]: loss: 0.0011341793433530256\n",
      "[8, 70]: loss: 0.0014635753468610346\n",
      "[8, 75]: loss: 0.016536848015675787\n",
      "[8, 80]: loss: 0.008111710892990232\n",
      "[8, 85]: loss: 0.003309663530671969\n",
      "[8, 90]: loss: 0.0024979883128253277\n",
      "[8, 95]: loss: 0.005474168778164312\n",
      "[8, 100]: loss: 0.004171879512796295\n",
      "[8, 105]: loss: 0.0011149271376780234\n",
      "[8, 110]: loss: 0.008952209551353008\n",
      "[8, 115]: loss: 0.0022888174207764678\n",
      "[8, 120]: loss: 0.026673975778976455\n",
      "[8, 125]: loss: 0.020378428904223256\n",
      "[8, 130]: loss: 0.003016818140167743\n",
      "[8, 135]: loss: 0.005789877439383417\n",
      "[8, 140]: loss: 0.0037371898361016065\n",
      "[8, 145]: loss: 0.004573455036734231\n",
      "[8, 150]: loss: 0.005915250054385979\n",
      "[8, 155]: loss: 0.0031714915530756116\n",
      "[9, 5]: loss: 0.0012528865117928945\n",
      "[9, 10]: loss: 0.0025594515500415582\n",
      "[9, 15]: loss: 0.0018406937597319484\n",
      "[9, 20]: loss: 0.008958098260336556\n",
      "[9, 25]: loss: 0.0015373269998235628\n",
      "[9, 30]: loss: 0.009370489744469523\n",
      "[9, 35]: loss: 0.006408056753571145\n",
      "[9, 40]: loss: 0.018672860460355878\n",
      "[9, 45]: loss: 0.004836679756408557\n",
      "[9, 50]: loss: 0.0009789887844817713\n",
      "[9, 55]: loss: 0.0010002782764786389\n",
      "[9, 60]: loss: 0.0012391049604048021\n",
      "[9, 65]: loss: 0.002397982036200119\n",
      "[9, 70]: loss: 0.006347834627376869\n",
      "[9, 75]: loss: 0.003660141352156643\n",
      "[9, 80]: loss: 0.0068959025338699576\n",
      "[9, 85]: loss: 0.005681810405803844\n",
      "[9, 90]: loss: 0.003933861924451776\n",
      "[9, 95]: loss: 0.007812061769072898\n",
      "[9, 100]: loss: 0.005525222106371075\n",
      "[9, 105]: loss: 0.00628178861370543\n",
      "[9, 110]: loss: 0.0026246792840538546\n",
      "[9, 115]: loss: 0.002837973523128312\n",
      "[9, 120]: loss: 0.0028748222612193786\n",
      "[9, 125]: loss: 0.008815412991680205\n",
      "[9, 130]: loss: 0.002102098354953341\n",
      "[9, 135]: loss: 0.0009522009167994838\n",
      "[9, 140]: loss: 0.005353138243663125\n",
      "[9, 145]: loss: 0.004657414319808595\n",
      "[9, 150]: loss: 0.00587338796867698\n",
      "[9, 155]: loss: 0.0010015325533458963\n",
      "[10, 5]: loss: 0.007457205530954525\n",
      "[10, 10]: loss: 0.004078665431734407\n",
      "[10, 15]: loss: 0.002395205563516356\n",
      "[10, 20]: loss: 0.0036732120461238082\n",
      "[10, 25]: loss: 0.028820388033636846\n",
      "[10, 30]: loss: 0.0013176864486013073\n",
      "[10, 35]: loss: 0.008487624196277466\n",
      "[10, 40]: loss: 0.008180413220543414\n",
      "[10, 45]: loss: 0.0096012622670969\n",
      "[10, 50]: loss: 0.005455414418975124\n",
      "[10, 55]: loss: 0.004186426071100868\n",
      "[10, 60]: loss: 0.0005293430804158561\n",
      "[10, 65]: loss: 0.002417624851659639\n",
      "[10, 70]: loss: 0.008311619269079529\n",
      "[10, 75]: loss: 0.0033235326773137785\n",
      "[10, 80]: loss: 0.0017094470968004316\n",
      "[10, 85]: loss: 0.002181357182053034\n",
      "[10, 90]: loss: 0.002383789211307885\n",
      "[10, 95]: loss: 0.002421968583803391\n",
      "[10, 100]: loss: 0.0011949339095735922\n",
      "[10, 105]: loss: 0.009637881797971204\n",
      "[10, 110]: loss: 0.010856978464289568\n",
      "[10, 115]: loss: 0.017412818473530933\n",
      "[10, 120]: loss: 0.006864344024506863\n",
      "[10, 125]: loss: 0.002375197520450456\n",
      "[10, 130]: loss: 0.0013261502754176036\n",
      "[10, 135]: loss: 0.0025525691380607896\n",
      "[10, 140]: loss: 0.002968073356896639\n",
      "[10, 145]: loss: 0.004691415131674148\n",
      "[10, 150]: loss: 0.002080334859783761\n",
      "[10, 155]: loss: 0.004206205325317569\n",
      "Finished Training\n",
      "Random Sampling Test Accuracy after iteration 20: 0.5527\n"
     ]
    }
   ],
   "source": [
    "accuracies_random = []\n",
    "\n",
    "# Perform iterative training and sampling for Random Sampling\n",
    "for iteration in range(49):  # Adjust iterations as needed\n",
    "    print(f\"Random Sampling Iteration {iteration+1}\")\n",
    "    \n",
    "    # Random Sampling\n",
    "    random_indices = random_sampling(n=1000, unlabeled_set_size=len(trainset_unlabeled_indices_random))\n",
    "    new_indices_random = [trainset_unlabeled_indices_random[idx] for idx in random_indices]\n",
    "    trainset_labeled_indices_random.extend(new_indices_random)\n",
    "    trainset_unlabeled_indices_random = [idx for idx in trainset_unlabeled_indices_random if idx not in new_indices_random]\n",
    "    \n",
    "    labeled_set_random = Subset(trainset, trainset_labeled_indices_random)\n",
    "    trainloader_random = DataLoader(labeled_set_random, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Retrain the model with the updated dataset\n",
    "    train_model(model_random, trainloader_random, epochs=10)  # Consistent with the Uncertainty loop\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy_random = evaluate_model(model_random, testloader)\n",
    "    accuracies_random.append(accuracy_random)\n",
    "    print(f\"Random Sampling Test Accuracy after iteration {iteration+1}: {accuracy_random}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 1000  # Number of samples to query in each iteration\n",
    "iterations = 49  # Number of iterations\n",
    "# X-axis values representing the number of labeled examples seen so far\n",
    "x_values_percentage = [(initial_train_size + n * i) / len(trainset) * 100 for i in range(iterations)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for Uncertainty Sampling\n",
    "plt.plot(x_values, accuracies, label='Uncertainty Sampling', marker='o')\n",
    "\n",
    "# Plot for Random Sampling (Baseline)\n",
    "plt.plot(x_values, accuracies_random, label='Random Sampling', marker='^')\n",
    "\n",
    "# Add a horizontal line for the benchmark accuracy\n",
    "benchmark_accuracy = 81.28  # Benchmark accuracy\n",
    "plt.axhline(y=benchmark_accuracy/100, color='r', linestyle='-', label=f'Benchmark Accuracy: {benchmark_accuracy}%')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Number of Labeled Examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy vs Number of Labeled Examples')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
